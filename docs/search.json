[
  {
    "objectID": "qmd_root/sdis_summary.html",
    "href": "qmd_root/sdis_summary.html",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each state’s DNA database. The data captures key dimensions including:\n\nTotal size of each state’s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "qmd_root/sdis_summary.html#overview",
    "href": "qmd_root/sdis_summary.html#overview",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each state’s DNA database. The data captures key dimensions including:\n\nTotal size of each state’s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "qmd_root/sdis_summary.html#setup",
    "href": "qmd_root/sdis_summary.html#setup",
    "title": "SDIS Summary Analysis",
    "section": "Setup",
    "text": "Setup\nLoad necessary packages for the analysis.\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"readr\",      # Reading CSV files\n  \"dplyr\",      # Data manipulation\n  \"tidyr\",      # Data tidying\n  \"purrr\",      # Functional programming tools\n  \"ggplot2\",    # Data visualization\n  \"heatmaply\",  # Interactive heatmaps\n  \"kableExtra\", # Enhanced tables for reporting\n  \"DT\",         # Interactive tables\n  \"flextable\",  # Flexible tables for reporting\n  \"maps\",       # Mapping tools\n  \"here\",       # File path management\n  \"remotes\",    # To install urbnmapr package\n  \"sf\",\n  \"patchwork\"\n)\n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr)\n  library(tidyr)\n  library(purrr)\n  library(ggplot2)\n  library(heatmaply)\n  library(kableExtra)\n  library(DT)\n  library(flextable)\n  library(maps)\n  library(here)\n  library(remotes)\n  library(sf)\n  library(patchwork)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\nif (all(loaded_packages)) {\n  message(\"📚 All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#import-sdis-data",
    "href": "qmd_root/sdis_summary.html#import-sdis-data",
    "title": "SDIS Summary Analysis",
    "section": "Import SDIS data",
    "text": "Import SDIS data\nLoad the SDIS dataset and display a glimpse of its structure, including data types, missing values, and unique counts.\nDatabase Columns Definition\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nstate\nName of the U.S. state or jurisdiction\n\n\nn_total\nTotal number of DNA profiles in the state database\n\n\nn_arrestees\nNumber of arrestee DNA profiles in the database\n\n\nn_offenders\nNumber of convicted offender DNA profiles in the database\n\n\nn_forensic\nNumber of forensic/crime scene DNA profiles in the database\n\n\narrestee_collection\nWhether the state collects DNA from arrestees (‘yes’/‘no’)\n\n\ncollection_statute\nCitation to the state’s DNA collection statute\n\n\nfam_search\nFamily search policy status (‘permitted’, ‘prohibited’, ‘unspecified’)\n\n\ndatabase_source\nURL source for the database statistics\n\n\ndatabase_source_year\nYear the database statistics were reported\n\n\nverification_comment\nAdditional notes or comments about data verification\n\n\n\n\n\nShow import code\n# Set up path to data file\ndata_file &lt;- file.path(here(\"data\", \"sdis\", \"raw\", \"sdis_raw.csv\"))\n\n# Load the SDIS data\nsdis_data &lt;- read_csv(data_file)\n\n# Display data types for each column\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    First_Values = sapply(df, function(x) {\n      if(is.numeric(x)) {\n        paste(round(head(x, 3), 2), collapse = \", \")\n      } else {\n        paste(encodeString(head(as.character(x), 3)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows ×\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\nenhanced_glimpse(sdis_data)\n\n\nColumnTypeRowsMissingUniqueFirst_Valuesstatecharacter50050Alabama, Alaska, Arizonan_totalnumeric502328360000, 79146, NAn_arresteesnumeric50278NA, 52149, NAn_offendersnumeric503417NA, 26997, NAn_forensicnumeric503912NA, 3866, NAarrestee_collectioncharacter5002yes, yes, yescollection_statutecharacter50050Ala. Code Sec.36-18-25, AK Stat Sec.44.41.035 (2014), AZ Rev Stat Sec.13-610 (2016)fam_searchcharacter5003unspecified, unspecified, unspecifieddatabase_sourcecharacter502031https://adfs.alabama.gov/services/fb/fb-statistics, https://dps.alaska.gov/Statewide/CrimeLab/Forensic-Biology/DNA, &lt;NA&gt;database_source_yearnumeric5020112024, 2024, NAverification_commentcharacter503516Alabama Department of Forensic Sciences notes that as of January 2024 the state DNA database contains about 360,000 convicted offender and arrestee profileshttps://adfs.alabama.gov/services/fb/fb-statistics#:~:text=The%20DNA%20Databank%20laboratory%20receives,DNA%20profiles%20are%20being%20searched., Alaska DPS crime‑lab CODIS metrics (Feb 2024) list 52,149 arrestee profiles, 26,997 convicted offender profiles and 3,866 forensic profiles in the Alaska state DNA databasehttps://dps.alaska.gov/Statewide/CrimeLab/Forensic-Biology/DNA#:~:text=CODIS%20Metrics%20as%20of%C2%A0February%202024,Offenders%20in%20database%2026%2C997%2015%2C268%2C774., &lt;NA&gt;Data frame dimensions: 50 rows × 11 columns"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#arrestee-collection-information",
    "href": "qmd_root/sdis_summary.html#arrestee-collection-information",
    "title": "SDIS Summary Analysis",
    "section": "Arrestee Collection Information",
    "text": "Arrestee Collection Information\nIn the United States, the collection of DNA from individuals upon arrest, prior to any conviction, is a significant law enforcement practice with complex legal and ethical dimensions.\nThe following map and table illustrate the current patchwork of state laws governing this practice.\n\n\nShow arrestee collection visualization code\n# Create summary data for arrestee collection\narrestee_summary &lt;- sdis_data %&gt;%\n  group_by(arrestee_collection) %&gt;%\n  summarise(\n    count = n(),\n    states = list(state),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Get US state map data\nus_states &lt;- map_data(\"state\")\n\n# Prepare map data\nmap_data &lt;- sdis_data %&gt;%\n  mutate(region = tolower(state)) %&gt;%\n  right_join(us_states, by = \"region\") %&gt;%\n  filter(!is.na(arrestee_collection))\n\n# Option 1: US Map Visualization\nmap_plot &lt;- ggplot(map_data, aes(x = long, y = lat, group = group, fill = arrestee_collection)) +\n  geom_polygon(color = \"white\", linewidth = 0.2) +\n  coord_fixed(1.3) +\n  scale_fill_manual(\n    values = c(\n      \"yes\" = \"#2E86AB\",      # Blue for collecting states\n      \"no\" = \"#FF6B6B\"       # Red for non-collecting states\n    ),\n    name = \"Collects DNA from Arrestees\",\n    labels = c(\"Yes\", \"No\")\n  ) +\n  labs(\n    title = \"DNA Collection from Arrestees by State\",\n    subtitle = \"States that collect DNA samples from individuals upon arrest\",\n    caption = \"Source: SDIS Database Analysis\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 12),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  )\n\n# Create detailed summary table\nsummary_table &lt;- arrestee_summary %&gt;%\n  mutate(\n    state_list = map_chr(states, ~paste(.x, collapse = \", \")),\n    percentage = round(count / sum(count) * 100, 1)\n  ) %&gt;%\n  select(Status = arrestee_collection, Count = count, Percentage = percentage, States = state_list)\n\n# Display the visualizations\nprint(map_plot)\n\n\n\n\n\n\n\n\n\nShow arrestee collection visualization code\n# Create a nice flextable for the report\nft &lt;- flextable(summary_table) %&gt;%\n  theme_zebra() %&gt;%\n  set_caption(\"Arrestee DNA Collection Status by State\") %&gt;%\n  autofit() %&gt;%\n  align(align = \"left\", part = \"all\") %&gt;%\n  bg(j = \"Status\", bg = function(x) ifelse(x == \"yes\", \"#2E86AB\", ifelse(x == \"no\", \"#FF6B6B\", \"#CCCCCC\"))) %&gt;%\n  color(j = \"Status\", color = \"white\") %&gt;%\n  bold(j = \"Status\") %&gt;%\n  fontsize(size = 11, part = \"all\")\n\nft\n\n\nStatusCountPercentageStatesyes3366Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Florida, Georgia, Illinois, Indiana, Kansas, Louisiana, Maryland, Michigan, Minnesota, Mississippi, Missouri, Nevada, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Wisconsinno1734Delaware, Hawaii, Idaho, Iowa, Kentucky, Maine, Massachusetts, Montana, Nebraska, New Hampshire, New York, Oregon, Pennsylvania, Vermont, Washington, West Virginia, Wyoming\n\n\n\nAdjusting n_arrestees\n\nAdjust the dataset to ensure consistency in arrestee DNA collection reporting.\nSetting n_arrestees to 0 for states that do not collect arrestee DNA, even if they report non-zero values.\nRemoved the arrestee_colection column after adjustments were made.\n\n\n\nShow pre-processing code\n# Create a copy of the data for processing\nsdis_data_processed &lt;- sdis_data\n\n# Count states affected by this adjustment\nstates_with_no_collection &lt;- sdis_data_processed %&gt;% \n  filter(arrestee_collection == 'no')\n\nstates_to_adjust &lt;- states_with_no_collection %&gt;% \n  filter(!is.na(n_arrestees) & n_arrestees != 0)\n\nif (nrow(states_to_adjust) &gt; 0) {\n  cat(\"States with arrestee_collection='no' but non-zero n_arrestees values:\\n\")\n  for (i in 1:nrow(states_to_adjust)) {\n    state &lt;- states_to_adjust[i, ]\n    cat(paste0(\"  • \", state$state, \": n_arrestees = \", format(state$n_arrestees, big.mark = \",\"), \"\\n\"))\n  }\n}\n\n# Set n_arrestees to 0 for states that don't collect arrestee DNA\nsdis_data_processed &lt;- sdis_data_processed %&gt;%\n  mutate(n_arrestees = ifelse(arrestee_collection == 'no', 0, n_arrestees))\n\n# Use processed data for all subsequent analyses\nsdis_data &lt;- sdis_data_processed"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#family-search-policy-status",
    "href": "qmd_root/sdis_summary.html#family-search-policy-status",
    "title": "SDIS Summary Analysis",
    "section": "Family Search Policy Status",
    "text": "Family Search Policy Status\nFamilial DNA searching is an advanced forensic technique that uses partial DNA matches to identify potential relatives of an unknown suspect in a criminal database.\nThe following visualization maps the current patchwork of state policies (‘permitted’, ‘prohibited’, ‘unspecified’).\n\n\nShow family search policy visualization code\n# Create summary data for family search policies\nfamily_search_summary &lt;- sdis_data %&gt;%\n  mutate(\n    fam_search = factor(fam_search, \n                       levels = c(\"permitted\", \"prohibited\", \"unspecified\"),\n                       labels = c(\"Permitted\", \"Prohibited\", \"Unspecified\")),\n    fam_search_simple = case_when(\n      fam_search == \"Permitted\" ~ \"Permitted\",\n      fam_search == \"Prohibited\" ~ \"Prohibited\",\n      TRUE ~ \"Unspecified/No Policy\"\n    )\n  ) %&gt;%\n  group_by(fam_search, fam_search_simple) %&gt;%\n  summarise(\n    count = n(),\n    states = list(state),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Get complete data for mapping (include states with missing data)\nall_states_map &lt;- map_data(\"state\") %&gt;%\n  as_tibble() %&gt;%\n  distinct(region) %&gt;%\n  mutate(\n    state_name = str_to_title(region),\n    has_data = state_name %in% sdis_data$state\n  )\n\n# Prepare map data for family search policies\nfamily_search_map_data &lt;- sdis_data %&gt;%\n  mutate(\n    region = tolower(state)) %&gt;%\n  right_join(map_data(\"state\"), by = c(\"region\" = \"region\")) %&gt;%\n  filter(!is.na(fam_search))\n\n# Option 1: US Map Visualization\nfamily_map_plot &lt;- ggplot(family_search_map_data, \n                         aes(x = long, y = lat, group = group, fill = fam_search)) +\n  geom_polygon(color = \"white\", linewidth = 0.2) +\n  coord_fixed(1.3) +\n  scale_fill_manual(\n    values = c(\n      \"permitted\" = \"#2E86AB\",      # Blue for permitted\n      \"prohibited\" = \"#F9A03F\",   # Orange for prohibited\n      \"unspecified\" = \"#CCCCCC\"  # Light gray for unspecified\n    ),\n    name = \"Familial Search Policy\",\n    drop = FALSE\n  ) +\n  labs(\n    title = \"Familial DNA Search Policies by State\",\n    subtitle = \"State policies regarding familial DNA searching in databases\",\n    caption = \"Source: SDIS Database Analysis\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 12),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  )\n\n# Display the visualizations\nprint(family_map_plot)\n\n\n\n\n\n\n\n\n\nShow family search policy visualization code\n# Create a comprehensive summary table\nsummary_table_data &lt;- family_search_summary %&gt;%\n  mutate(\n    state_list = map_chr(states, ~paste(.x, collapse = \", \")),\n    percentage = round(count / sum(count) * 100, 1),\n    icon = case_when(\n      fam_search == \"Allowed\" ~ \"🔵\",\n      fam_search == \"Restricted\" ~ \"🟠\",\n      TRUE ~ \"⚪\"\n    )\n  ) %&gt;%\n  select(Icon = icon, Policy = fam_search, Count = count, Percentage = percentage, States = state_list)\n\n# Create a nice flextable for the report\nft &lt;- flextable(summary_table_data) %&gt;%\n  theme_zebra() %&gt;%\n  set_caption(\"Familial DNA Search Policy Status by State\") %&gt;%\n  autofit() %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;%\n  align(align = \"left\", j = \"States\") %&gt;%\n  bg(j = \"Policy\", bg = function(x) {\n    case_when(\n      x == \"Permitted\" ~ \"#2E86AB\",\n      x == \"Prohibited\" ~ \"#F9A03F\",\n      TRUE ~ \"#CCCCCC\"\n    )\n  }) %&gt;%\n  color(j = \"Policy\", color = \"white\") %&gt;%\n  bold(j = \"Policy\") %&gt;%\n  fontsize(size = 11, part = \"all\") %&gt;%\n  width(j = \"States\", width = 3)\n\nft\n\n\nIconPolicyCountPercentageStates⚪Unspecified3774Alabama, Alaska, Arizona, Connecticut, Delaware, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Massachusetts, Minnesota, Mississippi, Missouri, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Vermont, Washington, West Virginia⚪Permitted1224Arkansas, California, Colorado, Florida, Michigan, Montana, New York, Texas, Utah, Virginia, Wisconsin, Wyoming⚪Prohibited12Maryland"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#three-panel-policy-map-horizontal-layout",
    "href": "qmd_root/sdis_summary.html#three-panel-policy-map-horizontal-layout",
    "title": "SDIS Summary Analysis",
    "section": "Three-Panel Policy Map: Horizontal Layout",
    "text": "Three-Panel Policy Map: Horizontal Layout\nThis visualization presents three key policy dimensions side-by-side: arrestee DNA collection, familial search policies, and FOIA data availability.\n\n\nShow three-panel map code\nremotes::install_github(\"UrbanInstitute/urbnmapr\")\nlibrary(urbnmapr)\n\n# Create FOIA availability column\nsdis_data &lt;- sdis_data %&gt;%\n  mutate(\n    foia_availability = case_when(\n      state %in% c(\"California\", \"Florida\", \"Indiana\", \"Maine\", \n                   \"Nevada\", \"South Dakota\", \"Texas\") ~ \"provided\",\n      TRUE ~ \"not_provided\"\n    )\n  )\n\n# Load US map data with proper AK/HI positioning\nus_map_data &lt;- get_urbn_map(\"states\", sf = TRUE)\n\n# Prepare map data\nsdis_data_map &lt;- sdis_data %&gt;%\n  mutate(region = tolower(state))\n\nus_map_data &lt;- us_map_data %&gt;%\n  mutate(region = tolower(state_name))\n\n# Merge polygon data with policy data\nmap_data_repos &lt;- us_map_data %&gt;%\n  left_join(sdis_data_map, by = \"region\")\n\n# Function to create individual maps\ncreate_policy_map &lt;- function(fill_var, fill_title, fill_colors, plot_title) {\n  ggplot(map_data_repos) +\n    geom_sf(aes(fill = !!sym(fill_var)), color = \"white\", linewidth = 0.3) +\n    scale_fill_manual(\n      name = fill_title,\n      values = fill_colors,\n      na.value = \"gray90\",\n      na.translate = FALSE,\n      guide = guide_legend(\n        direction = \"vertical\",\n        title.position = \"top\",\n        title.hjust = 0.5\n      )\n    ) +\n    theme_void() +\n    labs(title = plot_title) +\n    theme(\n      plot.title = element_text(size = 11, face = \"bold\", hjust = 0.5, margin = margin(b = 5)),\n      legend.position = \"bottom\",\n      legend.title = element_text(size = 9, face = \"bold\"),\n      legend.text = element_text(size = 8),\n      legend.box = \"vertical\",\n      legend.margin = margin(t = 5, b = 0)\n    )\n}\n\n# Create the three maps\nmap_arrestee_repos &lt;- create_policy_map(\n  \"arrestee_collection\", \"Arrestee Collection\",\n  c(\"yes\" = \"#2E8B57\", \"no\" = \"#FF6B6B\"), \n  \"A. Arrestee DNA Collection Policy\"\n)\n\nmap_familial_repos &lt;- create_policy_map(\n  \"fam_search\", \"Familial Search\",\n  c(\"permitted\" = \"#1f77b4\", \"prohibited\" = \"#ff7f0e\", \"unspecified\" = \"#9467bd\"),\n  \"B. Familial Search Policy\"\n)\n\nmap_foia_repos &lt;- create_policy_map(\n  \"foia_availability\", \"FOIA Response\",\n  c(\"provided\" = \"#2ca02c\", \"not_provided\" = \"gray70\"),\n  \"C. FOIA Response Status\"\n)\n\n# Combine maps horizontally\nfigure_combined &lt;- map_arrestee_repos | map_familial_repos | map_foia_repos\n\nfigure_combined + plot_annotation(\n  title = \"State DNA Policy Landscape and Data Availability\",\n  theme = theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5))\n)\n\n\n\n\n\n\n\n\n\nShow three-panel map code\n# Create summary statistics table\npolicy_summary &lt;- sdis_data %&gt;%\n  summarise(\n    `Arrestee Collection - Yes` = sum(arrestee_collection == \"yes\", na.rm = TRUE),\n    `Arrestee Collection - No` = sum(arrestee_collection == \"no\", na.rm = TRUE),\n    `Familial Search - Permitted` = sum(fam_search == \"permitted\", na.rm = TRUE),\n    `Familial Search - Prohibited` = sum(fam_search == \"prohibited\", na.rm = TRUE),\n    `Familial Search - Unspecified` = sum(fam_search == \"unspecified\", na.rm = TRUE),\n    `FOIA - Provided` = sum(foia_availability == \"provided\", na.rm = TRUE),\n    `FOIA - Not Provided` = sum(foia_availability == \"not_provided\", na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Count\") %&gt;%\n  mutate(Percentage = paste0(round(Count / 50 * 100, 1), \"%\"))\n\n# Display summary table\nflextable(policy_summary) %&gt;%\n  theme_zebra() %&gt;%\n  set_caption(\"Policy Summary Statistics (n = 50 states)\") %&gt;%\n  autofit() %&gt;%\n  align(align = \"left\", part = \"all\") %&gt;%\n  bold(j = \"Metric\") %&gt;%\n  fontsize(size = 10, part = \"all\")\n\n\nMetricCountPercentageArrestee Collection - Yes3366%Arrestee Collection - No1734%Familial Search - Permitted1224%Familial Search - Prohibited12%Familial Search - Unspecified3774%FOIA - Provided714%FOIA - Not Provided4386%"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#data-availability-overview",
    "href": "qmd_root/sdis_summary.html#data-availability-overview",
    "title": "SDIS Summary Analysis",
    "section": "Data Availability Overview",
    "text": "Data Availability Overview\nAssess the completeness of data across states, including which states are missing and the availability of key fields.\n\n\nShow overview code\n# Identify states present in the dataset\nstates_in_data &lt;- unique(sdis_data$state) %&gt;% sort()\n\n# Check if all 50 states are represented\nall_states &lt;- c('Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n                'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n                'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n                'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n                'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n                'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n                'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia',\n                'Wisconsin', 'Wyoming')\n\nmissing_states &lt;- setdiff(all_states, states_in_data)\nif (length(missing_states) &gt; 0) {\n  cat(paste(\"\\nMissing states:\", paste(missing_states, collapse = \", \"), \"\\n\"))\n} else {\n  cat(\"\\nAll 50 states are represented in the dataset\\n\")\n}\n\n# Assess data completeness for each state\n# Improved heatmap with enhanced visualization\ndata_availability &lt;- sdis_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(across(everything(), ~sum(!is.na(.)))) %&gt;%\n  select(-state) %&gt;%\n  as.data.frame()\n\nrownames(data_availability) &lt;- unique(sdis_data$state)\n\n# Generate visualization of data completeness\nkey_fields &lt;- c('n_total', 'n_arrestees', 'n_offenders', 'n_forensic')\n\n# Filter to include only key fields that exist in the data\navailable_key_fields &lt;- intersect(key_fields, names(data_availability))\n\nif (length(available_key_fields) &gt; 0) {\n  availability_subset &lt;- data_availability[, available_key_fields, drop = FALSE]\n  availability_binary &lt;- as.data.frame(ifelse(availability_subset &gt; 0, 1, 0))\n  \n  # Create heatmap data in long format\n  heatmap_long &lt;- availability_binary %&gt;%\n    rownames_to_column(\"state\") %&gt;%\n    pivot_longer(cols = -state, names_to = \"field\", values_to = \"available\") %&gt;%\n    mutate(\n      state = factor(state, levels = rownames(availability_binary)),\n      state = factor(state, levels = sort(unique(state), decreasing = TRUE)),\n      field = factor(field, levels = available_key_fields),\n      # Create labels with different symbols\n      label = case_when(\n        available == 1 ~ \"✓\",\n        available == 0 ~ \"!\",\n        TRUE ~ \"\"\n      ),\n      # Color coding for different values\n      fill_color = case_when(\n        available == 1 ~ \"Available\",\n        available == 0 ~ \"Missing\",\n        TRUE ~ \"Unknown\"\n      )\n    )\n  \n  # Create the enhanced heatmap\nggplot(heatmap_long, aes(x = field, y = state, fill = fill_color)) +\n  geom_tile(color = \"white\", linewidth = 0.8, width = 0.9, height = 0.9) +\n  geom_text(aes(label = label, color = fill_color), \n            size = 4, fontface = \"bold\", vjust = 0.8) +\n  scale_fill_manual(values = c(\n    \"Available\" = \"#2E86AB\",\n    \"Missing\" = \"#FF6B6B\",\n    \"Unknown\" = \"#f0f0f0\"\n  )) +\n  scale_color_manual(values = c(\n    \"Available\" = \"white\",\n    \"Missing\" = \"white\",\n    \"Unknown\" = \"gray30\"\n  )) +\n  labs(\n    title = \"Data Availability Heatmap by State\",\n    subtitle = \"✓ = Data available | ! = Data missing (0 values)\",\n    x = NULL,\n    y = NULL,\n    fill = \"Data Status\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"),\n    axis.text.x.top = element_text(angle = 45, hjust = 0, face = \"bold\"), \n    axis.text.y = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\", vjust = 0),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 10),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.ticks.x = element_line(),\n    axis.ticks.x.top = element_line()\n  ) +\n  guides(color = \"none\") +\n  coord_fixed() +\n  scale_x_discrete(position = \"top\")\n  \n} else {\n  message(\"No key fields available for heatmap visualization\")\n}\n\n\n\n\n\n\n\n\n\nShow overview code\n# Data coverage summary\ncat(\"\\nData field coverage across states:\\n\")\n\nkey_fields &lt;- c('n_total', 'n_arrestees', 'n_offenders', 'n_forensic', 'fam_search', 'collection_statute')\n\nfor (col in key_fields) {\n  if (col %in% names(sdis_data)) {\n    states_with_data &lt;- sum(!is.na(sdis_data[[col]]))\n    coverage_pct &lt;- states_with_data / length(states_in_data) * 100\n    cat(paste0(col, \": \", states_with_data, \" states (\", round(coverage_pct, 1), \"%)\\n\"))\n  }\n}\n\n\n\nAll 50 states are represented in the dataset\n\nData field coverage across states:\nn_total: 27 states (54%)\nn_arrestees: 23 states (46%)\nn_offenders: 16 states (32%)\nn_forensic: 11 states (22%)\nfam_search: 50 states (100%)\ncollection_statute: 50 states (100%)"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#total-profile-calculations-verification",
    "href": "qmd_root/sdis_summary.html#total-profile-calculations-verification",
    "title": "SDIS Summary Analysis",
    "section": "Total Profile Calculations Verification",
    "text": "Total Profile Calculations Verification\nThis section examines states reporting n_total alongside component counts to determine whether totals represent:\n\nSum of all profile types including forensic (n_arrestees + n_offenders + n_forensic)\nSum of combined profiles only (n_arrestees + n_offenders)\n\n\n\nShow calculations code\n# Identify states with n_total and at least one component count\nstates_with_totals &lt;- sdis_data %&gt;%\n  filter(!is.na(n_total)) %&gt;%\n  mutate(\n    sum_all = rowSums(select(., n_arrestees, n_offenders, n_forensic), na.rm = TRUE),\n    sum_arrestees_offenders = rowSums(select(., n_arrestees, n_offenders), na.rm = TRUE),\n    # Check if all components are available\n    all_components_available = !is.na(n_arrestees) & !is.na(n_offenders) & !is.na(n_forensic),\n    both_arrestees_offenders = !is.na(n_arrestees) & !is.na(n_offenders)\n  )\n\n# Check matches with tolerance\ntolerance &lt;- 10\n\nstates_with_totals &lt;- states_with_totals %&gt;%\n  mutate(\n    matches_combined_forensic = ifelse(\n      all_components_available,\n      abs(n_total - sum_all) &lt;= tolerance,\n      FALSE\n    ),\n    matches_combined = ifelse(\n      both_arrestees_offenders,\n      (abs(n_total - sum_arrestees_offenders) &lt;= tolerance) & \n        (n_arrestees &gt; 0),\n      FALSE\n    )\n  )\n\n# Filter to states with at least one component count\nhas_components &lt;- states_with_totals %&gt;%\n  filter(!is.na(n_arrestees) | !is.na(n_offenders) | !is.na(n_forensic))\n\ncat(paste(\"States with n_total and component data:\", nrow(has_components), \"\\n\"))\ncat(\"\\nTotal calculation patterns:\\n\")\n\n# Categorize states\nincludes_all &lt;- has_components %&gt;% filter(matches_combined_forensic) %&gt;% pull(state)\nforensic_only &lt;- has_components %&gt;% filter(matches_combined & !matches_combined_forensic) %&gt;% pull(state)\nneither &lt;- has_components %&gt;% filter(!matches_combined_forensic & !matches_combined) %&gt;% pull(state)\n\nif (length(includes_all) &gt; 0) {\n  cat(\"\\nn_total matches combined profiles with forensic (arrestees + offenders + forensic):\\n\")\n  for (state in includes_all) {\n    cat(paste(\"  •\", state, \"\\n\"))\n  }\n} else {\n  cat(\"\\nStates where n_total matches the sum of arrestees + offenders + forensic: None\\n\")\n}\n\nif (length(forensic_only) &gt; 0) {\n  cat(\"\\nStates where n_total includes combined profiles only (arrestees + offenders):\\n\")\n  for (state in forensic_only) {\n    cat(paste(\"  •\", state, \"\\n\"))\n  }\n}\n\nif (length(neither) &gt; 0) {\n  cat(\"\\nStates where n_total does not match calculated sums:\\n\")\n  for (state in neither) {\n    state_data &lt;- has_components %&gt;% filter(state == !!state)\n    cat(paste0(\"  • \", state, \":\\n n_total=\", format(state_data$n_total, big.mark = \",\"), \n               \"\\n Sum_all=\", format(state_data$sum_all, big.mark = \",\"),\n               \"\\n Sum_arrestees_offenders =\", format(state_data$sum_arrestees_offenders, big.mark = \",\"), \"\\n\\n\"))\n  }\n}\n\n# Display detailed breakdown\ncat(\"\\nDetailed breakdown:\\n\")\nhas_components %&gt;%\n  select(state, n_total, n_arrestees, n_offenders, n_forensic, matches_combined_forensic, matches_combined) %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.na(.), \"\", format(., big.mark = \",\")))) %&gt;%\n  flextable()\n\n\nStates with n_total and component data: 17 \n\nTotal calculation patterns:\n\nStates where n_total matches the sum of arrestees + offenders + forensic: None\n\nStates where n_total includes combined profiles only (arrestees + offenders):\n  • Alaska \n  • North Carolina \n  • Rhode Island \n  • Tennessee \n\nStates where n_total does not match calculated sums:\n  • California:\n n_total=3,365,402\n Sum_all=167,053\n Sum_arrestees_offenders =0\n\n  • Connecticut:\n n_total=127,940\n Sum_all=151,140\n Sum_arrestees_offenders =127,940\n\n  • Georgia:\n n_total=347,145\n Sum_all=347,145\n Sum_arrestees_offenders =324,864\n\n  • Idaho:\n n_total=39,000\n Sum_all=39,000\n Sum_arrestees_offenders =39,000\n\n  • Illinois:\n n_total=689,297\n Sum_all=759,224\n Sum_arrestees_offenders =689,297\n\n  • Indiana:\n n_total=449,000\n Sum_all=470,400\n Sum_arrestees_offenders =448,000\n\n  • Minnesota:\n n_total=180,000\n Sum_all=180,000\n Sum_arrestees_offenders =180,000\n\n  • Missouri:\n n_total=4e+05\n Sum_all=386,000\n Sum_arrestees_offenders =386,000\n\n  • Montana:\n n_total=32,284\n Sum_all=33,162\n Sum_arrestees_offenders =32,284\n\n  • South Carolina:\n n_total=175,629\n Sum_all=11,127\n Sum_arrestees_offenders =0\n\n  • Texas:\n n_total=1,308,774\n Sum_all=1,308,774\n Sum_arrestees_offenders =1,308,774\n\n  • Washington:\n n_total=272,000\n Sum_all=280,800\n Sum_arrestees_offenders =272,000\n\n  • West Virginia:\n n_total=47,444\n Sum_all=51,349\n Sum_arrestees_offenders =47,444\n\n\nDetailed breakdown:\n\n\nstaten_totaln_arresteesn_offendersn_forensicmatches_combined_forensicmatches_combinedAlaska   79,146 52,149   26,997  3,866falsetrueCalifornia3,365,402167,053falsefalseConnecticut  127,940  127,940 23,200falsefalseGeorgia  347,145  324,864 22,281falsefalseIdaho   39,000      0   39,000falsefalseIllinois  689,297  689,297 69,927falsefalseIndiana  449,000144,000  304,000 22,400falsefalseMinnesota  180,000  180,000falsefalseMissouri  400,000  386,000falsefalseMontana   32,284      0   32,284    878falsefalseNorth Carolina  350,000 50,000  300,000falsetrueRhode Island   27,818    484   27,334  1,798falsetrueSouth Carolina  175,629 11,127falsefalseTennessee  518,614226,569  292,045falsetrueTexas1,308,7741,308,774falsefalseWashington  272,000      0  272,000  8,800falsefalseWest Virginia   47,444      0   47,444  3,905falsefalse"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "href": "qmd_root/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "title": "SDIS Summary Analysis",
    "section": "Analysis of Database Totals and Data Quality Issues",
    "text": "Analysis of Database Totals and Data Quality Issues\nExamine states where N_total values reveal potential data quality issues or reporting inconsistencies.\n\n\nShow data quality assessment code\n# Create enhanced data quality analysis\nsdis_enhanced &lt;- sdis_data %&gt;%\n  rename(n_total_reported = n_total)\n\n# Calculate different total relationships with small tolerance\ntolerance &lt;- 10\n\nsdis_enhanced &lt;- sdis_enhanced %&gt;%\n  mutate(\n    total_equals_offenders = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & n_offenders &gt; 0,\n      abs(n_total_reported - n_offenders) &lt;= tolerance,\n      FALSE\n    ),\n    total_equals_off_arr = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & !is.na(n_arrestees) &\n        n_offenders &gt; 0 & n_arrestees &gt; 0,\n      abs(n_total_reported - (n_offenders + n_arrestees)) &lt;= tolerance,\n      FALSE\n    ),\n    total_equals_all = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & !is.na(n_arrestees) & \n        !is.na(n_forensic) & n_offenders &gt; 0 & n_arrestees &gt; 0 & n_forensic &gt; 0,\n      abs(n_total_reported - (n_offenders + n_arrestees + n_forensic)) &lt;= tolerance,\n      FALSE\n    ),\n    total_method = case_when(\n      total_equals_all ~ \"All components\",\n      total_equals_off_arr ~ \"Offenders + Arrestees\",\n      total_equals_offenders ~ \"Offenders only\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# Create n_total_estimated based on the rules specified\nsdis_enhanced &lt;- sdis_enhanced %&gt;%\n  mutate(\n    n_total_estimated = NA_real_,\n    n_total_estimated_comment = \"\"\n  )\n\n# Rule 1: States where n_total == n_offenders + n_arrestees\nmask_off_arr &lt;- sdis_enhanced$total_equals_off_arr\nsdis_enhanced$n_total_estimated[mask_off_arr] &lt;- sdis_enhanced$n_total_reported[mask_off_arr]\nsdis_enhanced$n_total_estimated_comment[mask_off_arr] &lt;- \"Used reported total (matches offenders + arrestees)\"\n\n# Rule 2: States where n_total == n_offenders + n_arrestees + n_forensic\nmask_all &lt;- sdis_enhanced$total_equals_all\nsdis_enhanced$n_total_estimated[mask_all] &lt;- sdis_enhanced$n_total_reported[mask_all] - sdis_enhanced$n_forensic[mask_all]\nsdis_enhanced$n_total_estimated_comment[mask_all] &lt;- \"Subtracted forensic from reported total\"\n\n# Rule 3: States where n_total == n_offenders\nmask_off_only &lt;- sdis_enhanced$total_equals_offenders\nsdis_enhanced$n_total_estimated[mask_off_only] &lt;- sdis_enhanced$n_total_reported[mask_off_only]\nsdis_enhanced$n_total_estimated_comment[mask_off_only] &lt;- \"Used reported total (matches offenders only)\"\n\n# Rule 4: For remaining states with n_total\nmask_total_only &lt;- !is.na(sdis_enhanced$n_total_reported) & \n  is.na(sdis_enhanced$n_total_estimated) &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0) &\n  (is.na(sdis_enhanced$n_offenders) | sdis_enhanced$n_offenders == 0) &\n  (is.na(sdis_enhanced$n_forensic) | sdis_enhanced$n_forensic == 0)\n\nsdis_enhanced$n_total_estimated[mask_total_only] &lt;- sdis_enhanced$n_total_reported[mask_total_only]\nsdis_enhanced$n_total_estimated_comment[mask_total_only] &lt;- \"Total only reported (no breakdown available)\"\n\n# Special case: States with n_total and n_forensic only\nmask_total_forensic_only &lt;- !is.na(sdis_enhanced$n_total_reported) & \n  is.na(sdis_enhanced$n_total_estimated) &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0) &\n  (is.na(sdis_enhanced$n_offenders) | sdis_enhanced$n_offenders == 0) &\n  !is.na(sdis_enhanced$n_forensic) & sdis_enhanced$n_forensic &gt; 0\n\nsdis_enhanced$n_total_estimated[mask_total_forensic_only] &lt;- sdis_enhanced$n_total_reported[mask_total_forensic_only]\nsdis_enhanced$n_total_estimated_comment[mask_total_forensic_only] &lt;- \"Total only reported (forensic reported separately)\"\n\n# States with total and some components but unclear calculation\nmask_has_total_unclear &lt;- !is.na(sdis_enhanced$n_total_reported) & is.na(sdis_enhanced$n_total_estimated)\nsdis_enhanced$n_total_estimated[mask_has_total_unclear] &lt;- sdis_enhanced$n_total_reported[mask_has_total_unclear]\nsdis_enhanced$n_total_estimated_comment[mask_has_total_unclear] &lt;- \"Total with discrepancy (calculation unclear)\"\n\n# For states without any total but with offenders and arrestees\nmask_no_total &lt;- is.na(sdis_enhanced$n_total_reported) & \n  !is.na(sdis_enhanced$n_offenders) & sdis_enhanced$n_offenders &gt; 0 &\n  !is.na(sdis_enhanced$n_arrestees) & sdis_enhanced$n_arrestees &gt; 0\n\nsdis_enhanced$n_total_estimated[mask_no_total] &lt;- sdis_enhanced$n_offenders[mask_no_total] + sdis_enhanced$n_arrestees[mask_no_total]\nsdis_enhanced$n_total_estimated_comment[mask_no_total] &lt;- \"Calculated from offenders + arrestees (no total reported)\"\n\n# For states without total but with only offenders &gt; 0\nmask_no_total_off_only &lt;- is.na(sdis_enhanced$n_total_reported) & \n  !is.na(sdis_enhanced$n_offenders) & sdis_enhanced$n_offenders &gt; 0 &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0)\n\nsdis_enhanced$n_total_estimated[mask_no_total_off_only] &lt;- sdis_enhanced$n_offenders[mask_no_total_off_only]\nsdis_enhanced$n_total_estimated_comment[mask_no_total_off_only] &lt;- \"Used offenders count (no total reported, no arrestee data)\"\n\n# Create enhanced data availability matrix with better visualization\navailability_matrix &lt;- sdis_enhanced %&gt;%\n  transmute(\n    State = state,\n    Arrestees = ifelse(!is.na(n_arrestees) & n_arrestees &gt; 0, \"✓\", \"!\"),\n    Offenders = ifelse(!is.na(n_offenders) & n_offenders &gt; 0, \"✓\", \"!\"),\n    Forensic = ifelse(!is.na(n_forensic) & n_forensic &gt; 0, \"✓\", \"!\"),\n    `Total Reported` = ifelse(!is.na(n_total_reported), \"✓\", \"!\"),\n    `Total Method` = case_when(\n      total_method == \"Offenders only\" ~ \"O\",\n      total_method == \"Offenders + Arrestees\" ~ \"O+A\",\n      total_method == \"All components\" ~ \"All\",\n      TRUE ~ \"?\"\n    )\n  )\n\n# Convert to long format for ggplot\navailability_long &lt;- availability_matrix %&gt;%\n  pivot_longer(cols = -State, names_to = \"Field\", values_to = \"Value\") %&gt;%\n  mutate(\n    State = factor(State, levels = sort(unique(State), decreasing = TRUE)),\n    Field = factor(Field, levels = c(\"Arrestees\", \"Offenders\", \"Forensic\", \"Total Reported\", \"Total Method\")),\n    # Create numeric values for coloring\n    NumericValue = case_when(\n      Value == \"✓\" ~ 1,\n      Value == \"!\" ~ 0,\n      Value == \"O\" ~ 2,\n      Value == \"O+A\" ~ 3,\n      Value == \"All\" ~ 4,\n      Value == \"?\" ~ 0,\n      TRUE ~ 0\n    ),\n    # Create display labels\n    DisplayLabel = case_when(\n      Field == \"Total Method\" & Value == \"?\" ~ \"?\",\n      Field == \"Total Method\" ~ Value,\n      TRUE ~ Value\n    )\n  )\n\n# Summary of how n_total_estimated was calculated\ncat(\"n_total_estimated Column Calculation Summary:\\n\")\ncat(\"=\", strrep(\"=\", 48), \"\\n\", sep = \"\")\n\n# Group states by how their n_total_estimated was determined\nestimation_groups &lt;- sdis_enhanced %&gt;%\n  group_by(n_total_estimated_comment) %&gt;%\n  summarise(\n    states = list(state),\n    count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Print summary in organized sections\ncat(\"\\n1. States with n_total reported and matching patterns:\\n\")\nmatching_patterns &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"matches|Subtracted\"))\nfor (i in 1:nrow(matching_patterns)) {\n  group &lt;- matching_patterns[i, ]\n  cat(sprintf(\"   • %s: %d states \\n(%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n2. States with only n_total reported (no breakdown):\\n\")\ntotal_only &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"Total only reported\"))\nfor (i in 1:nrow(total_only)) {\n  group &lt;- total_only[i, ]\n  cat(sprintf(\"   • %s: %d states \\n(%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n3. States without n_total reported (calculated):\\n\")\ncalculated &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"Calculated|Used offenders count\"))\nfor (i in 1:nrow(calculated)) {\n  group &lt;- calculated[i, ]\n  cat(sprintf(\"   • %s: %d states (%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n4. States with unclear calculation patterns:\\n\")\nunclear &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"discrepancy|unclear\"))\nfor (i in 1:nrow(unclear)) {\n  group &lt;- unclear[i, ]\n  cat(sprintf(\"   • %s: %d states (%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n  \n  # Show details for unclear states\n  unclear_states &lt;- sdis_enhanced %&gt;% \n    filter(state %in% unlist(group$states)) %&gt;%\n    select(state, n_total_reported, n_offenders, n_arrestees, n_forensic)\n  \n  for (j in 1:nrow(unclear_states)) {\n    s &lt;- unclear_states[j, ]\n    cat(sprintf(\"      - %s: Total=%s, Offenders=%s, Arrestees=%s, Forensic=%s\\n\",\n                s$state, \n                format(s$n_total_reported %||% 0, big.mark = \",\"),\n                format(s$n_offenders %||% 0, big.mark = \",\"),\n                format(s$n_arrestees %||% 0, big.mark = \",\"),\n                format(s$n_forensic %||% 0, big.mark = \",\")))\n  }\n}\n\n# Create enhanced heatmap with legend at top and x-labels on both top and bottom\np &lt;- ggplot(availability_long, aes(x = Field, y = State, fill = factor(NumericValue))) +\n  geom_tile(color = \"white\", linewidth = 0.8, width = 0.9, height = 0.9) +\n  geom_text(aes(label = DisplayLabel, color = factor(NumericValue)), \n            size = 3.5, fontface = \"bold\", vjust = 0.8) +\n  scale_fill_manual(\n    name = \"Legend\",\n    values = c(\n      \"0\" = \"#FF6B6B\",      # Red/Orange for missing/unknown\n      \"1\" = \"#2E86AB\",      # Blue for available\n      \"2\" = \"#4ECDC4\",      # Teal for Offenders only\n      \"3\" = \"#45B7D1\",      # Light blue for Offenders + Arrestees\n      \"4\" = \"#1A535C\"       # Dark teal for All components\n    ),\n    labels = c(\n      \"0\" = \"Missing/Unknown data\",\n      \"1\" = \"Data available\",\n      \"2\" = \"Total = Offenders only\",\n      \"3\" = \"Total = Offenders + Arrestees\",\n      \"4\" = \"Total = All components\"\n    )\n  ) +\n  scale_color_manual(values = c(\n    \"0\" = \"white\",        # White text on red\n    \"1\" = \"white\",        # White text on blue\n    \"2\" = \"white\",        # White text on teal\n    \"3\" = \"white\",        # White text on light blue\n    \"4\" = \"white\"         # White text on dark teal\n  )) +\n  labs(\n    title = \"Data Field Availability and Total Calculation Method\",\n    subtitle = \"✓ = Available | ! = Missing | O = Offenders only | O+A = Offenders + Arrestees | All = All components\",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"),\n    axis.text.x.top = element_text(angle = 45, hjust = 0, face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 9),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 9),\n    axis.ticks.x = element_line(),\n    axis.ticks.x.top = element_line()\n  ) +\n  guides(color = \"none\") +\n  coord_fixed() +\n  scale_x_discrete(position = \"top\")\n\n# Update the main dataframe for subsequent analyses\nsdis_data &lt;- sdis_enhanced\n\n# Helper function for null coalescing\n`%||%` &lt;- function(a, b) if (!is.null(a) && !is.na(a)) a else b\n\n\nn_total_estimated Column Calculation Summary:\n=================================================\n\n1. States with n_total reported and matching patterns:\n   • Used reported total (matches offenders only): 8 states \n(Connecticut, Idaho, Illinois, Minnesota, Montana, Texas, Washington, West Virginia)\n   • Used reported total (matches offenders + arrestees): 4 states \n(Alaska, North Carolina, Rhode Island, Tennessee)\n\n2. States with only n_total reported (no breakdown):\n   • Total only reported (no breakdown available): 10 states \n(Alabama, Arkansas, Colorado, Florida, Louisiana, Mississippi, Nevada, New Jersey, South Dakota, Virginia)\n   • Total only reported (forensic reported separately): 2 states \n(California, South Carolina)\n\n3. States without n_total reported (calculated):\n   • Calculated from offenders + arrestees (no total reported): 1 states (Maryland)\n\n4. States with unclear calculation patterns:\n   • Total with discrepancy (calculation unclear): 3 states (Georgia, Indiana, Missouri)\n      - Georgia: Total=347,145, Offenders=324,864, Arrestees=NA, Forensic=22,281\n      - Indiana: Total=449,000, Offenders=304,000, Arrestees=144,000, Forensic=22,400\n      - Missouri: Total=4e+05, Offenders=386,000, Arrestees=NA, Forensic=NA\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow summary visualization code\n# Create summary data\nsummary_data &lt;- sdis_data %&gt;%\n  mutate(\n    data_category = case_when(\n      str_detect(n_total_estimated_comment, \"matches offenders only\") ~ \"Matches Offenders Only\",\n      str_detect(n_total_estimated_comment, \"matches offenders \\\\+ arrestees\") ~ \"Matches Offenders + Arrestees\",\n      str_detect(n_total_estimated_comment, \"Total only reported \\\\(no breakdown\\\\)\") ~ \"Total Only (No Breakdown)\",\n      str_detect(n_total_estimated_comment, \"Total only reported \\\\(forensic\") ~ \"Total Only (Forensic Separate)\",\n      str_detect(n_total_estimated_comment, \"Calculated from offenders \\\\+ arrestees\") ~ \"Calculated (Offenders + Arrestees)\",\n      str_detect(n_total_estimated_comment, \"Used offenders count\") ~ \"Calculated (Offenders Only)\",\n      str_detect(n_total_estimated_comment, \"discrepancy|unclear\") ~ \"Unclear Calculation\",\n      TRUE ~ \"Other\"\n    ),\n    has_arrestees = !is.na(n_arrestees) & n_arrestees &gt; 0,\n    has_offenders = !is.na(n_offenders) & n_offenders &gt; 0,\n    has_forensic = !is.na(n_forensic) & n_forensic &gt; 0,\n    has_total_reported = !is.na(n_total_reported)\n  )\n\n# Create a summary table by category\ncategory_summary &lt;- summary_data %&gt;%\n  group_by(data_category) %&gt;%\n  summarise(\n    count = n(),\n    states = paste(sort(state), collapse = \", \"),\n    avg_total = mean(n_total_estimated, na.rm = TRUE),\n    median_total = median(n_total_estimated, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Print summary statistics\ncat(\"SUMMARY STATISTICS\\n\")\ncat(\"=================\\n\\n\")\n\ncat(\"Calculation Methods Distribution:\\n\")\nfor (i in 1:nrow(category_summary)) {\n  cat(sprintf(\"\\n%s: %d states (%s)\\n\", \n              category_summary$data_category[i], \n              category_summary$count[i],\n              category_summary$states[i]))\n}\n\ncat(\"\\nOverall Data Availability:\\n\")\ncat(sprintf(\"States with arrestee data: %d\\n\", sum(summary_data$has_arrestees)))\ncat(sprintf(\"States with offender data: %d\\n\", sum(summary_data$has_offenders)))\ncat(sprintf(\"States with forensic data: %d\\n\", sum(summary_data$has_forensic)))\ncat(sprintf(\"States with total reported: %d\\n\", sum(summary_data$has_total_reported)))\n\n\nSUMMARY STATISTICS\n=================\n\nCalculation Methods Distribution:\n\nOther: 32 states (Alabama, Arizona, Arkansas, Colorado, Delaware, Florida, Hawaii, Iowa, Kansas, Kentucky, Louisiana, Maine, Massachusetts, Michigan, Mississippi, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, South Dakota, Utah, Vermont, Virginia, Wisconsin, Wyoming)\n\nMatches Offenders Only: 8 states (Connecticut, Idaho, Illinois, Minnesota, Montana, Texas, Washington, West Virginia)\n\nMatches Offenders + Arrestees: 4 states (Alaska, North Carolina, Rhode Island, Tennessee)\n\nUnclear Calculation: 3 states (Georgia, Indiana, Missouri)\n\nTotal Only (Forensic Separate): 2 states (California, South Carolina)\n\nCalculated (Offenders + Arrestees): 1 states (Maryland)\n\nOverall Data Availability:\nStates with arrestee data: 6\nStates with offender data: 16\nStates with forensic data: 11\nStates with total reported: 27"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#stacked-bar-chart-dna-profile-composition-by-state",
    "href": "qmd_root/sdis_summary.html#stacked-bar-chart-dna-profile-composition-by-state",
    "title": "SDIS Summary Analysis",
    "section": "Stacked Bar Chart: DNA Profile Composition by State",
    "text": "Stacked Bar Chart: DNA Profile Composition by State\nThis visualization shows the composition of DNA profiles across states, with separate colors for offender, arrestee, and forensic profiles. States without detailed breakdowns show total counts (reported or calculated).\n\n\nShow stacked bar chart code\n# Prepare data for stacked bar chart\nbar_data &lt;- sdis_data %&gt;%\n  mutate(\n    # Determine if state has detailed breakdown\n    has_breakdown = (!is.na(n_offenders) & n_offenders &gt; 0) | \n                    (!is.na(n_arrestees) & n_arrestees &gt; 0) | \n                    (!is.na(n_forensic) & n_forensic &gt; 0),\n    \n    # For states with breakdown, use actual values\n    offenders_display = ifelse(has_breakdown & !is.na(n_offenders), n_offenders, 0),\n    arrestees_display = ifelse(has_breakdown & !is.na(n_arrestees), n_arrestees, 0),\n    forensic_display = ifelse(has_breakdown & !is.na(n_forensic), n_forensic, 0),\n    \n    # For states without breakdown, use total\n    total_reported_display = ifelse(!has_breakdown & !is.na(n_total_reported), \n                                    n_total_reported, 0),\n    total_calculated_display = ifelse(!has_breakdown & is.na(n_total_reported) & \n                                      !is.na(n_total_estimated), \n                                      n_total_estimated, 0)\n  ) %&gt;%\n  filter(!is.na(n_total_estimated) | has_breakdown) %&gt;%\n  select(state, offenders_display, arrestees_display, forensic_display, \n         total_reported_display, total_calculated_display) %&gt;%\n  pivot_longer(cols = -state, names_to = \"profile_type\", values_to = \"count\") %&gt;%\n  filter(count &gt; 0) %&gt;%\n  mutate(\n    profile_type = factor(profile_type, \n                         levels = c(\"offenders_display\", \"arrestees_display\", \n                                   \"forensic_display\", \"total_reported_display\", \n                                   \"total_calculated_display\"),\n                         labels = c(\"Offender Profiles\", \"Arrestee Profiles\", \n                                   \"Forensic Profiles\", \"Total (Reported)\", \n                                   \"Total (Calculated)\"))\n  )\n\n# Calculate total for ordering states\nstate_totals &lt;- bar_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(total = sum(count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(total))\n\n# Reorder states by total count\nbar_data &lt;- bar_data %&gt;%\n  mutate(state = factor(state, levels = state_totals$state))\n\n# Create custom label function for y-axis\nlabel_k_m &lt;- function(x) {\n  ifelse(x &gt;= 1e6, \n         paste0(round(x / 1e6, 1), \"M\"),\n         ifelse(x &gt;= 1e3,\n                paste0(round(x / 1e3, 0), \"k\"),\n                as.character(x)))\n}\n\n# Create stacked bar chart\n# Create stacked bar chart\nggplot(bar_data, aes(x = state, y = count, fill = profile_type)) +\n  geom_bar(stat = \"identity\", width = 0.8, color = \"white\", linewidth = 0.2) +\n  scale_fill_manual(\n    name = \"Profile Type\",\n    values = c(\n      \"Offender Profiles\" = \"#2E86AB\",\n      \"Arrestee Profiles\" = \"#F9A03F\",\n      \"Forensic Profiles\" = \"#2ca02c\",\n      \"Total (Reported)\" = \"#CCCCCC\",\n      \"Total (Calculated)\" = \"#808080\"\n    )\n  ) +\n  scale_y_continuous(\n    labels = label_k_m,\n    breaks = seq(0, max(bar_data %&gt;% group_by(state) %&gt;% summarise(total = sum(count)) %&gt;% pull(total), na.rm = TRUE), \n                 by = 250000)\n  ) +\n  labs(\n    title = \"DNA Profile Composition by State\",\n    subtitle = \"Detailed breakdown where available; gray bars show aggregate totals for states without detailed data\",\n    x = NULL,\n    y = \"Number of Profiles\",\n    caption = \"Source: SDIS Database Analysis\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8, vjust = 1, margin = margin(t = 0)),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 10),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    legend.spacing.x = unit(1, \"cm\"),\n    legend.spacing.y = unit(0.2, \"cm\"),\n    legend.box.spacing = unit(0.2, \"cm\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  guides(fill = guide_legend(nrow = 3, byrow = TRUE))"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#export-enhanced-dataset",
    "href": "qmd_root/sdis_summary.html#export-enhanced-dataset",
    "title": "SDIS Summary Analysis",
    "section": "Export Enhanced Dataset",
    "text": "Export Enhanced Dataset\nExport the enhanced dataset with the new n_total_estimated values and documentation.\n\n\nShow exportation code\n# Prepare final dataset with key columns in logical order\nfinal_columns &lt;- c(\n  'state', \n  'n_total_estimated',\n  'n_total_reported',\n  'n_total_estimated_comment',\n  'total_method',\n  'n_arrestees', \n  'n_offenders', \n  'n_forensic',\n  'arrestee_collection',\n  'fam_search',\n  'collection_statute'\n)\n\n# Select columns that exist in the dataset\navailable_columns &lt;- intersect(final_columns, names(sdis_enhanced))\nsdis_final &lt;- sdis_enhanced %&gt;% select(all_of(available_columns))\n\n# Export to CSV\noutput_path &lt;- file.path(here(\"data\", \"sdis\", \"final\", \"SDIS_cross_section.csv\"))\nwrite_csv(sdis_final, output_path)\ncat(paste(\"Exported enhanced SDIS dataset to:\", output_path, \"\\n\"))\n\n# Display final summary statistics\ncat(\"\\n\\nFinal Summary Statistics:\\n\")\ncat(\"=\", strrep(\"=\", 48), \"\\n\", sep = \"\")\ncat(paste(\"Total states in dataset:\", nrow(sdis_final), \"\\n\"))\ncat(paste(\"States with n_total_estimated:\", sum(!is.na(sdis_final$n_total_estimated)), \"\\n\"))\ncat(paste(\"States with n_total_reported:\", sum(!is.na(sdis_final$n_total_reported)), \"\\n\"))\ncat(paste(\"Total profiles (estimated):\", format(sum(sdis_final$n_total_estimated, na.rm = TRUE), big.mark = \",\"), \"\\n\"))\n\n# Prepare the data for the interactive table\nsummary_table &lt;- sdis_data %&gt;%\n  select(state, n_total_estimated, n_total_reported, n_offenders, n_arrestees, \n         n_forensic, n_total_estimated_comment) %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.na(.), NA, format(., big.mark = \",\", scientific = FALSE))))\n\n# Create interactive table\ndatatable(\n  summary_table,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 20,\n    columnDefs = list(\n      list(className = 'dt-right', targets = 1:5),  # Right-align numeric columns\n      list(className = 'dt-left', targets = c(0, 6))  # Left-align state and comment columns\n    )\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  caption = \"Enhanced Data with Estimated Totals\"\n)\n\n# Create final frozen version (v1.0)\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\n\nfrozen_path &lt;- here(frozen_dir, \"SDIS_cross_section.csv\")\nwrite_csv(sdis_final, frozen_path)\ncat(paste(\"✓ Created frozen version 1.0 at:\", frozen_path, \"\\n\"))\n\n\nExported enhanced SDIS dataset to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/sdis/final/SDIS_cross_section.csv \n\n\nFinal Summary Statistics:\n=================================================\nTotal states in dataset: 50 \nStates with n_total_estimated: 28 \nStates with n_total_reported: 27 \nTotal profiles (estimated): 12,814,222 \n\n\n\n\n\n\n✓ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/v1.0/SDIS_cross_section.csv"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html",
    "href": "qmd_root/ndis_analysis.html",
    "title": "NDIS Database",
    "section": "",
    "text": "Analyze patterns of state and federal participation in NDIS\n\n\nTrack which jurisdictions are actively contributing data and identify any geographic disparities in participation levels.\n\n\nIdentify periods of rapid growth or stagnation in DNA profile submissions\n\n\nDetect acceleration points and plateaus in the expansion of the national DNA database to understand adoption trends.\n\n\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time\n\n\nMonitor the growth trajectory of different profile categories to assess program effectiveness and resource allocation."
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#setup-configuration",
    "href": "qmd_root/ndis_analysis.html#setup-configuration",
    "title": "NDIS Database",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\nThis section prepares the environment for analysis by:\n\nEnsuring all required R packages are available\nLoading the NDIS dataset with proper type specifications\nProviding basic data validation checks\n\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"lubridate\",    # Date-time manipulation\n  \"DT\",           # Interactive tables\n  \"plotly\",       # Interactive visualizations\n  \"leaflet\",      # Geospatial mapping\n  \"kableExtra\",   # Enhanced table formatting\n  \"scales\",       # Axis scaling and formatting\n  \"dlookr\",       # Data validation and diagnostics\n  \"gt\",           # Table generation\n  \"assertr\",      # Data validation and assertions\n  \"flextable\",    # Enhanced table visualization\n  \"ggridges\",     # Ridge plots\n  \"here\",         # File path management\n  \"patchwork\",    # Data visualization  \n  \"scales\",       # Plot aesthetics\n  \"viridis\",      # Color pallete for plots\n  \"ggrepel\"       # Adjust legend location\n  )\n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(lubridate)\n  library(DT)\n  library(plotly)\n  library(leaflet)\n  library(kableExtra)\n  library(scales)\n  library(dlookr)\n  library(gt)\n  library(assertr)\n  library(flextable)\n  library(ggridges)\n  library(here)\n  library(patchwork)\n  library(scales)\n  library(viridis)\n  library(ggrepel)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\n\nif (all(loaded_packages)) {\n  message(\"📚 All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}\n\n\n\nData Import and Validation\n\n\nShow data import code\n# Define expected column structure\nexpected_cols &lt;- cols(\n  timestamp = col_character(),\n  report_month = col_character(),\n  report_year = col_character(),\n  jurisdiction = col_character(),\n  offender_profiles = col_double(),\n  arrestee = col_double(),\n  forensic_profiles = col_double(),\n  ndis_labs = col_double(),\n  investigations_aided = col_double()\n)\n\n# Read data with validation\nndis_data &lt;- read_csv(\n  here::here(\"data\", \"ndis\", \"raw\", \"ndis_outputs\", \"ndis_data_raw.csv\"),\n  col_types = expected_cols\n)"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#datacleaning",
    "href": "qmd_root/ndis_analysis.html#datacleaning",
    "title": "NDIS Database",
    "section": "Dataset Cleaning",
    "text": "Dataset Cleaning\nThis section outlines the preprocessing steps applied to the raw NDIS data before analysis.\nFirst of all, to ensure consistency for analysis, we fixed jurisdiction names that were not correctly scraped in the dataset as we see here:\n\n\n  [1] \"Alabama\"                                                        \n  [2] \"Alabama Alabama\"                                                \n  [3] \"Alabama Stats Alabama\"                                          \n  [4] \"Alaska\"                                                         \n  [5] \"Alaska Alaska\"                                                  \n  [6] \"Alaska Stats Alaska\"                                            \n  [7] \"and Legal profiles at NDIS. Statistics as of April 2025 Alabama\"\n  [8] \"and Legal profiles at NDIS. Statistics as of June 2025 Alabama\" \n  [9] \"and Legal profiles at NDIS. Statistics as of March 2025 Alabama\"\n [10] \"Arizona\"                                                        \n [11] \"Arizona Arizona\"                                                \n [12] \"Arizona Stats Arizona\"                                          \n [13] \"Arkansas\"                                                       \n [14] \"Arkansas Arkansas\"                                              \n [15] \"Arkansas Stats Arkansas\"                                        \n [16] \"Army\"                                                           \n [17] \"California\"                                                     \n [18] \"California California\"                                          \n [19] \"California Stats California\"                                    \n [20] \"Colorado\"                                                       \n [21] \"Colorado Colorado\"                                              \n [22] \"Colorado Stats Colorado\"                                        \n [23] \"Connecticut\"                                                    \n [24] \"Connecticut Connecticut\"                                        \n [25] \"Connecticut Stats Connecticut\"                                  \n [26] \"DC\"                                                             \n [27] \"DC map pin). Statistics as of November 2022 Alabama\"            \n [28] \"DC map pin.) Statistics as of August 2023 Alabama\"              \n [29] \"DC map pin.) Statistics as of February 2024 Alabama\"            \n [30] \"DC map pin.) Statistics as of January 2025 Alabama\"             \n [31] \"DC map pin.) Statistics as of November 2024 Alabama\"            \n [32] \"DC map pin.) Statistics as of October 2024 Alabama\"             \n [33] \"DC/FBI Lab\"                                                     \n [34] \"DC/Metro PD\"                                                    \n [35] \"Delaware\"                                                       \n [36] \"Delaware Delaware\"                                              \n [37] \"Delaware Stats Delaware\"                                        \n [38] \"Florida\"                                                        \n [39] \"Florida Florida\"                                                \n [40] \"Florida Stats Florida\"                                          \n [41] \"Georgia\"                                                        \n [42] \"Georgia Georgia\"                                                \n [43] \"Georgia Stats Georgia\"                                          \n [44] \"Hawaii\"                                                         \n [45] \"Hawaii Hawaii\"                                                  \n [46] \"Hawaii Stats Hawaii\"                                            \n [47] \"Idaho\"                                                          \n [48] \"Idaho Idaho\"                                                    \n [49] \"Illinois\"                                                       \n [50] \"Illinois Illinois\"                                              \n [51] \"Illinois Stats Illinois\"                                        \n [52] \"Indiana\"                                                        \n [53] \"Indiana Indiana\"                                                \n [54] \"Indiana Stats Indiana\"                                          \n [55] \"Iowa\"                                                           \n [56] \"Iowa Iowa\"                                                      \n [57] \"Iowa Stats Iowa\"                                                \n [58] \"Kansas\"                                                         \n [59] \"Kansas Kansas\"                                                  \n [60] \"Kansas Stats Kansas\"                                            \n [61] \"Kentucky\"                                                       \n [62] \"Kentucky Kentucky\"                                              \n [63] \"Kentucky Stats Kentucky\"                                        \n [64] \"Lab\"                                                            \n [65] \"Louisiana\"                                                      \n [66] \"Louisiana Louisiana\"                                            \n [67] \"Louisiana Stats Louisiana\"                                      \n [68] \"Maine\"                                                          \n [69] \"Maine Maine\"                                                    \n [70] \"Maine Stats Maine\"                                              \n [71] \"Maryland\"                                                       \n [72] \"Maryland Maryland\"                                              \n [73] \"Maryland Stats Maryland\"                                        \n [74] \"Massachusetts\"                                                  \n [75] \"Massachusetts Massachusetts\"                                    \n [76] \"Massachusetts Stats Massachusetts\"                              \n [77] \"Mexico Stats New Mexico\"                                        \n [78] \"Michigan\"                                                       \n [79] \"Michigan Michigan\"                                              \n [80] \"Michigan Stats Idaho\"                                           \n [81] \"Michigan Stats Michigan\"                                        \n [82] \"Michigan Stats Utah\"                                            \n [83] \"Minnesota\"                                                      \n [84] \"Minnesota Minnesota\"                                            \n [85] \"Minnesota Stats Minnesota\"                                      \n [86] \"Mississippi\"                                                    \n [87] \"Mississippi Mississippi\"                                        \n [88] \"Mississippi Stats Mississippi\"                                  \n [89] \"Missouri\"                                                       \n [90] \"Missouri Missouri\"                                              \n [91] \"Missouri Stats Missouri\"                                        \n [92] \"Montana\"                                                        \n [93] \"Montana Montana\"                                                \n [94] \"Montana Stats Montana\"                                          \n [95] \"Nebraska\"                                                       \n [96] \"Nebraska Nebraska\"                                              \n [97] \"Nebraska Stats Nebraska\"                                        \n [98] \"Nevada\"                                                         \n [99] \"Nevada Nevada\"                                                  \n[100] \"Nevada Stats Nevada\"                                            \n[101] \"New Hampshire\"                                                  \n[102] \"New Hampshire New Hampshire\"                                    \n[103] \"New Hampshire Stats New Hampshire\"                              \n[104] \"New Jersey\"                                                     \n[105] \"New Jersey New Jersey\"                                          \n[106] \"New Jersey Stats New Jersey\"                                    \n[107] \"New Mexico\"                                                     \n[108] \"New Mexico New Mexico\"                                          \n[109] \"New York\"                                                       \n[110] \"New York New York\"                                              \n[111] \"New York Stats New York\"                                        \n[112] \"North Carolina\"                                                 \n[113] \"North Carolina North Carolina\"                                  \n[114] \"North Carolina Stats North Carolina\"                            \n[115] \"North Dakota\"                                                   \n[116] \"North Dakota North Dakota\"                                      \n[117] \"North Dakota Stats North Dakota\"                                \n[118] \"Ohio\"                                                           \n[119] \"Ohio Ohio\"                                                      \n[120] \"Ohio Stats Ohio\"                                                \n[121] \"Oklahoma\"                                                       \n[122] \"Oklahoma Oklahoma\"                                              \n[123] \"Oklahoma Stats Oklahoma\"                                        \n[124] \"Oregon\"                                                         \n[125] \"Oregon Oregon\"                                                  \n[126] \"Oregon Stats Oregon\"                                            \n[127] \"Participant Alabama\"                                            \n[128] \"Pennsylvania\"                                                   \n[129] \"Pennsylvania Pennsylvania\"                                      \n[130] \"Pennsylvania Stats Pennsylvania\"                                \n[131] \"PR\"                                                             \n[132] \"Puerto Rico\"                                                    \n[133] \"Rhode Island\"                                                   \n[134] \"Rhode Island Rhode Island\"                                      \n[135] \"Rhode Island Stats Rhode Island\"                                \n[136] \"South Carolina\"                                                 \n[137] \"South Carolina South Carolina\"                                  \n[138] \"South Carolina Stats South Carolina\"                            \n[139] \"South Dakota\"                                                   \n[140] \"South Dakota South Dakota\"                                      \n[141] \"South Dakota Stats South Dakota\"                                \n[142] \"Tables by NDIS Participant Alabama\"                             \n[143] \"Tennessee\"                                                      \n[144] \"Tennessee Stats Tennessee\"                                      \n[145] \"Tennessee Tennessee\"                                            \n[146] \"Texas\"                                                          \n[147] \"Texas Stats Texas\"                                              \n[148] \"Texas Texas\"                                                    \n[149] \"U.S. Army\"                                                      \n[150] \"Utah\"                                                           \n[151] \"Utah Utah\"                                                      \n[152] \"Vermont\"                                                        \n[153] \"Vermont Stats Vermont\"                                          \n[154] \"Vermont Vermont\"                                                \n[155] \"Virginia\"                                                       \n[156] \"Virginia Stats Virginia\"                                        \n[157] \"Virginia Virginia\"                                              \n[158] \"Washington\"                                                     \n[159] \"Washington State Stats Washington\"                              \n[160] \"Washington State Washington\"                                    \n[161] \"West Virginia\"                                                  \n[162] \"West Virginia Stats West Virginia\"                              \n[163] \"West Virginia Stats Wyoming\"                                    \n[164] \"West Virginia West Virginia\"                                    \n[165] \"Wisconsin\"                                                      \n[166] \"Wisconsin Stats Wisconsin\"                                      \n[167] \"Wisconsin Wisconsin\"                                            \n[168] \"Wyoming\"                                                        \n[169] \"Wyoming Wyoming\"                                                \n\n\n\n\nShow cleaning code (jurisdiction)\n# Clean jurisdiction names with Alabama-specific patterns\nndis_data_jurisdiction &lt;- ndis_data %&gt;%\n  mutate(\n    jurisdiction = case_when(\n      # Standard state names\n      str_detect(jurisdiction, \"Alabama$|Alabama Stats\") ~ \"Alabama\",\n      str_detect(jurisdiction, \"Alaska$|Alaska Stats\") ~ \"Alaska\",\n      str_detect(jurisdiction, \"Arizona$|Arizona Stats\") ~ \"Arizona\",\n      str_detect(jurisdiction, \"Arkansas$|Arkansas Stats\") ~ \"Arkansas\",\n      str_detect(jurisdiction, \"California$|California Stats\") ~ \"California\",\n      str_detect(jurisdiction, \"Colorado$|Colorado Stats\") ~ \"Colorado\",\n      str_detect(jurisdiction, \"Connecticut$|Connecticut Stats\") ~ \"Connecticut\",\n      str_detect(jurisdiction, \"Delaware$|Delaware Stats\") ~ \"Delaware\",\n      str_detect(jurisdiction, \"Florida$|Florida Stats\") ~ \"Florida\",\n      str_detect(jurisdiction, \"Georgia$|Georgia Stats\") ~ \"Georgia\",\n      str_detect(jurisdiction, \"Hawaii$|Hawaii Stats\") ~ \"Hawaii\",\n      str_detect(jurisdiction, \"Idaho$|Idaho Stats\") ~ \"Idaho\",\n      str_detect(jurisdiction, \"Illinois$|Illinois Stats\") ~ \"Illinois\",\n      str_detect(jurisdiction, \"Indiana$|Indiana Stats\") ~ \"Indiana\",\n      str_detect(jurisdiction, \"Iowa$|Iowa Stats\") ~ \"Iowa\",\n      str_detect(jurisdiction, \"Kansas$|Kansas Stats\") ~ \"Kansas\",\n      str_detect(jurisdiction, \"Kentucky$|Kentucky Stats\") ~ \"Kentucky\",\n      str_detect(jurisdiction, \"Louisiana$|Louisiana Stats\") ~ \"Louisiana\",\n      str_detect(jurisdiction, \"Maine$|Maine Stats\") ~ \"Maine\",\n      str_detect(jurisdiction, \"Maryland$|Maryland Stats\") ~ \"Maryland\",\n      str_detect(jurisdiction, \"Massachusetts$|Massachusetts Stats\") ~ \"Massachusetts\",\n      str_detect(jurisdiction, \"Michigan$|Michigan Stats\") ~ \"Michigan\",\n      str_detect(jurisdiction, \"Minnesota$|Minnesota Stats\") ~ \"Minnesota\",\n      str_detect(jurisdiction, \"Mississippi$|Mississippi Stats\") ~ \"Mississippi\",\n      str_detect(jurisdiction, \"Missouri$|Missouri Stats\") ~ \"Missouri\",\n      str_detect(jurisdiction, \"Montana$|Montana Stats\") ~ \"Montana\",\n      str_detect(jurisdiction, \"Nebraska$|Nebraska Stats\") ~ \"Nebraska\",\n      str_detect(jurisdiction, \"Nevada$|Nevada Stats\") ~ \"Nevada\",\n      str_detect(jurisdiction, \"New Hampshire$|New Hampshire Stats\") ~ \"New Hampshire\",\n      str_detect(jurisdiction, \"New Jersey$|New Jersey Stats\") ~ \"New Jersey\",\n      str_detect(jurisdiction, \"New Mexico$|New Mexico Stats|Mexico Stats\") ~ \"New Mexico\",\n      str_detect(jurisdiction, \"New York$|New York Stats\") ~ \"New York\",\n      str_detect(jurisdiction, \"North Carolina$|North Carolina Stats\") ~ \"North Carolina\",\n      str_detect(jurisdiction, \"North Dakota$|North Dakota Stats\") ~ \"North Dakota\",\n      str_detect(jurisdiction, \"Ohio$|Ohio Stats\") ~ \"Ohio\",\n      str_detect(jurisdiction, \"Oklahoma$|Oklahoma Stats\") ~ \"Oklahoma\",\n      str_detect(jurisdiction, \"Oregon$|Oregon Stats\") ~ \"Oregon\",\n      str_detect(jurisdiction, \"Pennsylvania$|Pennsylvania Stats\") ~ \"Pennsylvania\",\n      str_detect(jurisdiction, \"Rhode Island$|Rhode Island Stats\") ~ \"Rhode Island\",\n      str_detect(jurisdiction, \"South Carolina$|South Carolina Stats\") ~ \"South Carolina\",\n      str_detect(jurisdiction, \"South Dakota$|South Dakota Stats\") ~ \"South Dakota\",\n      str_detect(jurisdiction, \"Tennessee$|Tennessee Stats\") ~ \"Tennessee\",\n      str_detect(jurisdiction, \"Texas$|Texas Stats\") ~ \"Texas\",\n      str_detect(jurisdiction, \"Utah$|Utah Stats\") ~ \"Utah\",\n      str_detect(jurisdiction, \"Vermont$|Vermont Stats\") ~ \"Vermont\",\n      str_detect(jurisdiction, \"West Virginia$|West Virginia Stats\") ~ \"West Virginia\",\n      str_detect(jurisdiction, \"Virginia$|Virginia Stats\") ~ \"Virginia\",\n      str_detect(jurisdiction, \"Washington$|Washington State Stats\") ~ \"Washington\",\n      str_detect(jurisdiction, \"Wisconsin$|Wisconsin Stats\") ~ \"Wisconsin\",\n      str_detect(jurisdiction, \"Wyoming$|Wyoming Stats\") ~ \"Wyoming\",\n      \n      # Special jurisdictions\n      str_detect(jurisdiction, \"DC/FBI|Washington DC Stats|Lab\") ~ \"DC/FBI Lab\",\n      str_detect(jurisdiction, \"DC/Metro|DC\") ~ \"DC/Metro PD\",\n      str_detect(jurisdiction, \"U.S. Army$|U.S. Army Stats\") ~ \"U.S. Army\",\n            str_detect(jurisdiction, \"Puerto Rico$|Puerto Rico Stats\") ~ \"Puerto Rico\",\n          \n      str_detect(jurisdiction, \"Tables by NDIS Participant\") ~ \"Alabama\", # Default to Alabama\n      \n      TRUE ~ jurisdiction\n    ),\n    \n    # Clean up any remaining whitespace\n    jurisdiction = str_trim(jurisdiction)\n     )  %&gt;%\n  \n  # Convert to factor with the 54 levels you want\n  mutate(\n    jurisdiction = factor(jurisdiction,\n                         levels = c(sort(state.name), \"Puerto Rico\", \"DC/FBI Lab\", \"DC/Metro PD\", \"U.S. Army\"))) %&gt;%\n  \n  # Filter out NA jurisdictions\n  filter(!is.na(jurisdiction))\n\n\nUpdated Jurisdiction names:\n\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"        \"Puerto Rico\"    \"DC/FBI Lab\"    \n[53] \"DC/Metro PD\"    \"U.S. Army\"     \n\n\nRecords from jurisdictions with 0 laboratories were removed, and variables were reformatted into consistent date and time structures.\nKey profile counts were combined into a total_profiles measure, and missing reporting periods were filled using available capture information.\nFinally, year and month variables were standardized, and the dataset was reordered to ensure a clean, consistent structure for validation and analysis.\n\n\nShow cleaning code (general)\nndis_data &lt;- ndis_data_jurisdiction %&gt;%\n  mutate(\n    capture_datetime = as_datetime(timestamp, format = \"%Y%m%d%H%M%S\"),\n    capture_year = year(capture_datetime),\n    capture_month = month(capture_datetime),\n    capture_day = day(capture_datetime),\n    total_profiles = offender_profiles + arrestee + forensic_profiles,\n    asof_month = report_month,\n    asof_year = report_year\n  ) %&gt;%\n  mutate(\n    year = ifelse(is.na(asof_year), capture_year, asof_year),\n    month = ifelse(is.na(asof_month), capture_month, asof_month),\n    year = as.integer(year),\n    month = as.integer(month),\n    year_month = paste(year, str_pad(month, 2, pad = \"0\"), sep = \"-\"),\n    year_month = ymd(paste0(year_month, \"-01\")),\n    year = as.factor(year),\n    month = as.factor(month)\n  )\n\nndis_intermediate &lt;- ndis_data %&gt;%\n  filter(ndis_labs &gt; 0) %&gt;%\n  select(\n    capture_datetime, asof_month, month, asof_year,\n    year, year_month, jurisdiction, offender_profiles, arrestee,\n    forensic_profiles, total_profiles, ndis_labs,\n    investigations_aided\n  ) %&gt;%\n  arrange(jurisdiction, year_month, desc(investigations_aided), desc(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year_month) %&gt;%\n  slice(1) %&gt;%\n  ungroup()\n\n\n\nSaving Intermediate Cleaned Data\nThe cleaned dataset preserves the core NDIS metrics while standardizing temporal and jurisdictional dimensions for consistent analysis. Key structural improvements include:\n· Temporal Standardization: Unified date handling with capture_datetime for data extraction timing and asof_month/asof_year for reported periods\n· Jurisdictional Harmonization: Normalized 54 jurisdiction names (50 states + Puerto Rico, DC/FBI Lab, DC/Metro PD, U.S. Army) using consistent naming conventions\n· Derived Metrics: Added total_profiles as the sum of offender, arrestee, and forensic profiles for comprehensive trend analysis\n· Data Integrity: Removed ambiguous records and ensured proper typing for analytical operations\n\n\nShow intermediate dataset saving code\n# Glimpse\n\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    First_Values = sapply(df, function(x) {\n      if(is.numeric(x)) {\n        paste(round(head(x, 3), 2), collapse = \", \")\n      } else {\n        paste(encodeString(head(as.character(x), 3)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows ×\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\nenhanced_glimpse(ndis_intermediate)\n\n\nColumnTypeRowsMissingUniqueFirst_Valuescapture_datetimePOSIXct, POSIXt9442032822001-07-15 04:15:59, 2001-08-22 11:55:31, 2001-09-14 21:16:11asof_monthcharacter9442316413&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;monthfactor94420127, 8, 9asof_yearcharacter9442316419&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;yearfactor94420252001, 2001, 2001year_monthDate944201972001-07-01, 2001-08-01, 2001-09-01jurisdictionfactor9442054Alabama, Alabama, Alabamaoffender_profilesnumeric9442073950, 0, 0arresteenumeric9442025150, 0, 0forensic_profilesnumeric9442062140, 0, 0total_profilesnumeric9442080700, 0, 0ndis_labsnumeric94420224, 4, 4investigations_aidednumeric94420423588, 88, 88Data frame dimensions: 9442 rows × 13 columns\n\n\nShow intermediate dataset saving code\n# Save cleaned data to CSV\nwrite_csv(ndis_intermediate, here::here(\"data\", \"ndis\", \"intermediate\", \"ndis_data_intermediate.csv\"))\n\nmessage(\"✅ Intermediate dataset saved to 'data/ndis/intermediate' folder\")\n\n\n\n\nVariables Growth and Corrections\nThe National DNA Index System (NDIS) data for each jurisdiction is expected to show consistent growth over time. However, reporting issues can create anomalies that need correction. This section documents our validation and cleaning approach.\nTechnical Validation Framework\nWe use multiple detection methods:\n· Monthly Change Analysis: Track increases/decreases between periods\n· Monotonicity Checks: Identify unexpected drops in cumulative data\n· Spike Detection: Flag rapid increases followed by immediate decreases\n· Dip Detection: Find values much lower than neighboring points\n· Time Gap Analysis: Account for legitimate reporting delays (&gt;100 days)\nDetection thresholds:\n· Decreases: Any negative change between months\n· Spikes: Values &gt;5× previous with &gt;80% subsequent drop\n· Dips: Values &lt;50% of neighboring observations\n· Time gaps: &gt;100 days between reports\nData Correction Process\nOur systematic approach:\n\nFlag anomalies using automated validation rules\nVisual verification with interactive plots\nRemove flagged points while preserving valid data\nVerify results with post-cleaning checks\n\n\nConvicted Offender Profiles\nValidation Approach\n· Strict monotonic growth expected (should only increase)\n· Three detection methods (flags):\n· Decrease detection for drops\n· Spike detection for temporary data surges\n· Dip detection for sudden low values\n· Accounts for legitimate reporting gaps\nCorrection Methods:\n· Remove all flagged anomalies\n· Two iterative cleaning passes for thoroughness\n· Yearly aggregation to verify growth trends\n· Preserves jurisdiction-level patterns\n\n\nShow Offender profiles visualization and correction code\n#### Raw Offender profiles plot\n\n# Flag anomalies for offender profiles with improved detection\noffender_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(offender_profiles),\n    next_value = lead(offender_profiles),\n    prev2_value = lag(offender_profiles, 2),\n    next2_value = lead(offender_profiles, 2),\n    \n    delta_offender = offender_profiles - prev_value,\n    delta_next = next_value - offender_profiles,\n    \n    # Check for time gaps (more than 3 months between observations)\n    time_gap = as.numeric(difftime(year_month, lag(year_month), units = \"days\")) &gt; 100,\n    time_gap_next = as.numeric(difftime(lead(year_month), year_month, units = \"days\")) &gt; 100,\n    \n    # Flag 1: Simple decrease (but not after a time gap)\n    flag_decrease = delta_offender &lt; 0 & !time_gap,\n    \n    # Flag 2: Spike detection - sharp up then sharp down\n    flag_spike = !is.na(delta_offender) & !is.na(delta_next) &\n                 delta_offender &gt; 0 & delta_next &lt; 0 &\n                 offender_profiles &gt; 5 * prev_value &\n                 abs(delta_next) &gt; (0.8 * offender_profiles) &\n                 !time_gap & !time_gap_next,\n    \n    # Flag 3: Dip detection - value is much lower than neighbors (checking further out too)\n    flag_dip = (\n      (!is.na(prev_value) & offender_profiles &lt; (0.5 * prev_value)) |\n      (!is.na(prev2_value) & offender_profiles &lt; (0.5 * prev2_value))\n    ) & (\n      (!is.na(next_value) & offender_profiles &lt; (0.5 * next_value)) |\n      (!is.na(next2_value) & offender_profiles &lt; (0.5 * next2_value))\n    ),\n    \n    # Combine all flags\n    flag_any = flag_decrease | flag_spike | flag_dip,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for offender profiles with flagged points\np_offender_raw &lt;- offender_validation %&gt;%\n  plot_ly(x = ~year_month, y = ~offender_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = offender_validation %&gt;% filter(flag_any),\n              x = ~year_month, y = ~offender_profiles, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Convicted Offender Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Offender Profiles\"))\n\np_offender_raw\n\n\n\n\n\n\nShow Offender profiles visualization and correction code\n#### Offender Profiles Correction #####\n\n# Create cleaned offender data\noffender_clean &lt;- offender_validation %&gt;%\n  filter(!flag_any) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"delta_\"), \n         -starts_with(\"time_gap\"), -prev_value, -next_value, \n         -prev2_value, -next2_value)\n\n# Additional filters: remove any remaining decreases\n# First batch\noffender_clean &lt;- offender_clean %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    delta_check = offender_profiles - lag(offender_profiles),\n    has_decrease = delta_check &lt; 0 & !is.na(delta_check)\n  ) %&gt;%\n  filter(!has_decrease) %&gt;%\n  select(-delta_check, -has_decrease) %&gt;%\n  ungroup()\n\n# Second batch for remaining decreases (necessary because of Florida and Texas, manually observed)\noffender_clean &lt;- offender_clean %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    delta_check = offender_profiles - lag(offender_profiles),\n    has_decrease = delta_check &lt; 0 & !is.na(delta_check)\n  ) %&gt;%\n  filter(!has_decrease) %&gt;%\n  select(-delta_check, -has_decrease) %&gt;%\n  ungroup()\n\n# Plot cleaned data\np_offender_clean &lt;- offender_clean %&gt;%\n  plot_ly(x = ~year_month, y = ~offender_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Convicted Offender Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Offender Profiles\"))\n\np_offender_clean\n\n\n\n\n\n\nShow Offender profiles visualization and correction code\n# Summarise highest offender profile per jurisdiction per year\noffender_yearly &lt;- offender_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_offender = max(offender_profiles, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_offender = sum(max_offender, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_offender_yearly &lt;- offender_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_offender,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"steelblue\", width = 3),\n          marker = list(size = 8, color = \"darkred\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Offender Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Offender Profiles\"))\n\np_offender_yearly\n\n\n\n\n\n\n\n\nForensic Profiles\nValidation Approach:\n· Similar growth patterns to offender profiles\n· Three detection methods (flags):\n· Decrease detection for drops\n· Spike detection for temporary data surges\n· Dip detection for sudden low values\n· Considers case backlog processing patterns\nCorrection Methods:\n· Single-pass anomaly filtering\n· Post-cleaning monotonicity check\n· Yearly maximum aggregation\n· Maintains legitimate growth trends\n\n\nShow Forensic profiles visualization and correction code\n#### Raw Forensic profiles plot\n\n# Flag anomalies for forensic profiles with improved detection\nforensic_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(forensic_profiles),\n    next_value = lead(forensic_profiles),\n    prev2_value = lag(forensic_profiles, 2),\n    next2_value = lead(forensic_profiles, 2),\n    \n    delta_forensic = forensic_profiles - prev_value,\n    delta_next = next_value - forensic_profiles,\n    \n    # Check for time gaps (more than 3 months between observations)\n    time_gap = as.numeric(difftime(year_month, lag(year_month), units = \"days\")) &gt; 100,\n    time_gap_next = as.numeric(difftime(lead(year_month), year_month, units = \"days\")) &gt; 100,\n    \n    # Flag 1: Simple decrease (but not after a time gap)\n    flag_decrease = delta_forensic &lt; 0 & !time_gap,\n    \n    # Flag 2: Spike detection - sharp up then sharp down\n    flag_spike = !is.na(delta_forensic) & !is.na(delta_next) &\n                 delta_forensic &gt; 0 & delta_next &lt; 0 &\n                 forensic_profiles &gt; 5 * prev_value &\n                 abs(delta_next) &gt; (0.8 * forensic_profiles) &\n                 !time_gap & !time_gap_next,\n    \n    # Flag 3: Dip detection - value is much lower than neighbors (checking further out too)\n    flag_dip = (\n      (!is.na(prev_value) & forensic_profiles &lt; (0.5 * prev_value)) |\n      (!is.na(prev2_value) & forensic_profiles &lt; (0.5 * prev2_value))\n    ) & (\n      (!is.na(next_value) & forensic_profiles &lt; (0.5 * next_value)) |\n      (!is.na(next2_value) & forensic_profiles &lt; (0.5 * next2_value))\n    ),\n    \n    # Combine all flags\n    flag_any = flag_decrease | flag_spike | flag_dip,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for forensic profiles with flagged points\np_forensic_raw &lt;- forensic_validation %&gt;%\n  plot_ly(x = ~year_month, y = ~forensic_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = forensic_validation %&gt;% filter(flag_any),\n              x = ~year_month, y = ~forensic_profiles, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Forensic Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Forensic Profiles\"))\n\np_forensic_raw\n\n\n\n\n\n\nShow Forensic profiles visualization and correction code\n#### Forensic Profiles Correction #####\n\n# Create cleaned forensic data\nforensic_clean &lt;- forensic_validation %&gt;%\n  filter(!flag_any) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"delta_\"), \n         -starts_with(\"time_gap\"), -prev_value, -next_value, \n         -prev2_value, -next2_value)\n\n# Additional filters: remove any remaining decreases\n# First batch\nforensic_clean &lt;- forensic_clean %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    delta_check = forensic_profiles - lag(forensic_profiles),\n    has_decrease = delta_check &lt; 0 & !is.na(delta_check)\n  ) %&gt;%\n  filter(!has_decrease) %&gt;%\n  select(-delta_check, -has_decrease) %&gt;%\n  ungroup()\n\n# Plot cleaned data\np_forensic_clean &lt;- forensic_clean %&gt;%\n  plot_ly(x = ~year_month, y = ~forensic_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Forensic Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Forensic Profiles\"))\n\np_forensic_clean\n\n\n\n\n\n\nShow Forensic profiles visualization and correction code\n# Summarise highest forensic profile per jurisdiction per year\nforensic_yearly &lt;- forensic_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%  # extract year\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_forensic = max(forensic_profiles, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_forensic = sum(max_forensic, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_forensic_yearly &lt;- forensic_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_forensic,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"darkgreen\", width = 3),\n          marker = list(size = 8, color = \"orange\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Forensic Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Forensic Profiles\"))\n\np_forensic_yearly\n\n\n\n\n\n\n\n\nArrestee Profiles\nValidation Approach:\n· Data validated from January 2012 onward\n· Accounts for program rollout patterns\n· Same core detection methods as other profiles\n· Focuses on major anomalies, not minor fluctuations\nCorrection Methods:\n· Targeted removal of significant violations\n· Preserves legitimate program growth\n· Single cleaning pass sufficient\n· Recognizes different growth characteristics\n\n\nShow Arrestee profiles visualization and correction code\n#### Raw Arrestee profiles plot\n\n# Flag anomalies for arrestee profiles with improved detection\narrestee_validation &lt;- ndis_intermediate %&gt;%\n  filter(year_month &gt;= as.Date(\"2012-01-01\")) %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(arrestee),\n    next_value = lead(arrestee),\n    prev2_value = lag(arrestee, 2),\n    next2_value = lead(arrestee, 2),\n    \n    delta_arrestee = arrestee - prev_value,\n    delta_next = next_value - arrestee,\n    \n    # Check for time gaps (more than 3 months between observations)\n    time_gap = as.numeric(difftime(year_month, lag(year_month), units = \"days\")) &gt; 100,\n    time_gap_next = as.numeric(difftime(lead(year_month), year_month, units = \"days\")) &gt; 100,\n    \n    # Flag 1: Simple decrease (but not after a time gap)\n    flag_decrease = delta_arrestee &lt; 0 & !time_gap,\n    \n    # Flag 2: Spike detection - sharp up then sharp down\n    flag_spike = !is.na(delta_arrestee) & !is.na(delta_next) &\n                 delta_arrestee &gt; 0 & delta_next &lt; 0 &\n                 arrestee &gt; 5 * prev_value &\n                 abs(delta_next) &gt; (0.8 * arrestee) &\n                 !time_gap & !time_gap_next,\n    \n    # Flag 3: Dip detection - value is much lower than neighbors (checking further out too)\n    flag_dip = (\n      (!is.na(prev_value) & arrestee &lt; (0.5 * prev_value)) |\n      (!is.na(prev2_value) & arrestee &lt; (0.5 * prev2_value))\n    ) & (\n      (!is.na(next_value) & arrestee &lt; (0.5 * next_value)) |\n      (!is.na(next2_value) & arrestee &lt; (0.5 * next2_value))\n    ),\n    \n    # Combine all flags\n    flag_any = flag_decrease | flag_spike | flag_dip,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for arrestee profiles with flagged points\np_arrestee_raw &lt;- arrestee_validation %&gt;%\n  plot_ly(x = ~year_month, y = ~arrestee, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = arrestee_validation %&gt;% filter(flag_any),\n              x = ~year_month, y = ~arrestee, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Arrestee Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Arrestee Profiles\"))\n\np_arrestee_raw\n\n\n\n\n\n\nShow Arrestee profiles visualization and correction code\n#### Arrestee Profiles Correction #####\n\n# Create cleaned arrestee data\narrestee_clean &lt;- arrestee_validation %&gt;%\n  filter(!flag_any) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"delta_\"), \n         -starts_with(\"time_gap\"), -prev_value, -next_value, \n         -prev2_value, -next2_value)\n\n# Plot cleaned data\np_arrestee_clean &lt;- arrestee_clean %&gt;%\n  plot_ly(x = ~year_month, y = ~arrestee, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Arrestee Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Arrestee Profiles\"))\n\np_arrestee_clean\n\n\n\n\n\n\nShow Arrestee profiles visualization and correction code\n# Summarise highest arrestee profile per jurisdiction per year\narrestee_yearly &lt;- arrestee_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;% \n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_arrestee = max(arrestee, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_arrestee = sum(max_arrestee, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_arrestee_yearly &lt;- arrestee_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_arrestee,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"purple\", width = 3),\n          marker = list(size = 8, color = \"magenta\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Arrestee Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Arrestee Profiles\"))\n\np_arrestee_yearly\n\n\n\n\n\n\n\n\nInvestigations Aided\nValidation Approach:\n· Same core detection methods as DNA profiles\n· Priority on extreme spikes and magnitude outliers\n· Separate detection for 5× magnitude differences\n· Recognizes legitimate investigative surges\nCorrection Methods:\n· Focuses on clear data errors\n· Preserves operational fluctuations\n· Removes extreme outliers\n· Maintains metric relevance\nExample: California 2024 Typo\nCalifornia 2024 Data Typo\n\n\nShow Investigations Aided visualization and correction code\n#### Raw Investigations Aided plot\n\n# Flag anomalies for investigations aided with improved detection\ninvestigations_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(investigations_aided),\n    next_value = lead(investigations_aided),\n    prev2_value = lag(investigations_aided, 2),\n    next2_value = lead(investigations_aided, 2),\n    \n    delta_investigations = investigations_aided - prev_value,\n    delta_next = next_value - investigations_aided,\n    \n    # Check for time gaps (more than 3 months between observations)\n    time_gap = as.numeric(difftime(year_month, lag(year_month), units = \"days\")) &gt; 100,\n    time_gap_next = as.numeric(difftime(lead(year_month), year_month, units = \"days\")) &gt; 100,\n    \n    # Flag spikes and magnitude outliers FIRST (before other flags)\n    # Flag: Spike detection - sharp up then sharp down\n    flag_spike = !is.na(delta_investigations) & !is.na(delta_next) &\n                 delta_investigations &gt; 0 & delta_next &lt; 0 &\n                 (investigations_aided &gt; 3 * prev_value |\n                  abs(delta_next) &gt; (0.9 * investigations_aided)) &\n                 !time_gap & !time_gap_next,\n    \n    # Flag: Order of magnitude outlier\n    flag_magnitude = !is.na(prev_value) & !is.na(next_value) &\n                     investigations_aided &gt; 5 * prev_value &\n                     investigations_aided &gt; 5 * next_value,\n    \n    # Check if previous value was a spike/magnitude outlier\n    prev_was_outlier = lag(flag_spike) | lag(flag_magnitude),\n    \n    # Flag 1: Simple decrease (but not after a time gap OR after an outlier)\n    flag_decrease = delta_investigations &lt; 0 & !time_gap & !prev_was_outlier,\n    \n    # Flag 3: Dip detection - value is much lower than neighbors\n    flag_dip = (\n      (!is.na(prev_value) & investigations_aided &lt; (0.5 * prev_value)) |\n      (!is.na(prev2_value) & investigations_aided &lt; (0.5 * prev2_value))\n    ) & (\n      (!is.na(next_value) & investigations_aided &lt; (0.5 * next_value)) |\n      (!is.na(next2_value) & investigations_aided &lt; (0.5 * next2_value))\n    ),\n    \n    # Combine all flags\n    flag_any = flag_decrease | flag_spike | flag_dip | flag_magnitude,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for investigations aided with flagged points\np_investigations_raw &lt;- investigations_validation %&gt;%\n  plot_ly(x = ~year_month, y = ~investigations_aided, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = investigations_validation %&gt;% filter(flag_any),\n              x = ~year_month, y = ~investigations_aided, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Investigations Aided - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Investigations Aided\"))\n\np_investigations_raw\n\n\n\n\n\n\nShow Investigations Aided visualization and correction code\n#### Investigations Aided Correction #####\n\n# Create cleaned investigations aided data\ninvestigations_clean &lt;- investigations_validation %&gt;%\n  filter(!flag_any) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"delta_\"), \n         -starts_with(\"time_gap\"), -prev_value, -next_value, \n         -prev2_value, -next2_value, -prev_was_outlier)\n\n# Plot cleaned data\np_investigations_clean &lt;- investigations_clean %&gt;%\n  plot_ly(x = ~year_month, y = ~investigations_aided, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Investigations Aided - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Investigations Aided\"))\n\np_investigations_clean\n\n\n\n\n\n\nShow Investigations Aided visualization and correction code\n# ---- Investigations Aided Yearly Sum ----\n\n# Summarise highest investigations_aided per jurisdiction per year\ninvestigations_yearly &lt;- investigations_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%  # extract year\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_investigations = max(investigations_aided, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_investigations = sum(max_investigations, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_investigations_yearly &lt;- investigations_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_investigations,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"darkblue\", width = 3),\n          marker = list(size = 8, color = \"red\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Investigations Aided per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Investigations Aided\"))\n\np_investigations_yearly\n\n\n\n\n\n\n\n\nParticipating Laboratories Changes\nValidation Approach:\n· Allows legitimate fluctuations (labs can join/leave)\n· Targets only extreme jumps\n· Spike detection: 3× increases with 90% drops\n· Recognizes accreditation changes\nCorrection Methods:\n· Selective removal of clear errors\n· Preserves legitimate lab network changes\n· Minimal intervention approach\n· Maintains operational reality\nExample: Oklahoma Typo\nOklahoma 2008 Data Typo\n\n\nShow Labs visualization and correction code\n#### Raw NDIS Labs plot\n\n# Flag anomalies for ndis_labs - only spike detection needed\nlabs_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(ndis_labs),\n    next_value = lead(ndis_labs),\n    \n    delta_labs = ndis_labs - prev_value,\n    delta_next = next_value - ndis_labs,\n    \n    # Check for time gaps (more than 3 months between observations)\n    time_gap = as.numeric(difftime(year_month, lag(year_month), units = \"days\")) &gt; 100,\n    time_gap_next = as.numeric(difftime(lead(year_month), year_month, units = \"days\")) &gt; 100,\n    \n    # Flag: Spike detection - sharp up then sharp down\n    flag_spike = !is.na(delta_labs) & !is.na(delta_next) &\n                 delta_labs &gt; 0 & delta_next &lt; 0 &\n                 (ndis_labs &gt; 3 * prev_value |\n                  abs(delta_next) &gt; (0.9 * ndis_labs)) &\n                 !time_gap & !time_gap_next,\n    \n    # Replace NA with FALSE\n    flag_spike = ifelse(is.na(flag_spike), FALSE, flag_spike)\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for ndis_labs with flagged points\np_labs_raw &lt;- labs_validation %&gt;%\n  plot_ly(x = ~year_month, y = ~ndis_labs, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = labs_validation %&gt;% filter(flag_spike),\n              x = ~year_month, y = ~ndis_labs, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"NDIS Labs - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"NDIS Labs\"))\n\np_labs_raw\n\n\n\n\n\n\nShow Labs visualization and correction code\n#### NDIS Labs Correction #####\n\n# Create cleaned ndis_labs data\nlabs_clean &lt;- labs_validation %&gt;%\n  filter(!flag_spike) %&gt;%\n  select(-flag_spike, -delta_labs, -delta_next, \n         -time_gap, -time_gap_next, -prev_value, -next_value)\n\n# Plot cleaned data\np_labs_clean &lt;- labs_clean %&gt;%\n  plot_ly(x = ~year_month, y = ~ndis_labs, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"NDIS Labs - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"NDIS Labs\"))\n\np_labs_clean\n\n\n\n\n\n\n\n\nCompiled Data Growth\nValidation Approach:\n· Cross-validation across all metrics\n· Yearly aggregation using maximum values\n· Consistency checks between profile types\n· Scale-appropriate visualization\nCorrection Methods:\n· Unified compilation from cleaned sources\n· Dual-axis visualization for different scales\n· Analysis-ready dataset creation\n· Comprehensive trend analysis\n\n\nShow compiled data visualization and correction code\n# Combine all cleaned datasets\nndis_clean &lt;- offender_clean %&gt;%\n  select(jurisdiction, year_month, offender_profiles) %&gt;%\n  full_join(\n    arrestee_clean %&gt;% select(jurisdiction, year_month, arrestee),\n    by = c(\"jurisdiction\", \"year_month\")\n  ) %&gt;%\n  full_join(\n    forensic_clean %&gt;% select(jurisdiction, year_month, forensic_profiles),\n    by = c(\"jurisdiction\", \"year_month\")\n  ) %&gt;%\n  full_join(\n    investigations_clean %&gt;% select(jurisdiction, year_month, investigations_aided),\n    by = c(\"jurisdiction\", \"year_month\")\n  ) %&gt;%\n  full_join(\n    labs_clean %&gt;% select(jurisdiction, year_month, ndis_labs),\n    by = c(\"jurisdiction\", \"year_month\")\n  ) %&gt;%\n  arrange(jurisdiction, year_month)\n\n# Get yearly data\ngrowth_data_yearly &lt;- ndis_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  mutate(\n    selection_priority = case_when(\n      year &lt;= 2018 ~ arrestee,\n      year &gt; 2018 ~ offender_profiles\n    )\n  ) %&gt;%\n  slice_max(order_by = selection_priority, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    offender_total = sum(offender_profiles, na.rm = TRUE),\n    arrestee_total = sum(arrestee, na.rm = TRUE),\n    forensic_total = sum(forensic_profiles, na.rm = TRUE),\n    investigations_total = sum(investigations_aided, na.rm = TRUE),\n    n_jurisdictions = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(date = as.Date(paste0(year, \"-01-01\")))\n\n# Calculate scale factor for dual y-axes\nmax_dna &lt;- max(c(growth_data_yearly$offender_total, \n                 growth_data_yearly$arrestee_total, \n                 growth_data_yearly$forensic_total), na.rm = TRUE)\nmax_investigations &lt;- max(growth_data_yearly$investigations_total, na.rm = TRUE)\nscale_factor &lt;- max_dna / max_investigations\n\n# Prepare data for plotting\ndna_data &lt;- growth_data_yearly %&gt;%\n  select(date, offender_total, arrestee_total, forensic_total) %&gt;%\n  pivot_longer(\n    cols = c(offender_total, arrestee_total, forensic_total),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_total\" ~ \"Offender\",\n      variable == \"arrestee_total\" ~ \"Arrestee\", \n      variable == \"forensic_total\" ~ \"Forensic\"\n    ),\n    count_scaled = count \n  )\n\ninvestigations_data &lt;- growth_data_yearly %&gt;%\n  select(date, investigations_total) %&gt;%\n  mutate(\n    variable = \"Investigations\",\n    count_scaled = investigations_total * scale_factor\n  )\n\n#### Interactive Plot ####\n\n# Create separate traces for each variable\np_interactive &lt;- plot_ly() %&gt;%\n  # DNA Profiles\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Offender\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Offender Profiles',\n    line = list(color = '#0072B2', width = 2),\n    marker = list(color = '#0072B2', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Arrestee\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Arrestee Profiles',\n    line = list(color = '#D55E00', width = 2),\n    marker = list(color = '#D55E00', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Forensic\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Forensic Profiles',\n    line = list(color = '#009E73', width = 2),\n    marker = list(color = '#009E73', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  # Investigations Aided\n  add_trace(\n    data = investigations_data,\n    x = ~date, y = ~investigations_total,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Investigations Aided',\n    line = list(color = '#CC79A7', width = 2),\n    marker = list(color = '#CC79A7', size = 6),\n    yaxis = 'y2'\n  ) %&gt;%\n  layout(\n    title = \"DNA Profiles and Investigations Aided Over Time (Yearly)\",\n    xaxis = list(\n      title = \"Year\",\n      tickformat = \"%Y\"\n    ),\n    yaxis = list(\n      title = \"DNA Profiles\",\n      side = 'left',\n      showgrid = TRUE\n    ),\n    yaxis2 = list(\n      title = \"Investigations Aided\",\n      side = 'right',\n      overlaying = 'y',\n      showgrid = FALSE\n    ),\n    legend = list(\n      x = 0.01,\n      y = 0.99,\n      bgcolor = 'rgba(255,255,255,0.9)',\n      bordercolor = 'black',\n      borderwidth = 1\n    ),\n    hovermode = 'x unified'\n  )\n\np_interactive\n\n\n\n\n\n\nShow compiled data visualization and correction code\n#### Publication-Ready Static Plot ####\n\n# Get the actual date range for proper x-axis limits\ndate_range &lt;- range(growth_data_yearly$date)\nextended_date_range &lt;- c(min(date_range) - years(1), max(date_range))\nlegend_start_date &lt;- extended_date_range[1]\n\ny_upper_limit &lt;- max_dna * 1.05\ny_lower_limit &lt;- 0\n\np_static &lt;- ggplot() +\n  geom_line(data = dna_data, \n            aes(x = date, y = count_scaled, color = variable), \n            linewidth = 1.2) +\n  geom_point(data = dna_data, \n             aes(x = date, y = count_scaled, color = variable), \n             size = 2) +\n  geom_line(data = investigations_data, \n            aes(x = date, y = count_scaled, color = variable), \n            linewidth = 1.2) +\n  geom_point(data = investigations_data, \n             aes(x = date, y = count_scaled, color = variable), \n             size = 2) +\n  scale_x_date(\n    date_breaks = \"2 years\",\n    date_labels = \"%Y\",\n    limits = extended_date_range, \n    expand = expansion(mult = 0.02)\n  ) +\n  scale_y_continuous(\n    name = \"DNA Profiles\",\n    labels = function(x) {\n      ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n             ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n    },\n    breaks = seq(0, max_dna, by = 2e6),\n    limits = c(y_lower_limit, y_upper_limit),\n    sec.axis = sec_axis(~./scale_factor, \n                        name = \"Investigations Aided\",\n                        labels = function(x) {\n                          ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                 ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n                        },\n                        breaks = seq(0, max_investigations, by = 200000))\n  ) +\n  scale_color_manual(\n    name = NULL,\n    values = c(\"Offender\" = \"#0072B2\", \n               \"Arrestee\" = \"#D55E00\", \n               \"Forensic\" = \"#009E73\",\n               \"Investigations\" = \"#CC79A7\")\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.5),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.5),\n    axis.text = element_text(color = \"black\", size = 10),\n    axis.text.x = element_text(angle = 0, hjust = 0.5),\n    axis.text.y.right = element_text(margin = margin(l = 15)),\n    axis.title = element_text(color = \"black\", size = 11, face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y.right = element_text(margin = margin(l = 25)),\n    axis.title.y.left = element_text(margin = margin(r = 10)),\n    legend.position = \"none\",\n    plot.margin = margin(5, 10, 5, 10),\n    aspect.ratio = 0.6\n  ) +\n  labs(\n    x = \"Year\",\n    title = \"DNA Profiles and Investigations Aided Over Time (Yearly)\"\n  ) +\n  # DNA Profiles legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(5), \n           ymin = max_dna * 0.90, ymax = max_dna, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # Investigations legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(6), \n           ymin = max_dna * 0.80, ymax = max_dna * 0.84, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # DNA Profiles legend items\n  annotate(\"point\", \n           x = legend_start_date + years(0) + months(6), \n           y = c(max_dna * 0.98, max_dna * 0.95, max_dna * 0.92),\n           color = c(\"#0072B2\", \"#D55E00\", \"#009E73\"), size = 3) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = c(max_dna * 0.98, max_dna * 0.95, max_dna * 0.92),\n           label = c(\"Offender Profiles\", \"Arrestee Profiles\", \"Forensic Profiles\"),\n           hjust = 0, size = 3.2) +\n  annotate(\"text\", \n           x = legend_start_date + years(0), \n           y = max_dna, \n           label = \"DNA Profiles\", \n           fontface = \"bold\", hjust = 0, size = 3.5, vjust = -0.5) +\n  # Investigations Aided legend\n  annotate(\"point\", \n           x = legend_start_date + years(0) + months(6), \n           y = max_dna * 0.82, \n           color = \"#CC79A7\", size = 3) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = max_dna * 0.82, \n           label = \"Investigations Aided\",\n           hjust = 0, size = 3.2) +\n  annotate(\"text\", \n           x = legend_start_date + years(0) + months(6), \n           y = max_dna * 0.84, \n           label = \"Investigations\", \n           fontface = \"bold\", hjust = 0, size = 3.5, vjust = -0.5)\n\np_static\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection and Metadata Logging\nThis section systematically documents all data anomalies detected during the validation process. Each flagged observation is recorded with comprehensive metadata including anomaly type, jurisdiction, timestamp, and contextual values. The log serves as both an audit trail for data quality decisions and a source for transparency reporting.\nKey outputs include:\n\nDetailed anomaly records for technical review\nSummary statistics for quality assessment\nVisualization of anomaly distribution patterns\nFormatted summaries for public reporting\n\n\n\nShow anomaly detection and logging code\n### Anomaly Detection and Metadata Logging\n\n# Function to create standardized anomaly records\ncreate_anomaly_record &lt;- function(data, metric_name, jurisdiction, date, value, \n                                 flags, prev_value = NA, next_value = NA) {\n  data.frame(\n    metric = metric_name,\n    jurisdiction = jurisdiction,\n    date = date,\n    value = value,\n    previous_value = prev_value,\n    next_value = next_value,\n    flag_decrease = flags$flag_decrease,\n    flag_spike = flags$flag_spike,\n    flag_dip = flags$flag_dip,\n    flag_magnitude = ifelse(\"flag_magnitude\" %in% names(flags), flags$flag_magnitude, FALSE),\n    flag_any = flags$flag_any,\n    time_gap = ifelse(\"time_gap\" %in% names(flags), flags$time_gap, FALSE),\n    anomaly_type = case_when(\n      flags$flag_spike ~ \"Spike\",\n      flags$flag_dip ~ \"Dip\",\n      flags$flag_decrease ~ \"Decrease\",\n      ifelse(\"flag_magnitude\" %in% names(flags), flags$flag_magnitude, FALSE) ~ \"Magnitude Outlier\",\n      TRUE ~ \"Other\"\n    ),\n    stringsAsFactors = FALSE\n  )\n}\n\n# Initialize empty anomaly log\nanomaly_log &lt;- data.frame()\n\n### Offender Profiles Anomalies\noffender_anomalies &lt;- offender_validation %&gt;%\n  filter(flag_any) \n\nif(nrow(offender_anomalies) &gt; 0) {\n  for(i in 1:nrow(offender_anomalies)) {\n    row &lt;- offender_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      data = row,\n      metric_name = \"Offender Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$year_month,\n      value = row$offender_profiles,\n      flags = list(\n        flag_decrease = row$flag_decrease,\n        flag_spike = row$flag_spike,\n        flag_dip = row$flag_dip,\n        flag_any = row$flag_any,\n        time_gap = row$time_gap\n      ),\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Forensic Profiles Anomalies\nforensic_anomalies &lt;- forensic_validation %&gt;%\n  filter(flag_any)\n\nif(nrow(forensic_anomalies) &gt; 0) {\n  for(i in 1:nrow(forensic_anomalies)) {\n    row &lt;- forensic_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      data = row,\n      metric_name = \"Forensic Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$year_month,\n      value = row$forensic_profiles,\n      flags = list(\n        flag_decrease = row$flag_decrease,\n        flag_spike = row$flag_spike,\n        flag_dip = row$flag_dip,\n        flag_any = row$flag_any,\n        time_gap = row$time_gap\n      ),\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Arrestee Profiles Anomalies\narrestee_anomalies &lt;- arrestee_validation %&gt;%\n  filter(flag_any)\n\nif(nrow(arrestee_anomalies) &gt; 0) {\n  for(i in 1:nrow(arrestee_anomalies)) {\n    row &lt;- arrestee_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      data = row,\n      metric_name = \"Arrestee Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$year_month,\n      value = row$arrestee,\n      flags = list(\n        flag_decrease = row$flag_decrease,\n        flag_spike = row$flag_spike,\n        flag_dip = row$flag_dip,\n        flag_any = row$flag_any,\n        time_gap = row$time_gap\n      ),\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Investigations Aided Anomalies\ninvestigations_anomalies &lt;- investigations_validation %&gt;%\n  filter(flag_any)\n\nif(nrow(investigations_anomalies) &gt; 0) {\n  for(i in 1:nrow(investigations_anomalies)) {\n    row &lt;- investigations_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      data = row,\n      metric_name = \"Investigations Aided\",\n      jurisdiction = row$jurisdiction,\n      date = row$year_month,\n      value = row$investigations_aided,\n      flags = list(\n        flag_decrease = row$flag_decrease,\n        flag_spike = row$flag_spike,\n        flag_dip = row$flag_dip,\n        flag_magnitude = row$flag_magnitude,\n        flag_any = row$flag_any,\n        time_gap = row$time_gap,\n        prev_was_outlier = row$prev_was_outlier\n      ),\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### NDIS Labs Anomalies\nlabs_anomalies &lt;- labs_validation %&gt;%\n  filter(flag_spike)\n\nif(nrow(labs_anomalies) &gt; 0) {\n  for(i in 1:nrow(labs_anomalies)) {\n    row &lt;- labs_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      data = row,\n      metric_name = \"NDIS Labs\",\n      jurisdiction = row$jurisdiction,\n      date = row$year_month,\n      value = row$ndis_labs,\n      flags = list(\n        flag_decrease = FALSE,\n        flag_spike = row$flag_spike,\n        flag_dip = FALSE,\n        flag_magnitude = FALSE,\n        flag_any = row$flag_spike,\n        time_gap = row$time_gap\n      ),\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Set metric order for consistent display\nmetric_order &lt;- c(\"Offender Profiles\", \"Forensic Profiles\", \"Arrestee Profiles\", \"Investigations Aided\", \"NDIS Labs\")\n\n# Convert metric to factor with desired order\nanomaly_log &lt;- anomaly_log %&gt;%\n  mutate(metric = factor(metric, levels = metric_order))\n\n### Create Summary for Website Display\nanomaly_summary &lt;- anomaly_log %&gt;%\n    group_by(metric, jurisdiction, anomaly_type) %&gt;%\n    summarise(\n      count = n(),\n      earliest_date = min(date),\n      latest_date = max(date),\n      avg_value = mean(value, na.rm = TRUE),\n      .groups = \"drop\"\n    ) %&gt;%\n    arrange(metric, jurisdiction, anomaly_type)\n\n### Display Summary Tables\n  \nanomaly_overview &lt;- anomaly_log %&gt;%\n    group_by(metric) %&gt;%\n    summarise(\n      total_anomalies = n(),\n      affected_jurisdictions = n_distinct(jurisdiction),\n      .groups = \"drop\"\n    ) %&gt;%\n    arrange(factor(metric, levels = metric_order))  # Ensure correct order\n  \nprint(knitr::kable(anomaly_overview, format = \"simple\", caption = \"Overview of Detected Anomalies\"))\n\n\n\n\nTable: Overview of Detected Anomalies\n\nmetric                  total_anomalies   affected_jurisdictions\n---------------------  ----------------  -----------------------\nOffender Profiles                   260                       52\nForensic Profiles                   183                       54\nArrestee Profiles                    97                       18\nInvestigations Aided                130                       53\nNDIS Labs                             1                        1\n\n\nShow anomaly detection and logging code\n### Create Visual Summary Plot\np_anomaly_summary &lt;- anomaly_log %&gt;%\n    group_by(metric, jurisdiction) %&gt;%\n    summarise(count = n(), .groups = \"drop\") %&gt;%\n    mutate(metric = factor(metric, levels = metric_order)) %&gt;%  # Ensure correct order\n    plot_ly(\n      x = ~jurisdiction,\n      y = ~count,\n      color = ~metric,\n      type = \"bar\",\n      text = ~count,\n      textposition = \"auto\",\n      colors = c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\")  # Consistent colors\n    ) %&gt;%\n    layout(\n      title = \"Anomaly Distribution by Jurisdiction and Metric\",\n      xaxis = list(title = \"Jurisdiction\"),\n      yaxis = list(title = \"Number of Anomalies\"),\n      barmode = \"stack\",\n      legend = list(traceorder = \"normal\")  # Ensures legend follows metric order\n    )\n  \np_anomaly_summary\n\n\n\n\n\n\nShow anomaly detection and logging code\n# Saving anomaly log\nanomaly_dir &lt;- here(\"data\", \"ndis\", \"intermediate\", \"anomaly_log.csv\")\nwrite.csv(anomaly_log, anomaly_dir, row.names = FALSE)\n\n\n\n\nTemporal Coverage Heat Map\nThe heat map visualizes the temporal coverage of NDIS data submissions across different jurisdictions over the years for the intermediate csv file (with outliers and reporting errors) and for the cleaned dataset. It highlights periods of active reporting and gaps in data submission.\n\n\nShow heatmap code\n# Prepare data for heatmap - INTERMEDIATE DATASET\ntemporal_coverage_intermediate &lt;-  ndis_data %&gt;%\n  mutate(year = year(year_month)) %&gt;%\n  count(jurisdiction, year) %&gt;%\n  complete(jurisdiction, year = 2001:2025, fill = list(n = 0)) %&gt;%\n  filter(!is.na(jurisdiction)) %&gt;%\n  mutate(jurisdiction = factor(jurisdiction, levels = rev(sort(unique(jurisdiction)))))\n\n# Create the heatmap for intermediate data\nheatmap_raw &lt;- ggplot(temporal_coverage_intermediate, aes(x = year, y = jurisdiction, fill = n)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  scale_fill_viridis(\n    name = \"Snapshots\\nper Year\",\n    option = \"plasma\",\n    direction = -1,\n    breaks = c(0, 6, 12, 24, 48),\n    labels = c(\"0\", \"6\", \"12\", \"24\", \"48+\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(2001, 2025, by = 1), \n    expand = expansion(mult = 0.01)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Jurisdiction\",\n    title = \"Temporal Coverage Heatmap - Original Data\",\n    subtitle = \"Shows snapshots frequency across jurisdictions and years\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n    axis.text.y = element_text(size = 6),\n    plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n    plot.subtitle = element_text(size = 9, hjust = 0),\n    legend.position = \"right\",\n    legend.key.height = unit(0.8, \"cm\"),\n    legend.key.width = unit(0.3, \"cm\"),\n    axis.title.x = element_text(margin = margin(t = 12))\n  )\n\nheatmap_raw\n\n\n\n\n\n\n\n\n\nShow heatmap code\n# Prepare data for heatmap - CLEANED DATASET\ntemporal_coverage_clean &lt;- ndis_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%\n  count(jurisdiction, year) %&gt;%\n  complete(jurisdiction, year = 2001:2025, fill = list(n = 0)) %&gt;%\n  filter(!is.na(jurisdiction)) %&gt;%\n  mutate(jurisdiction = factor(jurisdiction, levels = rev(sort(unique(jurisdiction)))))\n\n# Create the heatmap for cleaned data\nheatmap_after_clean &lt;- ggplot(temporal_coverage_clean, aes(x = year, y = jurisdiction, fill = n)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  scale_fill_viridis(\n    name = \"Snapshots\\nper Year\",\n    option = \"plasma\",\n    direction = -1,\n    breaks = c(0, 1, 3, 6, 10),\n    labels = c(\"0\", \"1\", \"3\", \"6\", \"10+\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(2001, 2025, by = 1),\n    expand = expansion(mult = 0.01)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Jurisdiction\",\n    title = \"Temporal Coverage Heatmap - Cleaned Data\",\n    subtitle = \"Shows data submission frequency after validation-based cleaning\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n    axis.text.y = element_text(size = 6),\n    plot.title = element_text(face = \"bold\", size = 11, hjust = 0),\n    plot.subtitle = element_text(size = 9, hjust = 0),\n    legend.position = \"right\",\n    legend.key.height = unit(0.8, \"cm\"),\n    legend.key.width = unit(0.3, \"cm\"),\n    axis.title.x = element_text(margin = margin(t = 12))\n  )\n\nheatmap_after_clean\n\n\n\n\n\n\n\n\n\n\n\nComparison with peer-reviewed papers\nAs an additional check, we compared corrected national aggregates against published NDIS totals from FBI press releases and peer-reviewed articles. As shown in Figure 6, the reconstructed dataset aligns closely with these independent milestones, supporting the technical quality of the NDIS time series.\n\n\nShow peer-reviewed literature comparison\n# Preparation of growth_data_yearly \ngrowth_data_yearly &lt;- ndis_clean %&gt;%\n  mutate(year = year(year_month)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  arrange(jurisdiction, year_month) %&gt;%\n  mutate(\n    selection_priority = case_when(\n      year &lt;= 2018 ~ arrestee,\n      year &gt; 2018 ~ offender_profiles\n    )\n  ) %&gt;%\n  slice_max(order_by = selection_priority, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    offender_total = sum(offender_profiles, na.rm = TRUE),\n    arrestee_total = sum(arrestee, na.rm = TRUE),\n    forensic_total = sum(forensic_profiles, na.rm = TRUE),\n    investigations_total = sum(investigations_aided, na.rm = TRUE),\n    n_jurisdictions = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    total_profiles = offender_total + arrestee_total + forensic_total,\n    date = as.Date(paste0(year, \"-01-01\"))\n  )\n\n# Prepare data for plotting DNA profiles\ndna_data &lt;- growth_data_yearly %&gt;%\n  select(date, offender_total, arrestee_total, forensic_total, total_profiles) %&gt;%\n  pivot_longer(\n    cols = c(offender_total, arrestee_total, forensic_total, total_profiles),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_total\" ~ \"Offender\",\n      variable == \"arrestee_total\" ~ \"Arrestee\", \n      variable == \"forensic_total\" ~ \"Forensic\",\n      variable == \"total_profiles\" ~ \"Total\"\n    )\n  )\n\n# Prepare data for plotting investigations\ninvestigations_data &lt;- growth_data_yearly %&gt;%\n  select(date, investigations_total)\n\n# Create literature dataset\nliterature_data &lt;- tribble(\n  ~citation, ~asof_date, ~offender_profiles, ~arrestee_profiles, ~forensic_profiles, ~total_profiles, ~investigations_aided, ~short_label,\n  \"Ge et al., 2012\", \"2011-06-01\", NA, NA, NA, 10000000, 141300, \"Ge et al., 2012\",\n  \"Ge et al., 2014\", \"2013-05-01\", NA, NA, NA, 12000000, 185000, \"Ge et al., 2014\",\n  \"Wickenheiser, 2022\", \"2021-10-01\", 14836490, 4513955, 1144255, NA, 587773, \"Wickenheiser, 2022\",\n  \"Link et al., 2023\", \"2022-11-01\", NA, NA, NA, 21791620, 622955, \"Link et al., 2023\",\n  \"Greenwald & Phiri, 2024\", \"2024-02-01\", 17000000, 5000000, 1300000, NA, 680000, \"Greenwald & Phiri, 2024\"\n) %&gt;%\n  mutate(\n    asof_date = as.Date(asof_date),\n    total_profiles = ifelse(\n      is.na(total_profiles),\n      rowSums(select(., offender_profiles, arrestee_profiles, forensic_profiles), na.rm = TRUE),\n      total_profiles\n    )\n  )\n\n# Prepare literature data for DNA profiles\nliterature_dna &lt;- literature_data %&gt;%\n  select(short_label, asof_date, offender_profiles, arrestee_profiles, forensic_profiles, total_profiles) %&gt;%\n  pivot_longer(\n    cols = c(offender_profiles, arrestee_profiles, forensic_profiles, total_profiles),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  filter(!is.na(count)) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_profiles\" ~ \"Offender\",\n      variable == \"arrestee_profiles\" ~ \"Arrestee\",\n      variable == \"forensic_profiles\" ~ \"Forensic\",\n      variable == \"total_profiles\" ~ \"Total\"\n    )\n  )\n\n# Prepare literature data for investigations\nliterature_investigations &lt;- literature_data %&gt;%\n  select(short_label, asof_date, investigations_aided) %&gt;%\n  filter(!is.na(investigations_aided))\n\n# Get date range\ndate_range &lt;- range(growth_data_yearly$date)\nextended_date_range &lt;- c(min(date_range) - years(1), max(date_range))\nlegend_start_date &lt;- extended_date_range[1]\n\n# Calculate y-axis limits for DNA profiles\nmax_dna &lt;- max(dna_data$count, na.rm = TRUE)\ny_upper_dna &lt;- max_dna * 1.05\n\n# Calculate y-axis limits for investigations\nmax_inv &lt;- max(investigations_data$investigations_total, na.rm = TRUE)\ny_upper_inv &lt;- max_inv * 1.05\n\n# Plot 1 DNA profiles\n\np_dna_profiles &lt;- ggplot() +\n  # Original data lines\n  geom_line(data = dna_data, \n            aes(x = date, y = count, color = variable), \n            linewidth = 1.2) +\n  geom_point(data = dna_data, \n             aes(x = date, y = count, color = variable), \n             size = 2.5) +\n  \n  # Add literature points - Total profiles\n  geom_point(data = literature_dna %&gt;% filter(variable == \"Total\"),\n             aes(x = asof_date, y = count),\n             shape = 4, size = 4, stroke = 1.5, color = \"black\") +\n  \n  # Add literature points - Individual profile types\n  geom_point(data = literature_dna %&gt;% filter(variable != \"Total\"),\n             aes(x = asof_date, y = count, color = variable),\n             shape = 4, size = 3, stroke = 1.2) +\n  \n  # Add citation labels\n  geom_text_repel(\n    data = literature_dna %&gt;% filter(variable == \"Total\"),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 3, box.padding = 0.5, point.padding = 0.3,\n    min.segment.length = 0, segment.color = \"gray50\",\n    max.overlaps = 20\n  ) +\n  \n  geom_text_repel(\n    data = literature_dna %&gt;% filter(variable != \"Total\"),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 3, box.padding = 0.5, point.padding = 0.3,\n    min.segment.length = 0, segment.color = \"gray50\",\n    max.overlaps = 20\n  ) +\n\n  scale_x_date(\n    date_breaks = \"1 years\",\n    date_labels = \"%Y\",\n    limits = extended_date_range, \n    expand = expansion(mult = 0.02)\n  ) +\n  scale_y_continuous(\n    name = \"DNA Profiles\",\n    labels = function(x) {\n      ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n             ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n    },\n    breaks = seq(0, max_dna, by = 5e6),\n    limits = c(0, y_upper_dna),\n    expand = expansion(mult = c(0, 0.02))\n  ) +\n  scale_color_manual(\n    name = NULL,\n    values = c(\"Offender\" = \"#0072B2\", \n               \"Arrestee\" = \"#D55E00\", \n               \"Forensic\" = \"#009E73\",\n               \"Total\" = \"#56B4E9\")\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.5),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.5),\n    axis.text = element_text(color = \"black\", size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title = element_text(color = \"black\", size = 11, face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    legend.position = \"none\",\n    plot.margin = margin(5, 10, 5, 10),\n    aspect.ratio = 0.6\n  ) +\n  labs(\n    x = \"Year\",\n    title = \"DNA Profiles Over Time with Literature Validation\"\n  ) +\n  # DNA Profiles legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(5), \n           ymin = max_dna * 0.84, ymax = max_dna, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # Legend items\n  annotate(\"point\", \n           x = legend_start_date + months(6), \n           y = c(max_dna * 0.98, max_dna * 0.95, max_dna * 0.92, max_dna * 0.89),\n           color = c(\"#0072B2\", \"#D55E00\", \"#009E73\", \"#56B4E9\"), size = 3) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = c(max_dna * 0.98, max_dna * 0.95, max_dna * 0.92, max_dna * 0.89),\n           label = c(\"Offender\", \"Arrestee\", \"Forensic\", \"Total\"),\n           hjust = 0, size = 3.2) +\n  # Literature reference symbol\n  annotate(\"point\",\n           x = legend_start_date + months(6),\n           y = max_dna * 0.86,\n           shape = 4, size = 3, stroke = 1.2, color = \"black\") +\n  annotate(\"text\",\n           x = legend_start_date + years(1),\n           y = max_dna * 0.86,\n           label = \"Literature Citation\",\n           hjust = 0, size = 3.2)\n\nprint(p_dna_profiles)\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\n# PLOT 2: INVESTIGATIONS AIDED\n\np_investigations &lt;- ggplot() +\n  # Original data\n  geom_line(data = investigations_data, \n            aes(x = date, y = investigations_total), \n            color = \"#CC79A7\",\n            linewidth = 1.2) +\n  geom_point(data = investigations_data, \n             aes(x = date, y = investigations_total), \n             color = \"#CC79A7\",\n             size = 2.5) +\n  \n  # Add literature points\n  geom_point(data = literature_investigations,\n             aes(x = asof_date, y = investigations_aided),\n             shape = 4, size = 4, stroke = 1.5, color = \"black\") +\n  \n  # Add text labels\n  geom_text_repel(\n    data = literature_investigations,\n    aes(x = asof_date, y = investigations_aided, label = short_label),\n    size = 3, box.padding = 0.5, point.padding = 0.3,\n    min.segment.length = 0, segment.color = \"gray50\",\n    max.overlaps = 20\n  ) +\n  \n  scale_x_date(\n    date_breaks = \"1 years\",\n    date_labels = \"%Y\",\n    limits = extended_date_range, \n    expand = expansion(mult = 0.02)\n  ) +\n  scale_y_continuous(\n    name = \"Investigations Aided\",\n    labels = function(x) {\n      ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n             ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n    },\n    breaks = seq(0, max_inv, by = 100000),\n    limits = c(0, y_upper_inv),\n    expand = expansion(mult = c(0, 0.02))\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.5),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.5),\n    axis.text = element_text(color = \"black\", size = 10),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title = element_text(color = \"black\", size = 11, face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    plot.margin = margin(5, 10, 5, 10),\n    aspect.ratio = 0.6\n  ) +\n  labs(\n    x = \"Year\",\n    title = \"Investigations Aided Over Time with Literature Validation\"\n  ) +\n  # Legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(6), \n           ymin = max_inv * 0.94, ymax = max_inv, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # Legend items\n  annotate(\"point\", \n           x = legend_start_date + months(6), \n           y = max_inv * 0.97,\n           color = \"#CC79A7\", size = 3) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = max_inv * 0.97,\n           label = \"Investigations Aided\",\n           hjust = 0, size = 3.2) +\n  # Literature reference symbol\n  annotate(\"point\",\n           x = legend_start_date + months(6),\n           y = max_inv * 0.91,\n           shape = 4, size = 3, stroke = 1.2, color = \"black\") +\n  annotate(\"text\",\n           x = legend_start_date + years(1),\n           y = max_inv * 0.91,\n           label = \"Literature Citation\",\n           hjust = 0, size = 3.2)\n\nprint(p_investigations)"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#summarystats",
    "href": "qmd_root/ndis_analysis.html#summarystats",
    "title": "NDIS Database",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nBasic descriptive statistics to understand the scope and characteristics of the NDIS data.\n\n\nShow summary statistics code\n# Summary statistics table\nndis_summary &lt;- ndis_clean %&gt;%\n  mutate(\n    year = year(year_month),\n    offender_profiles = ifelse(is.na(offender_profiles), 0, offender_profiles),\n    arrestee = ifelse(is.na(arrestee), 0, arrestee),\n    forensic_profiles = ifelse(is.na(forensic_profiles), 0, forensic_profiles)\n  ) %&gt;%\n  group_by(year, jurisdiction) %&gt;%\n  summarise(\n    offender = max(offender_profiles, na.rm = TRUE),\n    arrestee = max(arrestee, na.rm = TRUE),\n    forensic = max(forensic_profiles, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    jurisdictions = n(),\n    offender = sum(offender, na.rm = TRUE),\n    arrestee = sum(arrestee, na.rm = TRUE),\n    forensic = sum(forensic, na.rm = TRUE),\n    total_profiles = sum(offender + arrestee + forensic, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(year)\n\n# Print summary table\nkable(ndis_summary, caption = \"Annual Summary Statistics\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAnnual Summary Statistics\n\n\nyear\njurisdictions\noffender\narrestee\nforensic\ntotal_profiles\n\n\n\n\n2001\n50\n655293\n0\n24618\n679911\n\n\n2002\n50\n1036328\n0\n38022\n1074350\n\n\n2003\n52\n1445776\n0\n67295\n1513071\n\n\n2004\n52\n1907536\n0\n93798\n2001334\n\n\n2005\n52\n2758256\n0\n124488\n2882744\n\n\n2006\n52\n3720741\n0\n157888\n3878629\n\n\n2007\n52\n5070961\n0\n195433\n5266394\n\n\n2008\n51\n6580918\n0\n247936\n6828854\n\n\n2009\n52\n7378522\n0\n296876\n7675398\n\n\n2010\n52\n8548599\n0\n350495\n8899094\n\n\n2011\n52\n9298728\n0\n397257\n9695985\n\n\n2012\n52\n10064492\n1326280\n464512\n11855284\n\n\n2013\n52\n10708210\n1717528\n529330\n12955068\n\n\n2014\n52\n11521643\n2090409\n598615\n14210667\n\n\n2015\n52\n12085490\n2161620\n670870\n14917980\n\n\n2016\n54\n12689698\n2583782\n750217\n16023697\n\n\n2017\n54\n13148032\n2954303\n823682\n16926017\n\n\n2018\n54\n13636797\n3395253\n908110\n17940160\n\n\n2019\n54\n14047871\n3796117\n985797\n18829785\n\n\n2020\n54\n14403525\n4210294\n1075677\n19689496\n\n\n2021\n54\n14836566\n4513962\n1144266\n20494794\n\n\n2022\n54\n14998671\n4888234\n1222806\n21109711\n\n\n2023\n54\n16532335\n5190279\n1279078\n23001692\n\n\n2024\n53\n16828007\n5704548\n1374435\n23906990\n\n\n2025\n54\n18648655\n5954756\n1421751\n26025162"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#visualiz",
    "href": "qmd_root/ndis_analysis.html#visualiz",
    "title": "NDIS Database",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nGeospatial Mapping of Jurisdiction Participation\n\n\nShow jurisdiction mapping analysis code\njurisdiction_coords &lt;- tibble::tribble(\n  ~jurisdiction_std,        ~lat,     ~lng,\n  \"Alabama\",         32.8067,  -86.7911,\n  \"Alaska\",         66.1605, -153.3691,\n  \"Arizona\",        33.7298, -111.4312,\n  \"Arkansas\",       34.9697,  -92.3731,\n  \"California\",     36.1162, -119.6816,\n  \"Colorado\",       39.0598, -105.3111,\n  \"Connecticut\",    41.5978,  -72.7554,\n  \"Delaware\",       39.3185,  -75.5071,\n  \"DC/FBI Lab\",     38.9072,  -77.0369,\n  \"DC/Metro PD\",    39.9072,  -77.0369,\n  \"Florida\",        27.7663,  -81.6868,\n  \"Georgia\",        33.0406,  -83.6431,\n  \"Hawaii\",         21.3068, -157.7912,\n  \"Idaho\",          44.2405, -114.4788,\n  \"Illinois\",       40.3495,  -88.9861,\n  \"Indiana\",        39.8494,  -86.2583,\n  \"Iowa\",           42.0115,  -93.2105,\n  \"Kansas\",         38.5266,  -96.7265,\n  \"Kentucky\",       37.6681,  -84.6701,\n  \"Louisiana\",      31.1695,  -91.8678,\n  \"Maine\",          44.6939,  -69.3819,\n  \"Maryland\",       39.0639,  -76.8021,\n  \"Massachusetts\",  42.2302,  -71.5301,\n  \"Michigan\",       43.3266,  -84.5361,\n  \"Minnesota\",      45.6945,  -93.9002,\n  \"Mississippi\",    32.7416,  -89.6787,\n  \"Missouri\",       38.4561,  -92.2884,\n  \"Montana\",        46.9219, -110.4544,\n  \"Nebraska\",       41.1254,  -98.2681,\n  \"Nevada\",         38.3135, -117.0554,\n  \"New Hampshire\",  43.4525,  -71.5639,\n  \"New Jersey\",     40.2989,  -74.5210,\n  \"New Mexico\",     34.8405, -106.2485,\n  \"New York\",       42.1657,  -74.9481,\n  \"North Carolina\", 35.6301,  -79.8064,\n  \"North Dakota\",   47.5289,  -99.7840,\n  \"Ohio\",           40.3888,  -82.7649,\n  \"Oklahoma\",       35.5653,  -96.9289,\n  \"Oregon\",         44.5720, -122.0709,\n  \"Pennsylvania\",   40.5908,  -77.2098,\n  \"Rhode Island\",   41.6809,  -71.5118,\n  \"South Carolina\", 33.8569,  -80.9450,\n  \"South Dakota\",   44.2998,  -99.4388,\n  \"Tennessee\",      35.7478,  -86.6923,\n  \"Texas\",          31.0545,  -97.5635,\n  \"Utah\",           40.1500, -111.8624,\n  \"Vermont\",        44.0459,  -72.7107,\n  \"Virginia\",       37.7693,  -78.1700,\n  \"Washington\",     47.4009, -121.4905,\n  \"West Virginia\",  38.4912,  -80.9545,\n  \"Wisconsin\",      44.2685,  -89.6165,\n  \"Wyoming\",        42.7560, -107.3025,\n  \"US Army\",        40.9072,  -77.0369,\n  \"Puerto Rico\",    18.2208,  -66.5901\n)\n\nstate_abbs &lt;- tibble::tibble(\n  state = tolower(c(\n    \"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\n    \"Delaware\", \"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n    \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\n    \"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\n    \"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n    \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\n    \"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\n    \"West Virginia\",\"Wisconsin\",\"Wyoming\",\n    \"DC/Metro PD\",\n    \"DC/FBI Lab\",\n    \"Puerto Rico\",\n    \"US Army\"\n  )),\n  abb = c(\n    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\n    \"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\n    \"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\n    \"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\n    \"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\n    \"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\n    \"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\n    \"WV\",\"WI\",\"WY\",\n    \"DC\",\n    \"FBI\",\n    \"PR\",\n    \"US\"\n  )\n)\n\nmap_data &lt;- ndis_clean %&gt;%\n  left_join(jurisdiction_coords, by = c(\"jurisdiction\" = \"jurisdiction_std\"))\n\nset.seed(123) # for reproducibility\n\nmap_data &lt;- map_data %&gt;%\n  group_by(lat, lng) %&gt;%\n  mutate(\n    n = n(),\n    offset_needed = n &gt; 1,\n    lat_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),\n    lng_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),\n    lat_adj = lat + lat_offset,\n    lng_adj = lng + lng_offset\n  ) %&gt;%\n  ungroup()\n\njurisdiction_summary &lt;- map_data %&gt;%\n  group_by(jurisdiction) %&gt;%\n  filter(year_month == max(year_month, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(jurisdiction = tolower(trimws(jurisdiction))) %&gt;% \n  group_by(jurisdiction) %&gt;%\n  summarise(\n    year_month = max(year_month, na.rm = TRUE),\n    offender = if (all(is.na(offender_profiles))) 0 else max(offender_profiles, na.rm = TRUE),\n    arrestee = if (all(is.na(arrestee))) 0 else max(arrestee, na.rm = TRUE),\n    forensic = if (all(is.na(forensic_profiles))) 0 else max(forensic_profiles, na.rm = TRUE),\n    total_profiles = sum(c(offender, arrestee, forensic), na.rm = TRUE),\n    lat_adj = first(lat_adj),\n    lng_adj = first(lng_adj)\n  ) %&gt;%\n  left_join(state_abbs, by = c(\"jurisdiction\" = \"state\")) %&gt;%\n  filter(!is.na(lat_adj) & !is.na(lng_adj))\n\npal &lt;- colorNumeric(palette = \"Blues\", domain = jurisdiction_summary$total_profiles)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addLabelOnlyMarkers(\n    data = jurisdiction_summary,\n    lng = ~lng_adj,\n    lat = ~lat_adj,\n    label = ~abb,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"center\",\n      textOnly = FALSE,\n      style = list(\n        \"background\" = \"white\",\n        \"border\" = \"2px solid #1a5276\",\n        \"border-radius\" = \"3px\",\n        \"padding\" = \"2px 4px\",\n        \"font-weight\" = \"bold\",\n        \"font-size\" = \"10px\",\n        \"color\" = \"#1a5276\",\n        \"box-shadow\" = \"2px 2px 4px rgba(0,0,0,0.3)\"\n      )\n    )\n  ) %&gt;%\n  addCircleMarkers(\n    data = jurisdiction_summary,\n    lng = ~lng_adj,\n    lat = ~lat_adj,\n    stroke = TRUE,\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;div style='font-size:12px'&gt;\",\n      \"&lt;b&gt;\", tools::toTitleCase(jurisdiction), \" (\", abb, \")&lt;/b&gt;&lt;br&gt;\",\n      \"Date: \",  format(year_month, \"%Y-%m\"), \"&lt;br&gt;\",\n      \"Total: \", format(total_profiles, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Offender: \", format(offender, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Arrestee: \", format(arrestee, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Forensic: \", format(forensic, big.mark = \",\"),\n      \"&lt;/div&gt;\"\n    )\n  ) %&gt;%\n  addControl(\n    html = \"&lt;div style='background:white;padding:5px;border:2px solid #1a5276;border-radius:3px;font-weight:bold;'&gt;NDIS 2025 State Participation&lt;/div&gt;\",\n    position = \"topright\"\n  ) %&gt;%\n  setView(lng = -98.5833, lat = 39.8333, zoom = 4)\n\n\n\n\n\n\n\n\nInteractive Table of Profile Growth\nInteractive table for readers to explore the underlying data with filtering and export capabilities.\n\n\nShow interactive table code\nsummary_table &lt;- ndis_clean %&gt;%\n  group_by(jurisdiction, year_month) %&gt;%\n  summarise(\n    offender_profiles = ifelse(all(is.na(offender_profiles)), 0, max(offender_profiles, na.rm = TRUE)),\n    arrestee = ifelse(all(is.na(arrestee)), 0, max(arrestee, na.rm = TRUE)),\n    forensic_profiles = ifelse(all(is.na(forensic_profiles)), 0, max(forensic_profiles, na.rm = TRUE)),\n    investigations_aided = ifelse(all(is.na(investigations_aided)), 0, max(investigations_aided, na.rm = TRUE)),\n    ndis_labs = ifelse(all(is.na(ndis_labs)), 0, max(ndis_labs, na.rm = TRUE)),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(year_month = format(year_month, \"%Y-%m\")) %&gt;%\n  arrange(jurisdiction, year_month)\n\n# Count the number of numeric columns (excluding the first 2 grouping columns)\nnumeric_cols_start &lt;- 3\nnumeric_cols_end &lt;- ncol(summary_table)  \n\n# Interactive table\ndatatable(\n  summary_table,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 20,\n    columnDefs = list(\n      list(className = 'dt-right', targets = (numeric_cols_start-1):(numeric_cols_end-1))\n    )\n  ),\n  rownames = FALSE,\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nExport Cleaned Dataset\nAfter cleaning and processing the NDIS data, the final dataset is exported as a CSV file for further analysis or sharing. The file is saved to the data/v1.0/ directory to maintain an organized workflow.\nOutput: NDIS_time_series.csv\n\n\nColumnTypeRowsMissingUniqueFirst_Valuesjurisdictionfactor9442054Alabama, Alabama, Alabamayear_monthcharacter944201972001-07, 2001-08, 2001-09offender_profilesnumeric9442071810, 0, 0arresteenumeric9442024320, 0, 0forensic_profilesnumeric9442061420, 0, 0investigations_aidednumeric94420421188, 88, 88ndis_labsnumeric94420234, 4, 4Data frame dimensions: 9442 rows × 7 columns\n\n\n\n\nShow dataset exportation code\n# Create final frozen version (v1.0)\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\n\nfrozen_path &lt;- here(frozen_dir, \"NDIS_time_series.csv\")\nwrite_csv(ndis_clean, frozen_path)\ncat(paste(\"✓ Created frozen version 1.0 at:\", frozen_path, \"\\n\"))\n\n\n✓ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/v1.0/NDIS_time_series.csv"
  },
  {
    "objectID": "qmd_root/foia_processing.html",
    "href": "qmd_root/foia_processing.html",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "qmd_root/foia_processing.html#data-sources",
    "href": "qmd_root/foia_processing.html#data-sources",
    "title": "FOIA Document OCR Processing",
    "section": "2.1 Data Sources",
    "text": "2.1 Data Sources\n\n2.1.1 Raw FOIA Responses\nThe original FOIA responses are stored in two formats:\n\nPDFs: raw/foia_pdfs/ - Original scanned documents\nHTML: raw/foia_html/ - OCR’d versions for easier extraction\n\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"here\",         # File path management\n  \"knitr\",        # Dynamic report generation\n  \"kableExtra\",   # Enhanced table formatting\n  \"ggplot2\",      # Data visualization\n  \"patchwork\",    # Plot composition and layout\n  \"scales\",       # Axis scaling and formatting\n  \"tidyr\",        # Data tidying and reshaping\n  \"tibble\",       # Modern data frames\n  \"flextable\",    # Advanced table formatting\n  \"DT\",           # Interactive tables\n  \"cowplot\",      # Plotting composition\n  \"sf\",           # Simple Features for spatial data\n  \"usmap\"        # Mapping US states\n)\n  \n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(here)\n  library(knitr)\n  library(kableExtra)\n  library(ggplot2)\n  library(patchwork)\n  library(scales)\n  library(tidyr)\n  library(tibble)\n  library(flextable)\n  library(cowplot)\n  library(sf)\n  library(usmap)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\nif (all(loaded_packages)) {\n  message(\"All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}\n\n# Display options\noptions(tibble.width = Inf)\noptions(dplyr.summarise.inform = FALSE)\n\n# Path to per-state files (run notebook from analysis/)\nbase_dir &lt;- here(\"..\")\nper_state &lt;- here(\"data\", \"foia\", \"intermediate\")\n\n# ------------------------------------------------------------------\n# 1. Discover available per-state CSV files\n# ------------------------------------------------------------------\nstate_files &lt;- list.files(per_state, pattern = \"*_foia_data\\\\.csv$\", full.names = TRUE)\n\nif (length(state_files) == 0) {\n  stop(paste(\"No per-state FOIA files found in\", per_state, \". Check the folder path.\"))\n}\n\nstem_to_state &lt;- function(stem) {\n  toks &lt;- str_split(stem, \"_\")[[1]]\n  if (\"foia\" %in% toks) {\n    toks &lt;- toks[1:(which(toks == \"foia\") - 1)]\n  }\n  paste(tools::toTitleCase(toks), collapse = \" \")\n}\n\nstates_available &lt;- map_chr(basename(state_files), ~ stem_to_state(str_remove(.x, \"_foia_data\\\\.csv\")))\n\ncat(paste(\"✓ Found\", length(state_files), \"per-state files:\\n\"))\nfor (s in states_available) {\n  cat(paste(\"  •\", s, \"\\n\"))\n}\n\n# ------------------------------------------------------------------\n# 2. Initialize empty containers for the loop that follows\n# ------------------------------------------------------------------\nfoia_combined &lt;- tibble()\nfoia_state_metadata &lt;- list()\n\n\n✓ Found 7 per-state files:\n  • California \n  • Florida \n  • Indiana \n  • Maine \n  • Nevada \n  • South Dakota \n  • Texas"
  },
  {
    "objectID": "qmd_root/foia_processing.html#processing-workflow",
    "href": "qmd_root/foia_processing.html#processing-workflow",
    "title": "FOIA Document OCR Processing",
    "section": "2.2 Processing workflow",
    "text": "2.2 Processing workflow\nFor transparency, each state file is processed independently then merged into a single combined long‑format table (foia_combined):\n\nLoad one file per state from data/foia/intermediate/.\nAppend its rows to foia_combined. A parallel dataframe, foia_state_metadata, records what each state reported (counts, percentages, which categories) and any state-specific characteristics (e.g. Nevada’s “flags” terminology).\nQuality‑check each state:\n\nverify that race and gender percentages sum to ≈ 100 % when provided,\nconfirm that demographic counts sum to the state’s reported total profiles,\ncalculate any missing counts or percentages and tag those rows value_source = \"calculated\".\n\nSave outputs\n\ndata/foia/final/foia_data_clean.csv — the fully combined tidy table with both reported and calculated values,\ndata/foia/intermediate/foia_state_metadata.csv — one row per state summarising coverage and caveats. After QC passes, freeze foia_data_clean.csv to data/v1.0/FOIA_demographics.csv."
  },
  {
    "objectID": "qmd_root/foia_processing.html#helper-functions",
    "href": "qmd_root/foia_processing.html#helper-functions",
    "title": "FOIA Document OCR Processing",
    "section": "2.3 Helper Functions",
    "text": "2.3 Helper Functions\nThe functions below perform each transformation required for harmonizing the state‑level FOIA tables.\n\n2.3.1 Data Processing Helper Functions Reference\n\n\n\n\n\n\n\n\nFunction\nDefinition\nParameters\n\n\n\n\nload_state()\nLoads and preprocesses state FOIA data files, handling numeric conversion and validation\npath: File path to state CSV\n\n\nenhanced_glimpse()\nProvides an enhanced data overview with column types, missing values, unique counts, and unique values\ndf: Input dataframe\n\n\nfill_demographic_gaps()\nFills missing gender counts and adds Unknown race category when totals permit calculation\ndf: Input dataframe\n\n\nadd_combined()\nCreates Combined offender type by summing Convicted Offender and Arrestee counts when missing\ndf: Input dataframe\n\n\nadd_percentages()\nDerives percentage values from counts for all demographic categories\ndf: Input dataframe\n\n\ncounts_consistent()\nVerifies that demographic counts sum to total_profiles for each offender type\ndf: Input dataframe\n\n\npercentages_consistent()\nVerifies that percentages sum to 100 ± 0.5% for each category\ndf: Input dataframe\n\n\nreport_status()\nReports what data types (counts/percentages/both) are available for a category\ndf: Input dataframe, category: race or gender\n\n\nverify_category_totals()\nCompares demographic sums against reported totals and shows differences\ndf: Input dataframe\n\n\nverify_percentage_consistency()\nCompares reported vs calculated percentages for consistency\ndf_combined: Combined dataframe, state_name: State name\n\n\ncalculate_combined_totals()\nCalculates Combined totals by summing across offender types\ndf: Input dataframe, state_name: State name\n\n\ncalculate_percentages()\nCalculates percentages from counts for demographic categories\ndf_combined: Combined dataframe, state_name: State name\n\n\ncalculate_counts_from_percentages()\nCalculates counts from percentages for demographic categories\ndf_combined: Combined dataframe, state_name: State name\n\n\nstandardize_offender_types()\nStandardizes offender type names to consistent terminology\ndf: Input dataframe\n\n\nprepare_state_for_combined()\nPrepares state data for inclusion in combined dataset with proper columns\ndf: Input dataframe, state_name: State name\n\n\nformat_compact()\nFormats large numbers with K/M suffixes for readability\nx: Numeric value\n\n\ncreate_pie_chart()\nCreates pie charts for specific demographic categories\ndata: Input data, offender_type, category, value_type, title, show_values\n\n\ncreate_state_visualizations()\nCreates comprehensive pie chart visualizations for all metrics\ndf_combined: Combined dataframe, state_name: State name\n\n\ncreate_demographic_bar_charts()\nCreates side-by-side bar charts for gender and race distributions\ndf_combined: Combined dataframe, state_name: State name\n\n\nadd_state_metadata()\nCreates and appends a metadata record capturing state data characteristics including available offender types, demographic categories, data formats, and special features\ndf: Input dataframe, state_name: State name\n\n\nupdate_state_metadata()\nModifies existing state metadata to update QC results (count/percentage consistency) and append validation notes\nstate_name: State name, counts_ok: Count consistency flag, percentages_ok: Percentage consistency flag, notes_text: Additional notes\n\n\n\n\n\nShow helper functions setup\n# Columns retained from every raw table\nCOLS_NEEDED &lt;- c(\"state\", \"offender_type\", \"variable_category\",\n                 \"variable_detailed\", \"value\", \"value_type\")\n\n# ------------------------------------------------------------------\n# 1. Load and preprocess state files\n# ------------------------------------------------------------------\nload_state &lt;- function(path) {\n  \"\n  Read a *_foia_data.csv* file, enforce column order,\n  and convert &lt;1 to 0.5 so that trace counts are retained.\n  Execution halts if non-numeric values remain.\n  \"\n  df &lt;- read_csv(path, show_col_types = FALSE)\n  if (!\"state\" %in% colnames(df)) {\n    df &lt;- df %&gt;%\n      mutate(state = str_remove(basename(path), \"_foia_data\\\\.csv\") %&gt;%\n        str_replace_all(\"_\", \" \") %&gt;%\n        tools::toTitleCase())\n  }\n  df &lt;- df %&gt;% select(all_of(COLS_NEEDED))\n  df$value_source &lt;- \"reported\"\n\n  df &lt;- df %&gt;%\n    mutate(value = ifelse(value == \"&lt;1\", 0.5, value),\n           value = as.numeric(value))\n  \n  nonnumeric &lt;- df %&gt;% filter(is.na(value))\n  if (nrow(nonnumeric) &gt; 0) {\n    cat(paste(\"**Non-numeric rows in\", basename(path), \"; please amend**\\n\"))\n    print(nonnumeric)\n    stop(\"Numeric coercion failure\")\n  }\n  return(df)\n}\n\n\n# ------------------------------------------------------------------\n# 2. Enhanced glimpse\n# ------------------------------------------------------------------\n# Display data types for each column with unique values\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    Unique_Values = sapply(df, function(x) {\n      unique_vals &lt;- unique(x)\n      if (length(unique_vals) &gt; 10) {\n        paste(encodeString(as.character(unique_vals[1:10])), collapse = \", \", \"...\")\n      } else {\n        paste(encodeString(as.character(unique_vals)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows ×\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\n# ------------------------------------------------------------------\n# 3. Fill missing Male counts and Unknown race counts\n# ------------------------------------------------------------------\nfill_demographic_gaps &lt;- function(df) {\n  \"If exactly one gender or the Unknown race category is absent and\n  totals permit a residual, calculate and insert the missing count.\n  \"\n  inserts &lt;- list()\n  \n  for (ot in unique(df$offender_type)) {\n    tot &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"total\",\n             variable_detailed == \"total_profiles\",\n             value_type == \"count\")\n    \n    if (nrow(tot) == 0) next\n    \n    total &lt;- tot$value[1]\n    \n    # gender residual ----------------------------------------------\n    g &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"gender\",\n             value_type == \"count\")\n    \n    missing_gender &lt;- setdiff(c(\"Male\", \"Female\"), unique(g$variable_detailed))\n    if (nrow(g) == 1 && length(missing_gender) == 1) {\n      inserts[[length(inserts) + 1]] &lt;- tibble(\n        state = df$state[1],\n        offender_type = ot,\n        variable_category = \"gender\",\n        variable_detailed = missing_gender,\n        value = total - sum(g$value),\n        value_type = \"count\",\n        value_source = \"calculated\"\n      )\n    }\n    \n    # race residual -----------------------------------------------\n    r &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"race\",\n             value_type == \"count\")\n    \n    if (nrow(r) &gt; 0 && !\"Unknown\" %in% r$variable_detailed) {\n      gap &lt;- total - sum(r$value)\n      if (gap &gt; 0) {\n        inserts[[length(inserts) + 1]] &lt;- tibble(\n          state = df$state[1],\n          offender_type = ot,\n          variable_category = \"race\",\n          variable_detailed = \"Unknown\",\n          value = gap,\n          value_type = \"count\",\n          value_source = \"calculated\"\n        )\n      }\n    }\n  }\n  \n  if (length(inserts) &gt; 0) {\n    df &lt;- bind_rows(df, bind_rows(inserts))\n  }\n  return(df)\n}\n\n# ------------------------------------------------------------------\n# 4. Construct Combined offender type if absent (add_combined)\n# ------------------------------------------------------------------\nadd_combined &lt;- function(df) {\n  \"\n  When a state reports Convicted Offender and Arrestee counts but\n  omits Combined, create a Combined block by summing the two.\n  \"\n  if (\"Combined\" %in% df$offender_type) return(df)\n  \n  required &lt;- c(\"Convicted Offender\", \"Arrestee\")\n  if (!all(required %in% df$offender_type)) return(df)  # cannot construct\n  \n  summed &lt;- df %&gt;%\n    filter(value_type == \"count\") %&gt;%\n    group_by(variable_category, variable_detailed, value_type) %&gt;%\n    summarise(value = sum(value), .groups = \"drop\") %&gt;%\n    mutate(state = df$state[1],\n           offender_type = \"Combined\",\n           value_source = \"calculated\")\n  \n  return(bind_rows(df, summed))\n}\n\n# ------------------------------------------------------------------\n# 5. Derive percentages wherever only counts exist (add_percentages)\n# ------------------------------------------------------------------\nadd_percentages &lt;- function(df) {\n  \"\n  Ensure that every gender and race row has both count and percentage\n  values, derived from the offender-type total if necessary.\n  \"\n  totals &lt;- df %&gt;%\n    filter(variable_category == \"total\",\n           variable_detailed == \"total_profiles\",\n           value_type == \"count\") %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe()\n  \n  need_pct &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\")\n  \n  new_pct_rows &lt;- need_pct %&gt;%\n    rowwise() %&gt;%\n    mutate(has_percentage = nrow(df %&gt;%\n           filter(state == state,\n                  offender_type == offender_type,\n                  variable_category == variable_category,\n                  variable_detailed == variable_detailed,\n                  value_type == \"percentage\"))) %&gt;%\n    filter(has_percentage == 0) %&gt;%\n    mutate(value = round(value / totals[offender_type] * 100, 2),\n           value_type = \"percentage\",\n           value_source = \"calculated\") %&gt;%\n    select(-has_percentage)\n  \n  if (nrow(new_pct_rows) &gt; 0) {\n    df &lt;- bind_rows(df, new_pct_rows)\n  }\n  return(df)\n}\n\n# ------------------------------------------------------------------\n# 6. Counts consistency checks\n# ------------------------------------------------------------------\ncounts_consistent &lt;- function(df) {\n  \"\n  Verifies that demographic counts sum to total_profiles for each\n  offender type and category.\n  \"\n  demo_sum &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_value = sum(value), .groups = \"drop\")\n  \n  totals &lt;- df %&gt;%\n    filter(variable_category == \"total\",\n           variable_detailed == \"total_profiles\",\n           value_type == \"count\") %&gt;%\n    select(offender_type, value)\n  \n  merged &lt;- demo_sum %&gt;%\n    left_join(totals, by = \"offender_type\") %&gt;%\n    mutate(diff = abs(sum_value - value))\n  \n  all(merged$diff &lt; 1e-6)\n}\n\n# ------------------------------------------------------------------\n# 7. Percentage consistency checks\n# ------------------------------------------------------------------\n\npercentages_consistent &lt;- function(df) {\n  \"\n  Verifies that derived or reported percentages sum to 100 ± 0.5 %.\n  \"\n  result &lt;- df %&gt;%\n    filter(value_type == \"percentage\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_value = sum(value), .groups = \"drop\") %&gt;%\n    mutate(consistent = abs(sum_value - 100) &lt;= 0.5)\n  \n  all(result$consistent)\n}\n\n\n# ------------------------------------------------------------------\n# 8. Report status for each category\n# ------------------------------------------------------------------\n\n# Define columns needed for foia_combined\nreport_status &lt;- function(df, category) {\n  values &lt;- unique(df$value_type[df$variable_category == category])\n  \n  if (all(c(\"count\", \"percentage\") %in% values)) {\n    return(\"both\")\n  } else if (\"count\" %in% values) {\n    return(\"counts\")\n  } else if (\"percentage\" %in% values) {\n    return(\"percentages\")\n  } else {\n    return(\"neither\")\n  }\n}\n\n# ------------------------------------------------------------------\n# 9. Verify category totals\n# ------------------------------------------------------------------\n\nverify_category_totals &lt;- function(df) {\n  # 1 pull total_profiles per offender_type\n  total_map &lt;- df %&gt;%\n    filter(variable_category == \"total\", \n           variable_detailed == \"total_profiles\") %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  # 2 sum counts by offender_type and variable_category\n  demo_sum &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_counts = sum(value, na.rm = TRUE), .groups = \"drop\")\n  \n  # 3 attach total_profiles and compute difference\n  demo_sum &lt;- demo_sum %&gt;%\n    mutate(total_profiles = map_dbl(offender_type, ~total_map[[.x]]),\n           difference = total_profiles - sum_counts)\n  \n  # tidy columns order\n  demo_sum %&gt;%\n    select(offender_type, variable_category, total_profiles, \n           sum_counts, difference)\n}\n\n# ------------------------------------------------------------------\n# 10. Calculate Combined totals\n# ------------------------------------------------------------------\n\ncalculate_combined_totals &lt;- function(df, state_name) {\n  # Get all counts\n  counts_df &lt;- df %&gt;%\n    filter(value_type == 'count') %&gt;%\n    mutate(value_source = 'calculated')\n  \n  # Group by variable_category and variable_detailed, sum values\n  combined_sums &lt;- counts_df %&gt;%\n    group_by(variable_category, variable_detailed) %&gt;%\n    summarise(value = sum(value, na.rm = TRUE), .groups = \"drop\")\n  \n  # Create Combined rows\n  combined_rows &lt;- combined_sums %&gt;%\n    mutate(state = state_name,\n           offender_type = 'Combined',\n           value_type = 'count',\n           value_source = 'calculated') %&gt;%\n    select(all_of(COLS_NEEDED), value_source)\n  \n  return(combined_rows)\n}\n\n# ------------------------------------------------------------------\n# 11. Calculate percentages from counts\n# ------------------------------------------------------------------\n\ncalculate_percentages &lt;- function(df_combined, state_name) {\n  # Get total profiles for each offender type\n  totals_map &lt;- df_combined %&gt;%\n    filter(state == state_name,\n           variable_category == 'total',\n           variable_detailed == 'total_profiles') %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  percentage_rows &lt;- list()\n  \n  for (offender_type in names(totals_map)) {\n    total &lt;- totals_map[[offender_type]]\n    \n    # Get all demographic counts\n    demo_data &lt;- df_combined %&gt;%\n      filter(state == state_name,\n             offender_type == !!offender_type,\n             variable_category %in% c('gender', 'race'),\n             value_type == 'count')\n    \n    if (nrow(demo_data) &gt; 0) {\n      # Calculate percentage for each\n      demo_percentages &lt;- demo_data %&gt;%\n        mutate(value = round((value / total) * 100, 2),\n               value_type = 'percentage',\n               value_source = 'calculated') %&gt;%\n        select(all_of(COLS_NEEDED), value_source)\n      \n      percentage_rows &lt;- c(percentage_rows, list(demo_percentages))\n    }\n  }\n  \n  bind_rows(percentage_rows)\n}\n\n# ------------------------------------------------------------------\n# 12. Calculate counts from percentages\n# ------------------------------------------------------------------\n\ncalculate_counts_from_percentages &lt;- function(df_combined, state_name) {\n  # Get total profiles for each offender type\n  totals_map &lt;- df_combined %&gt;%\n    filter(state == state_name,\n           variable_category == 'total',\n           variable_detailed == 'total_profiles') %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  count_rows &lt;- list()\n  \n  for (offender_type in names(totals_map)) {\n    total &lt;- totals_map[[offender_type]]\n    \n    # Get all demographic percentages\n    demo_data &lt;- df_combined %&gt;%\n      filter(state == state_name,\n             offender_type == !!offender_type,\n             variable_category %in% c('gender', 'race'),\n             value_type == 'percentage')\n    \n    if (nrow(demo_data) &gt; 0) {\n      # Calculate count for each\n      demo_counts &lt;- demo_data %&gt;%\n        mutate(value = as.integer(round(total * (value / 100))),\n               value_type = 'count',\n               value_source = 'calculated') %&gt;%\n        select(all_of(COLS_NEEDED), value_source)\n      \n      count_rows &lt;- c(count_rows, list(demo_counts))\n    }\n  }\n  \n  bind_rows(count_rows)\n}\n\n# ------------------------------------------------------------------\n# 13. Standardize offender types\n# ------------------------------------------------------------------\n\nstandardize_offender_types &lt;- function(df) {\n  replacements &lt;- c(\n    'Offenders' = 'Convicted Offender',\n    'Convicted offenders' = 'Convicted Offender',\n    'Arrested offender' = 'Arrestee',\n    'All' = 'Combined'\n  )\n  \n  df %&gt;%\n    mutate(offender_type = recode(offender_type, !!!replacements))\n}\n\n# ------------------------------------------------------------------\n# 14. Prepare state data for combined dataset\n# ------------------------------------------------------------------\n\nprepare_state_for_combined &lt;- function(df, state_name) {\n  \n  df_prepared &lt;- df %&gt;%\n    select(any_of(COLS_NEEDED), value_source)\n  \n  df_prepared &lt;- df_prepared %&gt;%\n      mutate(value_source = case_when(\n        is.na(value_source) ~ \"calculated\",\n        value_source == \"\" ~ \"calculated\",\n        TRUE ~ value_source\n      ))\n\n  \n  df_prepared\n}\n\n# ------------------------------------------------------------------\n# 15. Compare reported vs calculated percentages\n# ------------------------------------------------------------------\n\nverify_percentage_consistency &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;%\n    filter(state == state_name)\n  \n  # Get all offender types that have both counts and percentages\n  offender_types &lt;- unique(state_data$offender_type)\n  \n  consistency_results &lt;- list()\n  \n  for (offender_type in offender_types) {\n    offender_data &lt;- state_data %&gt;%\n      filter(offender_type == !!offender_type)\n    \n    # Check if we have both reported and calculated percentages\n    for (category in c('gender', 'race')) {\n      reported_pcts &lt;- offender_data %&gt;%\n        filter(variable_category == !!category,\n               value_type == 'percentage',\n               value_source == 'reported')\n      \n      calculated_pcts &lt;- offender_data %&gt;%\n        filter(variable_category == !!category,\n               value_type == 'percentage',\n               value_source == 'calculated')\n      \n      if (nrow(reported_pcts) &gt; 0 && nrow(calculated_pcts) &gt; 0) {\n        # Compare each demographic value\n        for (i in 1:nrow(reported_pcts)) {\n          rep_row &lt;- reported_pcts[i, ]\n          calc_match &lt;- calculated_pcts %&gt;%\n            filter(variable_detailed == rep_row$variable_detailed)\n          \n          if (nrow(calc_match) &gt; 0) {\n            diff &lt;- abs(rep_row$value - calc_match$value[1])\n            consistency_results &lt;- c(consistency_results, list(data.frame(\n              offender_type = offender_type,\n              category = category,\n              variable = rep_row$variable_detailed,\n              reported = rep_row$value,\n              calculated = calc_match$value[1],\n              difference = diff,\n              consistent = diff &lt; 0.5\n            )))\n          }\n        }\n      }\n    }\n  }\n  \n  if (length(consistency_results) &gt; 0) {\n    consistency_df &lt;- bind_rows(consistency_results)\n    cat(paste0(\"\\nPercentage consistency check for \", state_name, \":\\n\"))\n    cat(paste0(\"All values consistent: \", all(consistency_df$consistent), \"\\n\"))\n    \n    if (!all(consistency_df$consistent)) {\n      cat(\"\\nInconsistent values:\\n\")\n      print(consistency_df %&gt;% filter(!consistent))\n    }\n    \n    return(all(consistency_df$consistent))\n  } else {\n    # No comparison possible - state only has one type of data\n    return(TRUE)\n  }\n}\n# ------------------------------------------------------------------\n# 16. Add compact formatting for large numbers\n# ------------------------------------------------------------------\n\nformat_compact &lt;- function(x) {\n  sapply(x, function(single_x) {\n    if (single_x &gt;= 1000000) {\n      if (single_x/1000000 == as.integer(single_x/1000000)) {\n        return(paste0(as.integer(single_x/1000000), \"M\"))\n      } else {\n        return(paste0(round(single_x/1000000, 1), \"M\"))\n      }\n    } else if (single_x &gt;= 1000) {\n      return(paste0(as.integer(single_x/1000), \"k\"))\n    } else {\n      return(paste0(as.integer(single_x)))\n    }\n  })\n}\n\n# ------------------------------------------------------------------\n# 17. Pie chart creation function\n# ------------------------------------------------------------------\n\ncreate_pie_chart &lt;- function(data, offender_type, category, value_type, title, show_values = FALSE) {\n  chart_data &lt;- data %&gt;%\n    filter(offender_type == !!offender_type,\n           variable_category == !!category,\n           value_type == !!value_type)\n  \n  # Check if we have data after filtering\n  if (nrow(chart_data) == 0) {\n    plot.new()\n    title(main = title, cex.main = 0.9)\n    text(0.5, 0.5, \"No data\", cex = 0.8)\n    return()\n  }\n  \n  # AGGREGATE DATA TO REMOVE DUPLICATES - KEY FIX\n  chart_data &lt;- chart_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarise(value = sum(value, na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  # Ensure consistent categories\n  if (category == 'gender') {\n    all_genders &lt;- data.frame(variable_detailed = c('Male', 'Female', 'Unknown'))\n    chart_data &lt;- chart_data %&gt;%\n      right_join(all_genders, by = \"variable_detailed\") %&gt;%\n      mutate(value = ifelse(is.na(value), 0, value)) %&gt;%\n      arrange(factor(variable_detailed, levels = c('Male', 'Female', 'Unknown')))\n  } else if (category == 'race') {\n    all_races &lt;- data.frame(variable_detailed = c('White', 'Black', 'Hispanic', \n                                                 'Asian', 'Native American', 'Other', 'Unknown'))\n    chart_data &lt;- chart_data %&gt;%\n      right_join(all_races, by = \"variable_detailed\") %&gt;%\n      mutate(value = ifelse(is.na(value), 0, value)) %&gt;%\n      arrange(factor(variable_detailed, levels = c('White', 'Black', 'Hispanic', \n                                                  'Asian', 'Native American', 'Other', 'Unknown')))\n  }\n  \n  # Filter out zero values and ensure we have data\n  chart_data &lt;- chart_data %&gt;% filter(value &gt; 0)\n  \n  if (nrow(chart_data) == 0) {\n    plot.new()\n    title(main = title, cex.main = 0.9)\n    text(0.5, 0.5, \"No data\", cex = 0.8)\n    return()\n  }\n  \n  # Define colors\n  if (category == 'gender') {\n    colors &lt;- c('Male' = '#4E79A7', 'Female' = '#E15759', 'Unknown' = '#BAB0AC')\n  } else {\n    colors &lt;- c('White' = '#4E79A7', 'Black' = '#F25E2B', 'Hispanic' = '#E14759',\n               'Asian' = '#76B7B2', 'Native American' = '#59A14F',\n               'Other' = '#9C755F', 'Unknown' = '#BAB0AC')\n  }\n  \n  # Filter colors to only include categories present in data\n  pie_colors &lt;- colors[names(colors) %in% chart_data$variable_detailed]\n  \n  # Calculate percentages\n  total_value &lt;- sum(chart_data$value)\n  chart_data &lt;- chart_data %&gt;%\n    mutate(pct = value / total_value * 100)\n  \n  # Create labels based on value_type and show_values\n  if (show_values && value_type == 'count') {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = paste0(variable_detailed, \"\\n(\", format(value, big.mark = \",\"), \")\"))\n  } else if (value_type == 'percentage') {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = paste0(variable_detailed, \"\\n(\", round(value, 1), \"%)\"))\n  } else {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = variable_detailed)\n  }\n  \n  # Only show labels for slices &gt;= 3%, otherwise empty string\n  chart_data &lt;- chart_data %&gt;%\n    mutate(label = ifelse(pct &gt;= 3, base_label, \"\"))\n  \n  # Create the pie chart\n  pie(chart_data$value, \n      labels = chart_data$label,\n      main = title,\n      col = pie_colors,\n      cex.main = 0.9,\n      cex = 0.8)\n  \n  # Add legend for small slices\n  small_slices &lt;- chart_data %&gt;% filter(pct &lt; 3)\n  if (nrow(small_slices) &gt; 0) {\n    legend_labels &lt;- paste0(small_slices$variable_detailed, \" (\", round(small_slices$pct, 1), \"%)\")\n    legend_colors &lt;- pie_colors[small_slices$variable_detailed]\n    \n    legend(\"bottomright\", \n           legend = legend_labels,\n           fill = legend_colors,\n           cex = 0.7,\n           bty = \"n\")\n  }\n}\n\n# ------------------------------------------------------------------\n# 18. State visualizations with 2 pies per row\n# ------------------------------------------------------------------\n\ncreate_state_visualizations &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;% filter(state == state_name)\n  \n  offender_types &lt;- sort(unique(state_data$offender_type))\n  plots &lt;- list()\n  \n  for (offender_type in offender_types) {\n    plots &lt;- c(plots, list(\n      create_pie_chart(state_data, offender_type, 'gender', 'count',\n                       paste(offender_type, \"Gender Counts\"), TRUE),\n      create_pie_chart(state_data, offender_type, 'gender', 'percentage',\n                       paste(offender_type, \"Gender Percentages\")),\n      create_pie_chart(state_data, offender_type, 'race', 'count',\n                       paste(offender_type, \"Race Counts\"), TRUE),\n      create_pie_chart(state_data, offender_type, 'race', 'percentage',\n                       paste(offender_type, \"Race Percentages\"))\n    ))\n  }\n}\n\n# ------------------------------------------------------------------\n# 19. Demographic bar chart function\n# ------------------------------------------------------------------\n\ncreate_demographic_bar_charts &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;%\n    filter(state == state_name)\n  \n  # Get offender types and ensure Combined is last\n  offender_types &lt;- state_data %&gt;%\n    filter(value_type == 'count') %&gt;%\n    pull(offender_type) %&gt;%\n    unique() %&gt;%\n    sort()\n  \n  if ('Combined' %in% offender_types) {\n    offender_types &lt;- c(setdiff(offender_types, 'Combined'), 'Combined')\n  }\n  \n  # Color palettes\n  gender_colors &lt;- c('Male' = '#4E79A7', 'Female' = '#E15759', 'Unknown' = '#BAB0AC')\n  race_colors &lt;- c(\n    'White' = '#4E79A7', \n    'Black' = '#F25E2B', \n    'Hispanic' = '#E14759',\n    'Asian' = '#76B7B2',\n    'Native American' = '#59A14F',\n    'Other' = '#9C755F',\n    'Unknown' = '#BAB0AC'\n  )\n  \n  # Gender data - ensure no duplicates by summing values\n  gender_data &lt;- state_data %&gt;%\n    filter(variable_category == 'gender',\n           value_type == 'count') %&gt;%\n    group_by(offender_type, variable_detailed) %&gt;%\n    summarize(value = sum(value, na.rm = TRUE), .groups = 'drop') %&gt;%\n    complete(offender_type, variable_detailed = c('Male', 'Female', 'Unknown'), \n             fill = list(value = 0))\n  \n  # Race data - ensure no duplicates by summing values\n  race_data &lt;- state_data %&gt;%\n    filter(variable_category == 'race',\n           value_type == 'count') %&gt;%\n    group_by(offender_type, variable_detailed) %&gt;%\n    summarize(value = sum(value, na.rm = TRUE), .groups = 'drop') %&gt;%\n    complete(offender_type, \n             variable_detailed = c('White', 'Black', 'Hispanic', \n                                  'Asian', 'Native American', 'Other', 'Unknown'), \n             fill = list(value = 0))\n  \n  # Create separate plots - one per row\n  par(mfrow = c(2, 1), mar = c(5, 9, 4, 9), oma = c(0, 0, 2, 0)) # Increased right margin for legend\n  \n  # Gender plot - ordered by total volume\n  gender_plot_data &lt;- gender_data %&gt;%\n    filter(variable_detailed %in% c('Male', 'Female', 'Unknown')) %&gt;%\n    mutate(offender_type = factor(offender_type, levels = rev(offender_types)))\n  \n  # Order gender categories by total volume (largest at bottom)\n  gender_order &lt;- gender_plot_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarize(total = sum(value)) %&gt;%\n    arrange(total) %&gt;%\n    pull(variable_detailed)\n  \n  gender_plot_data &lt;- gender_plot_data %&gt;%\n    mutate(variable_detailed = factor(variable_detailed, levels = gender_order))\n  \n  # Reshape for barplot\n  gender_matrix &lt;- gender_plot_data %&gt;%\n    pivot_wider(names_from = variable_detailed, values_from = value) %&gt;%\n    as.data.frame() %&gt;%\n    column_to_rownames(\"offender_type\") %&gt;%\n    as.matrix()\n  \n  # Ensure all columns exist\n  for (gender in gender_order) {\n    if (!gender %in% colnames(gender_matrix)) {\n      gender_matrix &lt;- cbind(gender_matrix, temp = 0)\n      colnames(gender_matrix)[ncol(gender_matrix)] &lt;- gender\n    }\n  }\n  \n  # Reorder columns by volume\n  gender_matrix &lt;- gender_matrix[, as.character(gender_order), drop = FALSE]\n  \n  # Format x-axis labels with \"k\" for thousands\n  max_x &lt;- max(rowSums(gender_matrix))\n  x_breaks &lt;- pretty(c(0, max_x))\n  x_labels &lt;- ifelse(x_breaks &gt;= 1000, \n                    paste0(x_breaks/1000, \"k\"), \n                    as.character(x_breaks))\n  \n  barplot(t(gender_matrix), \n          horiz = TRUE,\n          las = 1,\n          col = gender_colors[colnames(gender_matrix)],\n          main = 'Gender Distribution',\n          xlab = 'Number of Profiles',\n          xaxt = 'n',  # Remove default x-axis\n          legend.text = FALSE,  # Don't show legend in plot area\n          args.legend = list(x = \"right\", bty = \"n\", inset = c(-0.2, 0)))\n  \n  # Add custom x-axis with formatted labels\n  axis(1, at = x_breaks, labels = x_labels)\n  \n  # Add legend outside the plot area\n  legend(\"topright\", \n         legend = colnames(gender_matrix), \n         fill = gender_colors[colnames(gender_matrix)],\n         bty = \"n\", \n         xpd = TRUE,  # Allow plotting outside main area\n         inset = c(-0.25, 0),  # Move legend to the right\n         cex = 0.8)\n  \n  # Race plot - ordered by total volume\n  race_plot_data &lt;- race_data %&gt;%\n    mutate(offender_type = factor(offender_type, levels = rev(offender_types)))\n  \n  # Order race categories by total volume (largest at bottom)\n  race_order &lt;- race_plot_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarize(total = sum(value)) %&gt;%\n    arrange(total) %&gt;%\n    pull(variable_detailed)\n  \n  race_plot_data &lt;- race_plot_data %&gt;%\n    mutate(variable_detailed = factor(variable_detailed, levels = race_order))\n  \n  # Reshape for barplot\n  race_matrix &lt;- race_plot_data %&gt;%\n    pivot_wider(names_from = variable_detailed, values_from = value) %&gt;%\n    as.data.frame() %&gt;%\n    column_to_rownames(\"offender_type\") %&gt;%\n    as.matrix()\n  \n  # Ensure all columns exist\n  for (race in race_order) {\n    if (!race %in% colnames(race_matrix)) {\n      race_matrix &lt;- cbind(race_matrix, temp = 0)\n      colnames(race_matrix)[ncol(race_matrix)] &lt;- race\n    }\n  }\n  \n  # Reorder columns by volume\n  race_matrix &lt;- race_matrix[, as.character(race_order), drop = FALSE]\n  \n  # Format x-axis labels with \"k\" for thousands\n  max_x_race &lt;- max(rowSums(race_matrix))\n  x_breaks_race &lt;- pretty(c(0, max_x_race))\n  x_labels_race &lt;- ifelse(x_breaks_race &gt;= 1000, \n                         paste0(x_breaks_race/1000, \"k\"), \n                         as.character(x_breaks_race))\n  \n  barplot(t(race_matrix), \n          horiz = TRUE,\n          las = 1,\n          col = race_colors[colnames(race_matrix)],\n          main = 'Race Distribution',\n          xlab = 'Number of Profiles',\n          xaxt = 'n',  # Remove default x-axis\n          legend.text = FALSE)  # Don't show legend in plot area\n  \n  # Add custom x-axis with formatted labels\n  axis(1, at = x_breaks_race, labels = x_labels_race)\n  \n  # Add legend outside the plot area\n  legend(\"topright\", \n         legend = colnames(race_matrix), \n         fill = race_colors[colnames(race_matrix)],\n         bty = \"n\", \n         xpd = TRUE,  # Allow plotting outside main area\n         inset = c(-0.25, 0),  # Move legend to the right\n         cex = 0.8)\n  \n  title(paste(state_name, \"Demographic Distribution\"), outer = TRUE, cex.main = 1.5)\n}\n\n# ------------------------------------------------------------------\n# 20. Add state's metadata\n# ------------------------------------------------------------------\n\nadd_state_metadata &lt;- function(state_name, state_df) {\n  \n  raw_data &lt;- state_df %&gt;% filter(value_source == \"reported\")\n  offender_types_reported &lt;- unique(raw_data$offender_type)\n  \n  has_unknown &lt;- any(raw_data$variable_detailed == \"Unknown\", na.rm = TRUE)\n  has_other &lt;- any(raw_data$variable_detailed == \"Other\", na.rm = TRUE)\n  has_crosstab &lt;- any(raw_data$variable_category == \"gender_race\", na.rm = TRUE)\n  \n  nonstandard_terms &lt;- any(\n    grepl(\"All|Offenders\", raw_data$offender_type, ignore.case = TRUE),\n    grepl(\"Caucasian|African American| American Indian\", raw_data$variable_detailed, ignore.case = TRUE),\n    grepl(\"flag\", raw_data$variable_detailed, ignore.case = TRUE))\n  \n  new_row &lt;- tibble(\n    state = state_name,\n    race_data_provided = report_status(raw_data, \"race\"),\n    gender_data_provided = report_status(raw_data, \"gender\"),\n    total_profiles_provided = report_status(\n      raw_data %&gt;% filter(variable_category == \"total\"), \"total\"\n    ),\n    convicted_offender_reported = \"Convicted Offender\" %in% offender_types_reported,\n    arrestee_reported = \"Arrestee\" %in% offender_types_reported,\n    combined_reported = \"Combined\" %in% offender_types_reported,\n    has_unknown_category = has_unknown,\n    has_other_category = has_other,\n    uses_nonstandard_terminology = nonstandard_terms,\n    provides_crosstabulation = has_crosstab,\n    counts_sum_to_total = NA,\n    percentages_sum_to_100 = NA,\n    total_calculated_combined = !(\"Combined\" %in% offender_types_reported),\n    notes = \"\"\n  )\n  \n  foia_state_metadata &lt;&lt;- bind_rows(foia_state_metadata, new_row)\n  \n  cat(\"✓ Metadata added for:\", state_name, \"\\n\")\n  return(invisible(TRUE))\n}\n\n# ------------------------------------------------------------------\n# 21. Function to update a state's metadata after QC checks\n# ------------------------------------------------------------------\nupdate_state_metadata &lt;- function(state_name, \n                                  counts_ok = NA, \n                                  percentages_ok = NA, \n                                  notes_text = NULL) {\n  \n  row_index &lt;- which(foia_state_metadata$state == state_name)\n  \n  if (length(row_index) == 0) {\n    warning(\"State not found in metadata: \", state_name)\n    return(FALSE)\n  }\n  \n  if (!is.na(counts_ok)) {\n    foia_state_metadata$counts_sum_to_total[row_index] &lt;&lt;- counts_ok\n  }\n  if (!is.na(percentages_ok)) {\n    foia_state_metadata$percentages_sum_to_100[row_index] &lt;&lt;- percentages_ok\n  }\n  if (!is.null(notes_text)) {\n    current_notes &lt;- foia_state_metadata$notes[row_index]\n    if (current_notes == \"\") {\n      foia_state_metadata$notes[row_index] &lt;&lt;- notes_text\n    } else {\n      foia_state_metadata$notes[row_index] &lt;&lt;- paste(current_notes, notes_text, sep = \"; \")\n    }\n  }\n  \n  cat(\"✓ Metadata updated for:\", state_name, \"\\n\")\n}"
  },
  {
    "objectID": "qmd_root/foia_processing.html#file-structure-and-contents",
    "href": "qmd_root/foia_processing.html#file-structure-and-contents",
    "title": "FOIA Document OCR Processing",
    "section": "2.4 File Structure and Contents",
    "text": "2.4 File Structure and Contents\n\n2.4.1 State-Specific Files: data/foia/intermediate/[state]_foia_data.csv\nPurpose: Individual files for each state containing only their reported data.\nStructure: Long format with columns:\n\nstate: State name\noffender_type: Category of individuals (Convicted Offender, Arrestee, Combined, etc.)\nvariable_category: Type of data (total, gender, race, gender_race)\nvariable_detailed: Specific value (e.g., Male, Female, African American)\nvalue: The reported number or percentage\nvalue_type: Whether value is a “count” or “percentage”\ndate: Date of data snapshot, if reported\n\n\n\nShow per-state files loading code\nca_raw &lt;- load_state(here(per_state, \"california_foia_data.csv\"))\nfl_raw &lt;- load_state(here(per_state, \"florida_foia_data.csv\"))\nin_raw &lt;- load_state(here(per_state, \"indiana_foia_data.csv\"))\nme_raw &lt;- load_state(here(per_state, \"maine_foia_data.csv\"))\nnv_raw &lt;- load_state(here(per_state, \"nevada_foia_data.csv\"))\nsd_raw &lt;- load_state(here(per_state, \"south_dakota_foia_data.csv\"))\ntx_raw &lt;- load_state(here(per_state, \"texas_foia_data.csv\"))\n\n\n\n\n2.4.2 Raw Data Characteristics\nThe following table summarizes the structure and content of the data as originally received from each state prior to any standardization, calculation, or processing.\n\n\n\n\n\n\n\n\n\n\n\nState\nOffender Types\nValue Types\nTotal Profiles\nAction Needed\nKey Reporting Notes\n\n\n\n\nCalifornia\nCO, A\nCounts only\nReported per offender type\nAdd Unknown Race, Calculate % & Combined, Standardize Terminology\nDiscrepancy in Race: counts &lt; total profiles; Non-standard terminology (Caucasian and African American)\n\n\nFlorida\nCOMB\nCounts + %\nReported\nStandardize Terminology\nNon-standard terminology (Caucasian and African American)\n\n\nIndiana\nCO, A, COMB\nPercentage (Counts for totals only)\nReported per offender type\nCalculate Counts & Total Profiles Combined, Fix % inconsistency, Standardize Terminology\nDemographics only for Combined; Other race category as “&lt;1”; Non-standard terminology (Caucasian)\n\n\nMaine\nCOMB\nCounts + %\nReported\nSolve counts and Percentage inconsistency\n\n\n\nNevada\nCO, A, COMB\nCounts + %\nReported for all types\nStandardize Terminology\nNon-standard terminology (All, total_flags and American Indian)\n\n\nSouth Dakota\nCOMB\nCounts + %\nReported\nStandardize Terminology, Solve counts and % inconsistency\nIncludes gender×race cross-tabulation; Non-standard terminology\n\n\nTexas\nCO, A\nCounts only\nReported per offender type\nCalculate Male counts, Solve counts inconsistency, Calculate % & Combined, Standardize Terminology\nOnly female gender was reported; Non-standard term (Offenders, Caucasian, and African American)\n\n\n\nLegend:\n\nCO: Convicted Offender\nAR: Arrestee\nCOMB: Combined Total (all profiles)\nCounts + %: Both raw numbers and percentages were provided"
  },
  {
    "objectID": "qmd_root/foia_processing.html#prepare-combined-dataset",
    "href": "qmd_root/foia_processing.html#prepare-combined-dataset",
    "title": "FOIA Document OCR Processing",
    "section": "2.5 Prepare Combined Dataset",
    "text": "2.5 Prepare Combined Dataset\nThe goal of this step is to transform each state’s raw data into a standardized format before appending it to the master foia_combined DataFrame. This ensures consistency and enables seamless analysis across all seven states.\nThe ideal, standardized state dataset ready for combination must have the following columns:\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nExample Values\n\n\n\n\nstate\nThe name of the state.\n\"California\", \"Florida\"\n\n\noffender_type\nThe category of offender profile.\n\"Convicted Offender\", \"Arrestee\", \"Combined\"\n\n\nvariable_category\nThe broad demographic category.\n\"race\", \"gender\", \"total\", \"gender_race\"\n\n\nvariable_detailed\nThe specific value within the category.\n\"White\", \"Male\", \"total_profiles\", \"Male_White\"\n\n\nvalue\nThe numerical value for the metric.\n150000, 25.8\n\n\nvalue_type\nThe type of metric the value represents.\n\"count\", \"percentage\"\n\n\nvalue_source\nWhether the data was provided or derived.\n\"reported\", \"calculated\"\n\n\n\n\n\nShow the master foia_combined dataframe elaboration code\n# ------------------------------------------------------------------\n# Initialize the master foia_combined dataframe with correct schema\n# This empty structure ensures all state data is appended consistently\n# ------------------------------------------------------------------\n\nfoia_combined &lt;- tibble( state = character(),\noffender_type = character(),\nvariable_category = character(),\nvariable_detailed = character(), \nvalue = numeric(),\nvalue_type = character(),\nvalue_source = character()\n)\n\n# Create a data dictionary for foia_combined\nschema_dict &lt;- tribble(\n  ~Column,             ~Type,        ~Description, \n  \"state\",             \"character\",  \"'California', 'Florida'\",\n  \"offender_type\",     \"character\",  \"'Convicted Offender', 'Arrestee', 'Combined'\",\n  \"variable_category\", \"character\",  \"'race', 'gender', 'total', 'gender_race'\",\n  \"variable_detailed\", \"character\",  \"'White', 'Male', 'total_profiles', 'Male_White'\",\n  \"value\",             \"numeric\",    \"150000, 25.8\",\n  \"value_type\",        \"character\",  \"'count', 'percentage'\",\n  \"value_source\",      \"character\",  \"'reported', 'calculated'\"\n)\n\n# Turn into a nice flextable\nflextable(schema_dict) %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  set_header_labels(\n    Column = \"Column Name\",\n    Type = \"Data Type\",\n    Description = \"Example Values to be added\"\n  )\n\n\nColumn NameData TypeExample Values to be addedstatecharacter'California', 'Florida'offender_typecharacter'Convicted Offender', 'Arrestee', 'Combined'variable_categorycharacter'race', 'gender', 'total', 'gender_race'variable_detailedcharacter'White', 'Male', 'total_profiles', 'Male_White'valuenumeric150000, 25.8value_typecharacter'count', 'percentage'value_sourcecharacter'reported', 'calculated'"
  },
  {
    "objectID": "qmd_root/foia_processing.html#prepare-metadata-documentation-table",
    "href": "qmd_root/foia_processing.html#prepare-metadata-documentation-table",
    "title": "FOIA Document OCR Processing",
    "section": "2.6 Prepare Metadata Documentation Table",
    "text": "2.6 Prepare Metadata Documentation Table\nThis section creates a comprehensive metadata table (foia_state_metadata) to document the original content and structure of each state’s FOIA response before any processing or cleaning was applied.\nThis serves as a permanent record of data provenance, ensuring transparency and reproducibility by clearly distinguishing between what was provided by the states and what was calculated during analysis.\nKey Documentation Captured:\n\nData Types Provided: Whether each state reported counts, percentages, or both for race, gender, and total profiles.\nOffender Categories Reported: Which offender types (Convicted Offender, Arrestee, Combined) were originally included.\nDemographic Granularity: Presence of ‘Unknown’ or ‘Other’ categories and gender-race cross-tabulations.\nTerminology & Anomalies: Use of non-standard terms (e.g., “flags,” “offenders”) and other state-specific reporting notes.\nQC Results: Flags for whether cleaned data passes consistency checks (counts sum to totals, percentages sum to ~100%).\n\n\n\nShow the foia_state_metadata table elaboration code\n# ------------------------------------------------------------------\n# Initialize the foia_state_metadata as a tibble (not a list of lists)\n# This makes it easier to add rows and ensures consistent structure.\n# ------------------------------------------------------------------\n\n# Define the full schema for our metadata table\nfoia_state_metadata &lt;- tibble(\n  state = character(),\n  race_data_provided = character(),\n  gender_data_provided = character(),\n  total_profiles_provided = character(), \n  convicted_offender_reported = logical(),\n  arrestee_reported = logical(),\n  combined_reported = logical(),\n  has_unknown_category = logical(),\n  has_other_category = logical(),\n  uses_nonstandard_terminology = logical(),\n  provides_crosstabulation = logical(),\n  counts_sum_to_total = logical(),\n  percentages_sum_to_100 = logical(),\n  total_calculated_combined = logical(),\n  notes = character()\n)\n\n# Build data dictionary for foia_state_metadata\nschema_dict_meta &lt;- tribble(\n  ~Column,                        ~Type,       ~Description,\n  \"state\",                        \"character\", \"State name (e.g., 'California', 'Florida')\",\n  \"race_data_provided\",           \"character\", \"Race data availability: 'counts', 'percentages', 'both', 'none'\",\n  \"gender_data_provided\",         \"character\", \"Gender data availability: 'counts', 'percentages', 'both', 'none'\",\n  \"total_profiles_provided\",      \"character\", \"Total profiles availability: 'counts', 'percentages', 'both', 'none'\",\n  \"convicted_offender_reported\",  \"logical\",   \"Was convicted offender data reported?\",\n  \"arrestee_reported\",            \"logical\",   \"Was arrestee data reported?\",\n  \"combined_reported\",            \"logical\",   \"Was combined category reported?\",\n  \"has_unknown_category\",         \"logical\",   \"Does the state include 'Unknown' category?\",\n  \"has_other_category\",           \"logical\",   \"Does the state include 'Other' category?\",\n  \"uses_nonstandard_terminology\", \"logical\",   \"Does the state use non-standard terms?\",\n  \"provides_crosstabulation\",     \"logical\",   \"Does the state provide crosstabs (e.g., gender x race)?\",\n  \"counts_sum_to_total\",          \"logical\",   \"Do reported counts sum to the total?\",\n  \"percentages_sum_to_100\",       \"logical\",   \"Do reported percentages sum to ~100%?\",\n  \"total_calculated_combined\",    \"logical\",   \"Did we calculate combined total manually?\",\n  \"notes\",                        \"character\", \"Free-text notes for state-specific caveats\"\n)\n\n# Render with flextable\nflextable(schema_dict_meta) %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  set_header_labels(\n    Column = \"Column Name\",\n    Type = \"Data Type\",\n    Description = \"Meaning\"\n  )\n\n\nColumn NameData TypeMeaningstatecharacterState name (e.g., 'California', 'Florida')race_data_providedcharacterRace data availability: 'counts', 'percentages', 'both', 'none'gender_data_providedcharacterGender data availability: 'counts', 'percentages', 'both', 'none'total_profiles_providedcharacterTotal profiles availability: 'counts', 'percentages', 'both', 'none'convicted_offender_reportedlogicalWas convicted offender data reported?arrestee_reportedlogicalWas arrestee data reported?combined_reportedlogicalWas combined category reported?has_unknown_categorylogicalDoes the state include 'Unknown' category?has_other_categorylogicalDoes the state include 'Other' category?uses_nonstandard_terminologylogicalDoes the state use non-standard terms?provides_crosstabulationlogicalDoes the state provide crosstabs (e.g., gender x race)?counts_sum_to_totallogicalDo reported counts sum to the total?percentages_sum_to_100logicalDo reported percentages sum to ~100%?total_calculated_combinedlogicalDid we calculate combined total manually?notescharacterFree-text notes for state-specific caveats"
  },
  {
    "objectID": "qmd_root/foia_processing.html#california-ca",
    "href": "qmd_root/foia_processing.html#california-ca",
    "title": "FOIA Document OCR Processing",
    "section": "3.1 California (CA)",
    "text": "3.1 California (CA)\nOverview: California supplies counts only for gender and race plus a separate total for each offender type; no percentages are reported.\n\n3.1.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1601Californiaoffender_typecharacter1602Convicted Offender, Arresteevariable_categorycharacter1603total, gender, racevariable_detailedcharacter1608total_profiles, Female, Male, Unknown, African American, Caucasian, Hispanic, Asianvaluenumeric160162019899 ..., 751822 ..., 309827 ..., 1603222 ..., 106850 ..., 208225 ..., 524231 ..., 19366 ..., 368952 ..., 588555 ...value_typecharacter1601countvalue_sourcecharacter1601reportedData frame dimensions: 16 rows × 7 columns\n\n\n\n\n3.1.2 Verify Data Consistency\nRuns the first quality check using the verify_category_totals() and counts_consistent() functions.\nThis identifies any immediate discrepancies, such as the sum of demographic counts not matching the reported total profiles, which flags data issues that need to be resolved.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n751822\n751822\n0\n\n\nArrestee\nrace\n751822\n655695\n96127\n\n\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\nConvicted Offender\nrace\n2019899\n1626012\n393887\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\n\n3.1.3 Address Data Gaps\n\n3.1.3.1 Create Unknown Category\n\n“Racial classification is not considered a required field on the collection card; thus, an unknown number of offenders may have no racial classification listed.” — California DOJ FOIA letter, July 10 2018 (raw/foia_pdfs/FOIA_RacialComp_California.pdf)\n\nThe 393,887 Convicted Offender profiles and 96,127 Arrestee profiles that do not appear in any of the four reported race categories must belong to an unreported “Unknown” category.\nThe calculated values are added with a value_source = \"calculated\" tag to maintain transparency about what was provided versus what was derived.\n\n\nShow unknown addition code\n# Start with the raw data\nca_clean &lt;- ca_raw\n\n# Add Unknown race category to reconcile totals\nca_clean &lt;- fill_demographic_gaps(ca_clean)\n\n# Verify the fix\ncat(\"Category totals after adding Unknown race category:\\n\")\nverify_category_totals(ca_clean) %&gt;% kable() %&gt;% kable_styling()\n\ncat(\"\\nCounts consistency after adding Unknown:\\n\")\ncat(paste(\"All counts consistent:\", counts_consistent(ca_clean), \"\\n\"))\n\n\nCategory totals after adding Unknown race category:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n751822\n751822\n0\n\n\nArrestee\nrace\n751822\n751822\n0\n\n\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\nConvicted Offender\nrace\n2019899\n2019899\n0\n\n\n\n\n\n\nCounts consistency after adding Unknown:\nAll counts consistent: TRUE \n\n\n\n\n3.1.3.2 Create Combined Totals\nSince California only reported data for “Convicted Offender” and “Arrestee” separately.\nThis step uses the add_combined() helper function to calculate a new “Combined” offender type by summing the counts from the other two categories.\n\n\nShow combined addition code\n# Calculate Combined totals using helper function\nca_clean &lt;- add_combined(ca_clean)\n\ncat(\"✓ Created Combined totals for California\\n\")\n\n# Show the Combined total\ncombined_total &lt;- ca_clean %&gt;%\n  filter(offender_type == \"Combined\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\n\n\n✓ Created Combined totals for California\nCombined total profiles: 2,771,721 \n\n\n\n\n3.1.3.3 Calculate Percentages\nTransforms the data from counts into percentages for comparative analysis.\nThe add_percentages() helper function calculates each demographic group’s proportion relative to its offender type’s total.\nA final consistency check ensures all percentages logically sum to approximately 100%.\n\n\nShow percentage calculation code\n# Derive percentages from counts\nca_clean &lt;- add_percentages(ca_clean)\n\ncat(\"✓ Added percentages for all demographic categories\\n\")\n\n# Check percentage consistency\ncat(\"Percentage consistency check:\\n\")\ncat(paste(\"All percentages sum to ~100%:\", percentages_consistent(ca_clean), \"\\n\\n\"))\n\n# Show current data availability\ncat(\"Final data availability:\\n\")\ncat(paste(\"Race data:\", report_status(ca_clean, \"race\"), \"\\n\"))\ncat(paste(\"Gender data:\", report_status(ca_clean, \"gender\"), \"\\n\"))\n\n\n✓ Added percentages for all demographic categories\nPercentage consistency check:\nAll percentages sum to ~100%: TRUE \n\nFinal data availability:\nRace data: both \nGender data: both \n\n\n\n\n3.1.3.4 Standardize Terminology\nCalifornia uses “African American” instead of “Black” and “Caucasian” instead of “White”.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nca_clean &lt;- ca_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"African American\" ~ \"Black\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"✓ Standardized terminology: 'African American' → 'Black'\\n\")\n\nca_clean &lt;- ca_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"✓ Standardized terminology: 'Caucasian' → 'White'\\n\")\n\n\n✓ Standardized terminology: 'African American' → 'Black'\n✓ Standardized terminology: 'Caucasian' → 'White'\n\n\n\n\n\n3.1.4 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow California data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nca_prepared &lt;- prepare_state_for_combined(ca_clean, \"California\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, ca_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(ca_prepared), \" California rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 51 California rows to foia_combined\n✓ Total rows in foia_combined: 51\n\n\n\n\n3.1.5 Document Metadata\nThe metadata is added with the raw information and updated with the results of the quality checks and a note on the processing steps taken.\n\n\nShow California data preparation and addition to metadata table\n# Add California to the metadata table using the helper function\nadd_state_metadata(\"California\", ca_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"California\", \n                      counts_ok = counts_consistent(ca_clean),\n                      percentages_ok = percentages_consistent(ca_clean),\n                      notes_text = \"Added Unknown race category to reconcile totals; calculated Combined totals and all percentages\")\n\n\n✓ Metadata added for: California \n✓ Metadata updated for: California \n\n\n\n\n3.1.6 Visualizations\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\n\n\nCalifornia Demographic Distributions by Offender Type\n\n\n\n\n\n\n3.1.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"California DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"California\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"California\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"California\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"California\")), \"\\n\"))\n\n\nCalifornia DNA Database Summary:\n= ======================================== \n# A tibble: 3 × 3\n  offender_type        value value_formatted\n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;          \n1 Convicted Offender 2019899 \"2,019,899\"    \n2 Arrestee            751822 \"  751,822\"    \n3 Combined           2771721 \"2,771,721\"    \n\nData completeness:\n# A tibble: 5 × 3\n  offender_type      value_source n_values\n  &lt;chr&gt;              &lt;chr&gt;           &lt;int&gt;\n1 Arrestee           calculated          9\n2 Arrestee           reported            8\n3 Combined           calculated         17\n4 Convicted Offender calculated          9\n5 Convicted Offender reported            8\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.1.8 Summary of California Processing\nCalifornia data processing complete. The dataset now includes:\n\n✅ Reported data: Counts for Convicted Offender and Arrestee\n✅ Calculated additions:\n\nUnknown race category to reconcile reported totals\nCombined totals across all offender types\nPercentage values for all demographic categories\n“Caucasian” and “African American” converted to “White” and “Black”.\n\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: All values include appropriate value_source indicators\n\nThe California data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#florida-fl",
    "href": "qmd_root/foia_processing.html#florida-fl",
    "title": "FOIA Document OCR Processing",
    "section": "3.2 Florida (FL)",
    "text": "3.2 Florida (FL)\nOverview: Florida provides both counts and percentages for gender and race categories and already includes a “Combined” total for all offender types, making it one of the most complete and straightforward datasets.\nOnly requires to standardize terminology for gender and race categories to match the common data model.\n\n3.2.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter2201Floridaoffender_typecharacter2201Combinedvariable_categorycharacter2203total, gender, racevariable_detailedcharacter22010total_profiles, Female, Male, Unknown, African American, Asian, Caucasian, Hispanic, Native American, Othervaluenumeric220221175391 ..., 100 ..., 260885 ..., 22.2 ..., 901126 ..., 76.67 ..., 13380 ..., 1.14 ..., 413733 ..., 35.2 ...value_typecharacter2202count, percentagevalue_sourcecharacter2201reportedData frame dimensions: 22 rows × 7 columns\n\n\n\n\n3.2.2 Verify Data Consistency\nRuns the first quality check using the Verify_category_totals() and counts_consistent() functions.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n1175391\n1175391\n0\n\n\nCombined\nrace\n1175391\n1175391\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.2.3 Address Data Gaps\n\n3.2.3.1 Standardize Terminology\nFlorida uses “African American” instead of “Black” and “Caucasian” instead of “White”.\n\n\nShow terminology standardization code\nfl_clean &lt;- fl_raw\n\n# Standardize racial terminology\nfl_clean &lt;- fl_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"African American\" ~ \"Black\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"✓ Standardized terminology: 'African American' → 'Black'\\n\")\n\nfl_clean &lt;- fl_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"✓ Standardized terminology: 'Caucasian' → 'White'\\n\")\n\n\n✓ Standardized terminology: 'African American' → 'Black'\n✓ Standardized terminology: 'Caucasian' → 'White'\n\n\n\n\n\n3.2.4 Prepare for Combined Dataset\nThe Florida data is already complete and consistent. It is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Florida data preparation to combined dataset\n# Prepare the data for the combined dataset\nfl_prepared &lt;- prepare_state_for_combined(fl_clean, \"Florida\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, fl_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(fl_prepared), \" Florida rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 22 Florida rows to foia_combined\n✓ Total rows in foia_combined: 73\n\n\n\n\n3.2.5 Document Metadata\nThe metadata is added with a note that the data was complete and required no processing.\n\n\nShow Florida data preparation and addition to metadata table\n# Add Florida to the metadata table using the helper function\nadd_state_metadata(\"Florida\", fl_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"Florida\", \n                      counts_ok = counts_consistent(fl_clean),\n                      percentages_ok = percentages_consistent(fl_clean),\n                      notes_text = \"Complete dataset provided. No processing or calculations required. All values are reported.\")\n\n\n✓ Metadata added for: Florida \n✓ Metadata updated for: Florida \n\n\n\n\n3.2.6 Visualizations\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n3.2.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Florida DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Florida\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Florida\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Florida\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Florida\")), \"\\n\"))\n\n\nFlorida DNA Database Summary:\n= ======================================== \n# A tibble: 1 × 3\n  offender_type   value value_formatted\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      1175391 1,175,391      \n\nData completeness:\n# A tibble: 1 × 3\n  offender_type value_source n_values\n  &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;\n1 Combined      reported           22\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.2.8 Summary of Florida Processing\nFlorida data processing complete. The dataset is exemplary and required no adjustments:\n\n✅ Reported data: Both counts and percentages for all Convicted Offender, Arrestee, and Combined categories.\n✅ Terminology standardization: “Caucasian” and “African American” converted to “White” and “Black”.\n✅ No calculated additions needed: All values are sourced directly from the state report (value_source = \"reported\").\n✅ Quality checks: All counts and percentages pass consistency validation.\n✅ Provenance tracking: All values maintain their original value_source as “reported”.\n\nThe Florida data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#indiana-in",
    "href": "qmd_root/foia_processing.html#indiana-in",
    "title": "FOIA Document OCR Processing",
    "section": "3.3 Indiana (IN)",
    "text": "3.3 Indiana (IN)\nOverview: Indiana presents a unique reporting pattern where total counts are provided by offender type, but demographic breakdowns are given only as percentages for the Combined total.\nValues were provided as strings, including a “&lt;1” notation, requiring conversion.\n\n3.3.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter801Indianaoffender_typecharacter803Convicted Offender, Arrestee, Combinedvariable_categorycharacter803total, gender, racevariable_detailedcharacter807total_profiles, Female, Male, Caucasian, Black, Hispanic, Othervaluenumeric808279654, 21087, 20, 80, 70, 26, 4, 0.5value_typecharacter802count, percentagevalue_sourcecharacter801reportedData frame dimensions: 8 rows × 7 columns\n\n\n\n\n3.3.2 Verify Data Consistency\nInitial checks reveal Indiana’s unique structure: counts for totals, percentages only for Combined demographics.\n\n\nInitial data availability:\n\n\nRace data: percentages \n\n\nGender data: percentages \n\n\n\nValue types in raw data:\n\n\ncount, percentage\n\n\n\n\n3.3.3 Address Data Gaps\n\n3.3.3.1 Convert String Values to Numeric\nThe raw data contains string values including “&lt;1” which we convert to 0.5.\n\n\nShow value conversion code\n# Start with raw data\nin_clean &lt;- in_raw\n\n# Convert string values to numeric, handling \"&lt;1\" as 1\nin_clean$value &lt;- sapply(in_clean$value, function(x) {\n  if (x == \"&lt;1\") {\n    0.5\n  } else {\n    as.numeric(x)\n  }\n})\n\n# Update value_type for converted percentages\nin_clean &lt;- in_clean %&gt;%\n  mutate(value_type = ifelse(value_type == \"percentage\", \"percentage\", value_type))\n\ncat(\"✓ Converted Indiana values from String to numeric\\n\")\ncat(paste(\"Unique values after conversion:\", paste(unique(in_clean$value), collapse = \", \"), \"\\n\"))\n\n\n✓ Converted Indiana values from String to numeric\nUnique values after conversion: 279654, 21087, 20, 80, 70, 26, 4, 0.5 \n\n\n\n\n3.3.3.2 Solve Percentages Inconsistency\nRacial percentages summed to 100.5% instead of 100%\nProportional scaling was applied and value_source was updated to “calculated” for all adjusted values.\n\n\nShow percentage recalculation code\n# Adjust percentages to ensure they sum to 100% and mark as calculated\nin_clean &lt;- in_clean %&gt;%\n  group_by(value_type, variable_category) %&gt;%\n  mutate(\n    value = ifelse(\n      value_type == \"percentage\" & variable_category == \"race\",\n      value * (100 / sum(value, na.rm = TRUE)),\n      value\n    ),\n    value_source = ifelse(\n      value_type == \"percentage\" & variable_category == \"race\",\n      \"calculated\",\n      value_source\n    )\n  ) %&gt;%\n  ungroup()\n\n# Verify the new sum\npercentage_sum &lt;- in_clean %&gt;%\n  filter(value_type == \"percentage\" & variable_category == \"race\") %&gt;%\n  summarise(total = sum(value, na.rm = TRUE))\n\ncat(\"✓ Recalculated percentages for Indiana - New sum:\", percentage_sum$total, \"%\\n\")\n\n\n✓ Recalculated percentages for Indiana - New sum: 100 %\n\n\n\n\n3.3.3.3 Standardize Terminology\nIndiana uses “Caucasian” instead of “White”.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nin_clean &lt;- in_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"✓ Standardized terminology: 'Caucasian' → 'White'\\n\")\n\n\n✓ Standardized terminology: 'Caucasian' → 'White'\n\n\n\n\n3.3.3.4 Create Combined Total Profiles\nIndiana provides separate totals for Convicted Offenders and Arrestees, but we need a Combined total to match the demographic percentages.\n\n\nShow combined total calculation code\n# Calculate Combined total from separate offender type totals\nconvicted_total &lt;- in_clean %&gt;%\n  filter(offender_type == \"Convicted Offender\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\narrestee_total &lt;- in_clean %&gt;%\n  filter(offender_type == \"Arrestee\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncombined_total &lt;- convicted_total + arrestee_total\n\n# Add Combined total to the data\ncombined_row &lt;- data.frame(\n  state = \"Indiana\",\n  offender_type = \"Combined\",\n  variable_category = \"total\",\n  variable_detailed = \"total_profiles\",\n  value = combined_total,\n  value_type = \"count\",\n  value_source = \"calculated\"\n)\n\nin_clean &lt;- bind_rows(in_clean, combined_row)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\ncat(\"✓ Added Combined total profiles\\n\")\n\n\nCombined total profiles: 300,741 \n✓ Added Combined total profiles\n\n\n\n\n3.3.3.5 Calculate Counts from Percentages\nIndiana only provides percentages for demographic categories. We calculate the actual counts using the Combined total.\n\n\nShow count calculation code\n# Calculate counts from percentages for Combined offender type\nin_clean &lt;- bind_rows(in_clean, calculate_counts_from_percentages(in_clean, \"Indiana\"))\n\ncat(\"✓ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(in_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\n✓ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n300741\n300741\n0\n\n\nCombined\nrace\n300741\n300741\n0\n\n\n\n\n\n\n\n\n3.3.4 Verify Data Consistency\nFinal checks to ensure all data is now consistent and complete.\n\n\nFinal data consistency checks:\n\n\nCounts consistent: TRUE \n\n\nPercentages consistent: TRUE \n\n\n\nFinal data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\n\n3.3.5 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Indiana data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nin_prepared &lt;- prepare_state_for_combined(in_clean, \"Indiana\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, in_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(in_prepared), \" Indiana rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 15 Indiana rows to foia_combined\n✓ Total rows in foia_combined: 88\n\n\n\n\n3.3.6 Document Metadata\nThe metadata is added with details on all processing steps performed.\n\n\nShow Indiana data preparation and addition to metadata table\n# Add Indiana to the metadata table using the helper function\nadd_state_metadata(\"Indiana\", in_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Indiana\", \n                      counts_ok = counts_consistent(in_clean),\n                      percentages_ok = percentages_consistent(in_clean),\n                      notes_text = \"Converted string values to numeric; standardized 'Black' to 'African American'; calculated Combined total profiles; derived all demographic counts from reported percentages\")\n\n\n✓ Metadata added for: Indiana \n✓ Metadata updated for: Indiana \n\n\n\n\n3.3.7 Visualizations\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n3.3.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Indiana DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Indiana\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value, value_source) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by value source\ncat(\"\\nData completeness by source:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Indiana\") %&gt;%\n  group_by(value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Indiana\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Indiana\")), \"\\n\"))\n\n\nIndiana DNA Database Summary:\n= ======================================== \n# A tibble: 3 × 4\n  offender_type       value value_source value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;          \n1 Convicted Offender 279654 reported     \"279,654\"      \n2 Arrestee            21087 reported     \" 21,087\"      \n3 Combined           300741 calculated   \"300,741\"      \n\nData completeness by source:\n# A tibble: 2 × 2\n  value_source n_values\n  &lt;chr&gt;           &lt;int&gt;\n1 calculated         11\n2 reported            4\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.3.9 Summary of Indiana Processing\nIndiana data processing complete. The unique dataset required:\n\n✅ Data conversion: String values converted to numeric, handling “&lt;1” as 0.5\n✅ Terminology standardization: “Caucasian” converted to “White”\n✅ Calculated additions:\n\nCombined total profiles across offender types\nAll demographic counts derived from reported percentages\n\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: Clear distinction between reported and calculated values\n\nThe Indiana data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#maine-me",
    "href": "qmd_root/foia_processing.html#maine-me",
    "title": "FOIA Document OCR Processing",
    "section": "3.4 Maine (ME)",
    "text": "3.4 Maine (ME)\nOverview: Maine provides comprehensive reporting with both counts and percentages for all gender and race categories across all offender types, including pre-calculated Combined totals. The data is complete and requires no processing.\n\n3.4.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1901Maineoffender_typecharacter1901Combinedvariable_categorycharacter1903total, gender, racevariable_detailedcharacter1909total_profiles, Male, Female, Unknown, White, Black, Native American, Hispanic, Asianvaluenumeric1901933711 ..., 27694 ..., 82.7 ..., 5734 ..., 17 ..., 83 ..., 0.2 ..., 31298 ..., 92.8 ..., 1299 ...value_typecharacter1902count, percentagevalue_sourcecharacter1901reportedData frame dimensions: 19 rows × 7 columns\n\n\n\n\n3.4.2 Verify Data Consistency\nRuns quality checks using the verify_category_totals(), counts_consistent(), and percentages_consistent() functions.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n33711\n33511\n200\n\n\nCombined\nrace\n33711\n33711\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.4.3 Address Data Gaps\n\n3.4.3.1 Solve Percentages Inconsistency\nRacial percentages summed to 99.9% instead of 100%\nProportional scaling was applied and value_source was updated to “calculated” for all adjusted values.\n\n\nShow percentage recalculation code\n# Start with the raw data\nme_clean &lt;- me_raw\n\n# Adjust percentages to ensure they sum to 100% and mark as calculated\nme_clean &lt;- me_clean %&gt;%\n  group_by(value_type, variable_category) %&gt;%\n  mutate(\n    value = ifelse(\n      value_type == \"percentage\" & variable_category == \"gender\",\n      value * (100 / sum(value, na.rm = TRUE)),\n      value\n    ),\n    value_source = ifelse(\n      value_type == \"percentage\" & variable_category == \"gender\",\n      \"calculated\",\n      value_source\n    )\n  ) %&gt;%\n  ungroup()\n\n# Verify the new sum\npercentage_sum &lt;- me_clean %&gt;%\n  filter(value_type == \"percentage\" & variable_category == \"gender\") %&gt;%\n  summarise(total = sum(value, na.rm = TRUE))\n\ncat(\"✓ Recalculated percentages for Maine - New sum:\", round(percentage_sum$total, 2), \"%\\n\")\n\n\n✓ Recalculated percentages for Maine - New sum: 100 %\n\n\n\n\n3.4.3.2 Recalculate Counts from Percentages\nMaine’s reported gender counts sum were inconsistent with the total_profiles.\nWe removed existing gender count data and recalculated counts using percentage values and combined totals.\nAll recalculated values flagged with value_source = \"calculated\"\n\n\nShow count recalculation code\n# Remove existing gender count rows to avoid duplication\nme_clean &lt;- me_clean %&gt;%\n  filter(!(variable_category == \"gender\" & value_type == \"count\"))\n\ncat(\"✓ Removed existing gender count data\\n\")\n\nme_gender &lt;- me_clean %&gt;%\n    filter(variable_category == \"gender\" | variable_category == \"total\")\n\n# Calculate counts from percentages for Combined offender type\nme_gender &lt;- calculate_counts_from_percentages(me_gender, \"Maine\")\n\n# Append recalculated gender counts to the main dataset\nme_clean &lt;- bind_rows(me_clean, me_gender)\n\ncat(\"✓ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(me_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\n✓ Removed existing gender count data\n✓ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n33711\n33711\n0\n\n\nCombined\nrace\n33711\n33711\n0\n\n\n\n\n\n\n\n\n3.4.4 Verify Data Consistency\nFinal checks to ensure all data is now consistent and complete.\n\n\nFinal data consistency checks:\n\n\nCounts consistent: TRUE \n\n\nPercentages consistent: TRUE \n\n\n\nFinal data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\n\n3.4.5 Prepare for Combined Dataset\nThe Maine data is already complete and consistent. It is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Maine data preparation to combined dataset\n# Prepare the data for the combined dataset\nme_prepared &lt;- prepare_state_for_combined(me_clean, \"Maine\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, me_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(me_prepared), \" Maine rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 19 Maine rows to foia_combined\n✓ Total rows in foia_combined: 107\n\n\n\n\n3.4.6 Document Metadata\nThe metadata is added with a note that the data was complete and required no processing.\n\n\nShow Maine data preparation and addition to metadata table\n# Add Maine to the metadata table using the helper function\nadd_state_metadata(\"Maine\", me_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"Maine\", \n                      counts_ok = counts_consistent(me_clean),\n                      percentages_ok = percentages_consistent(me_clean),\n                      notes_text = \"Complete dataset provided with both counts and percentages. No processing or calculations required. All values are reported.\")\n\n\n✓ Metadata added for: Maine \n✓ Metadata updated for: Maine \n\n\n\n\n3.4.7 Visualizations\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n3.4.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Maine DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Maine\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Maine\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Maine\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Maine\")), \"\\n\"))\n\n\nMaine DNA Database Summary:\n= ======================================== \n# A tibble: 1 × 3\n  offender_type value value_formatted\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      33711 33,711         \n\nData completeness:\n# A tibble: 2 × 3\n  offender_type value_source n_values\n  &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;\n1 Combined      calculated          6\n2 Combined      reported           13\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.4.9 Summary of Maine Processing\nMaine data processing complete. The dataset is exemplary and required no adjustments:\n\n✅ Reported data: Both counts and percentages for all Convicted Offender, Arrestee, and Combined categories\n✅ No calculated additions needed: All values are sourced directly from the state report (value_source = \"reported\")\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: All values maintain their original value_source as “reported”\n\nThe Maine data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#nevada-nv",
    "href": "qmd_root/foia_processing.html#nevada-nv",
    "title": "FOIA Document OCR Processing",
    "section": "3.5 Nevada (NV)",
    "text": "3.5 Nevada (NV)\nOverview: Nevada provides both counts and percentages for gender and race categories but uses non-standard terminology that requires conversion for consistency with our schema.\n\n3.5.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter2101Nevadaoffender_typecharacter2104All, Arrestee, Convicted Offender, Combinedvariable_categorycharacter2103total, gender, racevariable_detailedcharacter2109total_flags, total_profiles, Female, Male, Unknown, White, American Indian, Black, Asianvaluenumeric21021344097 ..., 185074 ..., 53.785 ..., 159023 ..., 46.215 ..., 63287 ..., 18.392 ..., 280738 ..., 81.587 ..., 72 ...value_typecharacter2102count, percentagevalue_sourcecharacter2101reportedData frame dimensions: 21 rows × 7 columns\n\n\n\n\n3.5.2 Verify Data Consistency\nInitial check reveals Nevada’s non-standard terminology.\n\n\nInitial data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\nNon-standard terminology found:\n\n\nOffender types: All, Arrestee, Convicted Offender, Combined \n\n\n\n\n3.5.3 Address Data Gaps\n\n3.5.3.1 Standardize Terminology\nNevada uses “All” instead of “Combined”, “total_flags” instead of “total_profiles” and “American Indian” instead of “Native American”.\n\n\nShow terminology standardization code\n# Start with raw data\nnv_clean &lt;- nv_raw\n\n# Standardize offender types and racial terminology\nnv_clean &lt;- nv_clean %&gt;%\n  mutate(\n    offender_type = case_when(\n      offender_type == \"All\" ~ \"Combined\",\n      TRUE ~ offender_type\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"total_flags\" ~ \"total_profiles\",\n      TRUE ~ variable_detailed\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"American Indian\" ~ \"Native American\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"✓ Standardized terminology:\\n\")\ncat(\"  - 'All' → 'Combined'\\n\")\ncat(\"  - 'total_flags' → 'total_profiles'\\n\")\ncat(\"  - 'American Indian' → 'Native American'\\n\")\n\n\n✓ Standardized terminology:\n  - 'All' → 'Combined'\n  - 'total_flags' → 'total_profiles'\n  - 'American Indian' → 'Native American'\n\n\n\n\n3.5.3.2 Verify Consistency\nNow that the offender types are standardized, we can verify the counts and percentages.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n344097\n344097\n0\n\n\nCombined\nrace\n344097\n344097\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\nSum of 'race' percentages: 100 %\n\n\nSum of 'gender' percentages: 100 %\n\n\n\n\n\n3.5.4 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Nevada data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nnv_prepared &lt;- prepare_state_for_combined(nv_clean, \"Nevada\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, nv_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(nv_prepared), \" Nevada rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 21 Nevada rows to foia_combined\n✓ Total rows in foia_combined: 128\n\n\n\n\n3.5.5 Document Metadata\nThe metadata is added with details on the terminology standardization performed.\n\n\nShow Nevada data preparation and addition to metadata table\n# Add Nevada to the metadata table using the helper function\nadd_state_metadata(\"Nevada\", nv_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Nevada\", \n                      counts_ok = counts_consistent(nv_clean),\n                      percentages_ok = percentages_consistent(nv_clean),\n                      notes_text = \"Standardized terminology: 'All' to 'Combined' and 'American Indian' to 'Native American'. All values remain reported.\")\n\n\n✓ Metadata added for: Nevada \n✓ Metadata updated for: Nevada \n\n\n\n\n3.5.6 Visualizations\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n3.5.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Nevada DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Nevada\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Nevada\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Nevada\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Nevada\")), \"\\n\"))\n\n\nNevada DNA Database Summary:\n= ======================================== \n# A tibble: 3 × 3\n  offender_type       value value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;          \n1 Combined           344097 344,097        \n2 Arrestee           185074 185,074        \n3 Convicted Offender 159023 159,023        \n\nData completeness:\n# A tibble: 3 × 3\n  offender_type      value_source n_values\n  &lt;chr&gt;              &lt;chr&gt;           &lt;int&gt;\n1 Arrestee           reported            2\n2 Combined           reported           17\n3 Convicted Offender reported            2\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: FALSE \n\n\n\n\n3.5.8 Summary of Nevada Processing\nNevada data processing complete. The dataset required minimal adjustments:\n\n✅ Terminology standardization:\n\n“All” → “Combined” (offender type)\n“American Indian” → “Native American” (race category)\n\n✅ Reported data: Both counts and percentages for all categories\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: All values maintain value_source = \"reported\" as only terminology changes were made\n\nThe Nevada data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#south-dakota-sd",
    "href": "qmd_root/foia_processing.html#south-dakota-sd",
    "title": "FOIA Document OCR Processing",
    "section": "3.6 South Dakota (SD)",
    "text": "3.6 South Dakota (SD)\nOverview: South Dakota provides the most comprehensive reporting with both counts and percentages for all standard categories plus unique intersectional gender×race data. Minor terminology standardization is required for consistency.\n\n3.6.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter4101South Dakotaoffender_typecharacter4101Combinedvariable_categorycharacter4104total, gender, race, gender_racevariable_detailedcharacter41021total_profiles ..., Male ..., Female ..., Asian ..., Black ..., Hispanic ..., Native American ..., Other/Unknown ..., White/Caucasian ..., Male_Asian ...valuenumeric4103867753 ..., 51197 ..., 75.56 ..., 16556 ..., 24.44 ..., 5 ..., 0.08 ..., 4041 ..., 5.96 ..., 2949 ...value_typecharacter4102count, percentagevalue_sourcecharacter4101reportedData frame dimensions: 41 rows × 7 columns\n\n\n\n\n3.6.2 Gender-race intersection analysis\nSince South Dakota is the only state that reported gender-race intersection data, we can analyze it in detail.\n\n\n\n\n\n\nSouth Dakota Intersectional Gender × Race Analysis\n\n\n\n\n\n\n\n3.6.3 Verify Data Consistency\nInitial check reveals South Dakota’s comprehensive data structure with some non-standard terminology.\n\n\nInitial data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\nNon-standard terminology found:\n\n\nRace terms: Asian, Black, Hispanic, Native American, Other/Unknown, White/Caucasian \n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67702\n51\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\nSum of 'race' percentages: 100 %\n\n\nSum of 'gender' percentages: 100 %\n\n\n\n\n3.6.4 Address Data Gaps\n\n3.6.4.1 Standardize Terminology\nSouth Dakota uses “White/Caucasian” and “Other/Unknown” which need standardization.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nsd_clean &lt;- sd_clean %&gt;%\n  mutate(\n    variable_detailed = case_when(\n      variable_detailed == \"White/Caucasian\" ~ \"White\",\n      variable_detailed == \"Other/Unknown\" ~ \"Unknown\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"✓ Standardized terminology:\\n\")\ncat(\"  - 'White/Caucasian' → 'White'\\n\")\ncat(\"  - 'Other/Unknown' → 'Unknown'\\n\")\n\n# Verify the changes\ncat(\"\\nRace categories after standardization:\\n\")\nsd_clean %&gt;%\n  filter(variable_category == \"race\") %&gt;%\n  distinct(variable_detailed) %&gt;%\n  pull() %&gt;%\n  paste(collapse = \", \") %&gt;%\n  cat()\n\n\n✓ Standardized terminology:\n  - 'White/Caucasian' → 'White'\n  - 'Other/Unknown' → 'Unknown'\n\nRace categories after standardization:\nAsian, Black, Hispanic, Native American, Unknown, White\n\n\n\n\n3.6.4.2 Recalculate Counts from Percentages\nSouth Dakota’s reported race counts sum were inconsistent with the total_profiles.\nWe removed existing gender count data and recalculated counts using percentage values and combined totals.\nAll recalculated values flagged with value_source = \"calculated\"\n\n\nShow count recalculation code\n# Remove existing gender count rows to avoid duplication\nsd_clean &lt;- sd_clean %&gt;%\n  filter(!(variable_category == \"race\" & value_type == \"count\"))\n\ncat(\"✓ Removed existing race count data\\n\")\n\nsd_race &lt;- sd_clean %&gt;%\n    filter(variable_category == \"race\" | variable_category == \"total\")\n\n# Calculate counts from percentages for Combined offender type\nsd_race &lt;- calculate_counts_from_percentages(sd_race, \"South Dakota\")\n\n# Append recalculated race counts to the main dataset\nsd_clean &lt;- bind_rows(sd_clean, sd_race)\n\ncat(\"✓ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(sd_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\n✓ Removed existing race count data\n✓ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67752\n1\n\n\n\n\n\nWe handled this diffence of 1 by adding it to the most representative race (White).\n\n\nShow difference handle code\n# Handle the difference of 1 by adding it to the most representative race\nsd_clean &lt;- sd_clean %&gt;%\n  mutate(value = ifelse(variable_detailed == \"White\" & value_type == \"count\", value + 1, value))\n\n\n\n\n\n3.6.5 Verify Data Consistency\nFinal checks to ensure standardization didn’t affect data integrity.\n\n\nFinal data consistency checks after standardization:\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67753\n0\n\n\n\n\n\n\nCounts consistency check:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.6.6 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow South Dakota data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nsd_prepared &lt;- prepare_state_for_combined(sd_clean, \"South Dakota\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, sd_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(sd_prepared), \" South Dakota rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n# Show the comprehensive nature of South Dakota's data\ncat(\"\\nSouth Dakota's comprehensive data structure:\\n\")\nsd_prepared %&gt;%\n  group_by(variable_category) %&gt;%\n  summarise(n_rows = n(), .groups = \"drop\") %&gt;%\n  kable() %&gt;% kable_styling()\n\n\n✓ Appended 17 South Dakota rows to foia_combined\n✓ Total rows in foia_combined: 145\n\nSouth Dakota's comprehensive data structure:\n\n\n\n\n\nvariable_category\nn_rows\n\n\n\n\ngender\n4\n\n\nrace\n12\n\n\ntotal\n1\n\n\n\n\n\n\n\n3.6.7 Document Metadata\nThe metadata is added with details on South Dakota’s comprehensive reporting and the terminology standardization performed.\n\n\nShow South Dakota data preparation and addition to metadata table\n# Add South Dakota to the metadata table using the helper function\nadd_state_metadata(\"South Dakota\", sd_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"South Dakota\", \n                      counts_ok = counts_consistent(sd_clean),\n                      percentages_ok = percentages_consistent(sd_clean),\n                      notes_text = \"Standardized terminology: 'White/Caucasian' to 'White' and 'Other/Unknown' to 'Unknown'. Includes comprehensive gender_race intersectional data. All values remain reported.\")\n\n\n✓ Metadata added for: South Dakota \n✓ Metadata updated for: South Dakota \n\n\n\n\n3.6.8 Visualizations\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n3.6.9 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"South Dakota DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"South Dakota\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by category\ncat(\"\\nData completeness by category:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"South Dakota\") %&gt;%\n  group_by(variable_category) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"South Dakota\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"South Dakota\")), \"\\n\"))\n\n\nSouth Dakota DNA Database Summary:\n= ======================================== \n# A tibble: 1 × 3\n  offender_type value value_formatted\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      67753 67,753         \n\nData completeness by category:\n# A tibble: 3 × 2\n  variable_category n_values\n  &lt;chr&gt;                &lt;int&gt;\n1 gender                   4\n2 race                    12\n3 total                    1\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.6.10 Summary of South Dakota Processing\nSouth Dakota data processing complete. The state provided exemplary data with minimal adjustments needed:\n\n✅ Terminology standardization:\n\n“White/Caucasian” → “White”\n“Other/Unknown” → “Unknown”\n\n✅ Comprehensive reporting: Standard demographics plus unique gender×race intersectional data\n✅ Reported data: Both counts and percentages for all categories\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: All values maintain value_source = \"reported\" as only terminology changes were made\n\nSouth Dakota’s data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#texas-tx",
    "href": "qmd_root/foia_processing.html#texas-tx",
    "title": "FOIA Document OCR Processing",
    "section": "3.7 Texas (TX)",
    "text": "3.7 Texas (TX)\nOverview: Texas provides counts only for gender and race categories. The Male gender is missing in the dataset. The state uses non-standard terminology that requires conversion and needs Combined totals and percentages calculated.\n\n3.7.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1601Texasoffender_typecharacter1602Offenders, Arresteevariable_categorycharacter1603total, gender, racevariable_detailedcharacter1608total_profiles, Female, Asian, African American, Caucasian, Hispanic, Native American, Othervaluenumeric16016845322 ..., 73631 ..., 121434 ..., 18721 ..., 3361 ..., 254366 ..., 309010 ..., 276245 ..., 138 ..., 2173 ...value_typecharacter1601countvalue_sourcecharacter1601reportedData frame dimensions: 16 rows × 7 columns\n\n\n\n\n3.7.2 Verify Data Consistency\nInitial checks reveal Texas’s reporting structure and terminology differences.\n\n\nInitial data availability:\n\n\nRace data: counts \n\n\nGender data: counts \n\n\n\nNon-standard terminology found:\n\n\nOffender types: Offenders, Arrestee \n\n\nRace terms: Asian, African American, Caucasian, Hispanic, Native American, Other \n\n\n\n\n3.7.3 Address Data Gaps\n\n3.7.3.1 Add Missing Male category\nTexas data reports only Female counts explicitly. We calculated Male counts by subtracting Female counts from total profiles, assuming binary gender classification in the dataset.\n\n\nShow male addition code\n# First, let's examine the current structure of gender data\ngender_data &lt;- tx_raw %&gt;%\n  filter(variable_category == \"gender\")\n\ncat(\"Current gender structure:\\n\")\nprint(unique(gender_data$variable_detailed))\n\n# Get total profiles for each offender type\ntotal_profiles &lt;- tx_raw %&gt;%\n  filter(variable_category == \"total\" & variable_detailed == \"total_profiles\") %&gt;%\n  select(offender_type, total_value = value)\n\n# Join total profiles with gender data\ngender_with_totals &lt;- gender_data %&gt;%\n  left_join(total_profiles, by = \"offender_type\")\n\n# Create Male entries for each offender type\nmale_entries &lt;- gender_with_totals %&gt;%\n  filter(variable_detailed == \"Female\") %&gt;%\n  mutate(\n    variable_detailed = \"Male\",\n    value = total_value - value, \n    value_source = \"calculated\",\n    total_value = NULL \n  )\n\n# Add these entries to the original dataset\ntx_raw_with_male &lt;- tx_raw %&gt;%\n  bind_rows(male_entries)\n\n# Update the tx_raw object\ntx_clean &lt;- tx_raw_with_male\n\n# Verify the addition\ncat(\"\\nAfter adding Male entries - gender categories:\\n\")\nprint(unique(tx_clean %&gt;% \n       filter(variable_category == \"gender\") %&gt;% \n       pull(variable_detailed)))\n\n\nCurrent gender structure:\n[1] \"Female\"\n\nAfter adding Male entries - gender categories:\n[1] \"Female\" \"Male\"  \n\n\n\n\n3.7.3.2 Standardize Terminology\nTexas uses “Offenders” instead of “Convicted Offender” and “Caucasian” instead of “White”.\n\n\nShow terminology standardization code\n# Standardize offender types and racial terminology\ntx_clean &lt;- tx_clean %&gt;%\n  mutate(\n    offender_type = case_when(\n      offender_type == \"Offenders\" ~ \"Convicted Offender\",\n      TRUE ~ offender_type\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"Caucasian\" ~ \"White\",\n      variable_detailed == \"African American\" ~ \"Black\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"✓ Standardized terminology:\\n\")\ncat(\"  - 'Offenders' → 'Convicted Offender'\\n\")\ncat(\"  - 'Caucasian' → 'White'\\n\")\ncat(\"  - 'African American' → 'Black'\\n\")\ncat(paste(\"Offender types after standardization:\", paste(sort(unique(tx_clean$offender_type)), collapse = \", \"), \"\\n\"))\n\n\n✓ Standardized terminology:\n  - 'Offenders' → 'Convicted Offender'\n  - 'Caucasian' → 'White'\n  - 'African American' → 'Black'\nOffender types after standardization: Arrestee, Convicted Offender \n\n\n\n\n3.7.3.3 Create Unknown Category\nTexas race count is inconsistent, with a significant number of profiles not reported in any racial category.\nUnknown category was created to account for these missing profiles.\nThe calculated values are added with a value_source = \"calculated\" tag to maintain transparency about what was provided versus what was derived.\n\n\nShow unknown addition code\n# Add Unknown race category to reconcile totals\ntx_clean &lt;- fill_demographic_gaps(tx_clean)\n\n# Verify the fix\ncat(\"Category totals after adding Unknown race category:\\n\")\nverify_category_totals(tx_clean) %&gt;% kable() %&gt;% kable_styling()\n\ncat(\"\\nCounts consistency after adding Unknown:\\n\")\ncat(paste(\"All counts consistent:\", counts_consistent(tx_clean), \"\\n\"))\n\n\nCategory totals after adding Unknown race category:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n73631\n73631\n0\n\n\nArrestee\nrace\n73631\n73631\n0\n\n\nConvicted Offender\ngender\n845322\n845322\n0\n\n\nConvicted Offender\nrace\n845322\n845322\n0\n\n\n\n\n\n\nCounts consistency after adding Unknown:\nAll counts consistent: TRUE \n\n\n\n\n3.7.3.4 Create Combined Totals\nTexas only reported data for “Convicted Offender” and “Arrestee” separately. We calculate Combined totals.\n\n\nShow combined addition code\n# Calculate Combined totals using helper function\ntx_clean &lt;- add_combined(tx_clean)\n\ncat(\"✓ Created Combined totals for Texas\\n\")\n\n# Show the Combined total\ncombined_total &lt;- tx_clean %&gt;%\n  filter(offender_type == \"Combined\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\n\n\n✓ Created Combined totals for Texas\nCombined total profiles: 918,953 \n\n\n\n\n3.7.3.5 Calculate Percentages\nTransforms the data from counts into percentages for comparative analysis.\n\n\nShow percentage calculation code\n# Derive percentages from counts\ntx_clean &lt;- add_percentages(tx_clean)\n\ncat(\"✓ Added percentages for all demographic categories\\n\")\n\n# Check percentage consistency\ncat(\"Percentage consistency check:\\n\")\ncat(paste(\"All percentages sum to ~100%:\", percentages_consistent(tx_clean), \"\\n\\n\"))\n\n# Show current data availability\ncat(\"Final data availability:\\n\")\ncat(paste(\"Race data:\", report_status(tx_clean, \"race\"), \"\\n\"))\ncat(paste(\"Gender data:\", report_status(tx_clean, \"gender\"), \"\\n\"))\n\n\n✓ Added percentages for all demographic categories\nPercentage consistency check:\nAll percentages sum to ~100%: TRUE \n\nFinal data availability:\nRace data: both \nGender data: both \n\n\n\n\n\n3.7.4 Verify Data Consistency\nFinal checks to ensure all processing maintained data integrity.\n\n\nFinal data consistency checks:\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n73631\n73631\n0\n\n\nArrestee\nrace\n73631\n73631\n0\n\n\nCombined\ngender\n918953\n918953\n0\n\n\nCombined\nrace\n918953\n918953\n0\n\n\nConvicted Offender\ngender\n845322\n845322\n0\n\n\nConvicted Offender\nrace\n845322\n845322\n0\n\n\n\n\n\n\nCounts consistency check:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.7.5 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Texas data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\ntx_prepared &lt;- prepare_state_for_combined(tx_clean, \"Texas\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, tx_prepared)\n\ncat(paste0(\"✓ Appended \", nrow(tx_prepared), \" Texas rows to foia_combined\\n\"))\ncat(paste0(\"✓ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\n✓ Appended 57 Texas rows to foia_combined\n✓ Total rows in foia_combined: 202\n\n\n\n\n3.7.6 Document Metadata\nThe metadata is added with details on all processing steps performed.\n\n\nShow Texas data preparation and addition to metadata table\n# Add Texas to the metadata table using the helper function\nadd_state_metadata(\"Texas\", tx_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Texas\", \n                      counts_ok = counts_consistent(tx_clean),\n                      percentages_ok = percentages_consistent(tx_clean),\n                      notes_text = \"Standardized terminology: 'Offenders' to 'Convicted Offender', 'Caucasian' to 'White', 'African American' to 'Black'; calculated Combined totals and all percentages\")\n\n\n✓ Metadata added for: Texas \n✓ Metadata updated for: Texas \n\n\n\n\n3.7.7 Visualizations\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\n\n\nTexas Demographic Distributions by Offender Type\n\n\n\n\n\n\n3.7.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Texas DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Texas\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value, value_source) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by value source\ncat(\"\\nData completeness by source:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Texas\") %&gt;%\n  group_by(value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Texas\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Texas\")), \"\\n\"))\n\n\nTexas DNA Database Summary:\n= ======================================== \n# A tibble: 3 × 4\n  offender_type       value value_source value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;          \n1 Convicted Offender 845322 reported     \"845,322\"      \n2 Arrestee            73631 reported     \" 73,631\"      \n3 Combined           918953 calculated   \"918,953\"      \n\nData completeness by source:\n# A tibble: 2 × 2\n  value_source n_values\n  &lt;chr&gt;           &lt;int&gt;\n1 calculated         41\n2 reported           16\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.7.9 Summary of Texas Processing\nTexas data processing complete. The dataset required several adjustments:\n\n✅ Male Category Addition:\n\n“Male” added to variable_detailed\n\n✅ Terminology standardization:\n\n“Offenders” → “Convicted Offender”\n“Caucasian” → “White”\n“African American” → “Black”\n\n✅ Calculated additions:\n\nCombined totals across all offender types\nPercentage values for all demographic categories\n\n✅ Quality checks: All counts and percentages pass consistency validation\n✅ Provenance tracking: Clear distinction between reported and calculated values\n\nThe Texas data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#combined-dataset",
    "href": "qmd_root/foia_processing.html#combined-dataset",
    "title": "FOIA Document OCR Processing",
    "section": "3.8 Combined Dataset",
    "text": "3.8 Combined Dataset"
  },
  {
    "objectID": "qmd_root/foia_processing.html#metadata-table",
    "href": "qmd_root/foia_processing.html#metadata-table",
    "title": "FOIA Document OCR Processing",
    "section": "3.9 Metadata table",
    "text": "3.9 Metadata table"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html",
    "href": "qmd_root/appendix_analysis.html",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "",
    "text": "This document provides complete transparency regarding the data sources and methodology used to compile racial disparities in DNA collection across U.S. states. The original data was collected in Summer 2017, with most data points from 2013-2016.\n\n\n\nConsistent racial disparities: Black populations show the highest DNA collection rates relative to their population percentage in nearly all states\nData limitations: Many states lack comprehensive conviction data, requiring the use of prison admission proxies\nMethodological challenges: Hispanic/Latino populations often uncounted or miscategorized in state data"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#executive-summary",
    "href": "qmd_root/appendix_analysis.html#executive-summary",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "",
    "text": "This document provides complete transparency regarding the data sources and methodology used to compile racial disparities in DNA collection across U.S. states. The original data was collected in Summer 2017, with most data points from 2013-2016.\n\n\n\nConsistent racial disparities: Black populations show the highest DNA collection rates relative to their population percentage in nearly all states\nData limitations: Many states lack comprehensive conviction data, requiring the use of prison admission proxies\nMethodological challenges: Hispanic/Latino populations often uncounted or miscategorized in state data"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#methodology-overview",
    "href": "qmd_root/appendix_analysis.html#methodology-overview",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Methodology Overview",
    "text": "Methodology Overview\n\nData Collection Period\n\nPrimary collection: Summer 2017\nData years used: Single year per state (2012-2016, varies by availability)\nCensus baseline: 2010 U.S. Census for demographic comparisons\n\n\n\nGeneral Challenges Encountered\n\nConviction Data Scarcity: Most states do not publicly disclose comprehensive felony conviction data\nPrison Admission Proxy: Prison admissions used as substitute for conviction data in majority of states\nRacial Classification Inconsistencies:\n\n-   Many states only report \"Black\" and \"White\" categories\n\n-   Hispanic/Latino often classified as ethnicity rather than race\n\n-   \"Other\" category frequently used without specification\n\nArrest Data Gaps: Racial makeup of arrests often unavailable or incomplete"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#national-summary-table",
    "href": "qmd_root/appendix_analysis.html#national-summary-table",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "National Summary Table",
    "text": "National Summary Table\n\nMethodology: Parsing DNA Collection Data\nThe national summary data was extracted from a structured text file (MurphyTong_Racial_Breakdown.txt) containing three distinct data sections for each state:\n\nDNA Collection Data: Percentage and absolute counts of DNA profiles collected by race (e.g., “46% B (18,253)”)\nPopulation Demographics: Percentage of state population by race from 2010 Census\n\nProcessing Steps:\n\nState Identification: The parser identifies state entries using standard two-letter abbreviations (AL, AK, AZ, etc.)\nSection Segmentation: For each state, the text is divided into three sections based on the pattern of “% B” (Black percentage) occurrences\nData Extraction: Regular expressions extract both percentages and counts from the first section, and percentages only from the demographic and rate sections\nPattern Matching: The code uses regex patterns like ([0-9.]+)%\\s*B\\s*\\(([0-9,]+)\\) to capture both percentage (e.g., 46%) and count (e.g., 18,253) for DNA collection data\nRace Categories: Data is extracted for five racial categories: Black (B), Hispanic (H), Asian (A), Native American (N), and White (W)\n\nThis automated parsing ensures reproducibility and allows for systematic extraction of data from the original Murphy & Tong study format.\n\n\nShow summary code\n# Function to parse DNA collection data from text file\nparse_dna_data &lt;- function(file_path) {\n  \n  # Read the entire file\n  text_data &lt;- readLines(file_path, warn = FALSE)\n  \n  # Remove empty lines\n  text_data &lt;- text_data[text_data != \"\"]\n  \n  # Initialize list to store parsed data\n  parsed_data &lt;- list()\n  \n  # State abbreviations (for reference)\n  state_abbrevs &lt;- c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n                     \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n                     \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n                     \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n                     \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\")\n  \n  # Function to extract percentage and count from patterns like \"46% B (18,253)\"\n  extract_race_data &lt;- function(text, race_letter) {\n    pattern &lt;- paste0(\"([0-9.]+)%\\\\s*\", race_letter, \"\\\\s*\\\\(([0-9,]+)\\\\)\")\n    matches &lt;- str_match(text, pattern)\n    if (!is.na(matches[1])) {\n      pct &lt;- as.numeric(matches[2])\n      count &lt;- as.numeric(gsub(\",\", \"\", matches[3]))\n      return(list(pct = pct, count = count))\n    }\n    return(list(pct = NA, count = NA))\n  }\n  \n  # Function to extract just percentage (for demographics and collection rates)\n  extract_percentage &lt;- function(text, race_letter) {\n    pattern &lt;- paste0(\"([0-9.]+)%\\\\s*\", race_letter)\n    matches &lt;- str_match(text, pattern)\n    if (!is.na(matches[1])) {\n      return(as.numeric(matches[2]))\n    }\n    return(NA)\n  }\n  \n  # Process each line\n  i &lt;- 1\n  while (i &lt;= length(text_data)) {\n    line &lt;- text_data[i]\n    \n    # Check if line is a state abbreviation\n    if (line %in% state_abbrevs) {\n      state &lt;- line\n      \n      # Initialize data structure for this state\n      state_data &lt;- list(\n        State = state,\n        Black_DNA_Pct = NA, Black_DNA_N = NA,\n        Hispanic_DNA_Pct = NA, Hispanic_DNA_N = NA,\n        Asian_DNA_Pct = NA, Asian_DNA_N = NA,\n        Native_American_DNA_Pct = NA, Native_American_DNA_N = NA,\n        White_DNA_Pct = NA, White_DNA_N = NA,\n        Black_Pop_Pct = NA, Hispanic_Pop_Pct = NA, Asian_Pop_Pct = NA,\n        Native_American_Pop_Pct = NA, White_Pop_Pct = NA,\n        Black_Collection_Rate = NA, Hispanic_Collection_Rate = NA,\n        Asian_Collection_Rate = NA, Native_American_Collection_Rate = NA,\n        White_Collection_Rate = NA\n      )\n      \n      # Collect all lines for this state until we hit the next state or end of file\n      state_lines &lt;- c()\n      i &lt;- i + 1\n      while (i &lt;= length(text_data) && !(text_data[i] %in% state_abbrevs)) {\n        state_lines &lt;- c(state_lines, text_data[i])\n        i &lt;- i + 1\n      }\n      \n      # Combine all lines for this state into one text block\n      state_text &lt;- paste(state_lines, collapse = \" \")\n      \n      # Split into sections based on the pattern of % B occurrences\n      # We need to identify the three sections: DNA Collection, Demographics, Collection Rates\n      \n      # Find all \"% B\" patterns to help identify sections\n      b_patterns &lt;- str_locate_all(state_text, \"[0-9.]+%\\\\s*B\")[[1]]\n      \n      if (nrow(b_patterns) &gt;= 1) {\n        # First section: DNA Collection (has counts in parentheses)\n        dna_section_start &lt;- 1\n        dna_section_end &lt;- if(nrow(b_patterns) &gt;= 2) b_patterns[2,1] - 1 else nchar(state_text)\n        dna_section &lt;- substr(state_text, dna_section_start, dna_section_end)\n        \n        # Extract DNA collection data (with counts)\n        black_dna &lt;- extract_race_data(dna_section, \"B\")\n        hispanic_dna &lt;- extract_race_data(dna_section, \"H\")\n        asian_dna &lt;- extract_race_data(dna_section, \"A\")\n        native_dna &lt;- extract_race_data(dna_section, \"N\")\n        white_dna &lt;- extract_race_data(dna_section, \"W\")\n        \n        state_data$Black_DNA_Pct &lt;- black_dna$pct\n        state_data$Black_DNA_N &lt;- black_dna$count\n        state_data$Hispanic_DNA_Pct &lt;- hispanic_dna$pct\n        state_data$Hispanic_DNA_N &lt;- hispanic_dna$count\n        state_data$Asian_DNA_Pct &lt;- asian_dna$pct\n        state_data$Asian_DNA_N &lt;- asian_dna$count\n        state_data$Native_American_DNA_Pct &lt;- native_dna$pct\n        state_data$Native_American_DNA_N &lt;- native_dna$count\n        state_data$White_DNA_Pct &lt;- white_dna$pct\n        state_data$White_DNA_N &lt;- white_dna$count\n      }\n      \n      if (nrow(b_patterns) &gt;= 2) {\n        # Second section: Demographics (percentages only, no parentheses)\n        demo_section_start &lt;- b_patterns[2,1]\n        demo_section_end &lt;- if(nrow(b_patterns) &gt;= 3) b_patterns[3,1] - 1 else nchar(state_text)\n        demo_section &lt;- substr(state_text, demo_section_start, demo_section_end)\n        \n        # Extract demographic percentages\n        state_data$Black_Pop_Pct &lt;- extract_percentage(demo_section, \"B\")\n        state_data$Hispanic_Pop_Pct &lt;- extract_percentage(demo_section, \"H\")\n        state_data$Asian_Pop_Pct &lt;- extract_percentage(demo_section, \"A\")\n        state_data$Native_American_Pop_Pct &lt;- extract_percentage(demo_section, \"N\")\n        state_data$White_Pop_Pct &lt;- extract_percentage(demo_section, \"W\")\n      }\n      \n      if (nrow(b_patterns) &gt;= 3) {\n        # Third section: Collection Rates (percentages only, no parentheses)\n        rate_section_start &lt;- b_patterns[3,1]\n        rate_section &lt;- substr(state_text, rate_section_start, nchar(state_text))\n        \n        # Extract collection rate percentages\n        state_data$Black_Collection_Rate &lt;- extract_percentage(rate_section, \"B\")\n        state_data$Hispanic_Collection_Rate &lt;- extract_percentage(rate_section, \"H\")\n        state_data$Asian_Collection_Rate &lt;- extract_percentage(rate_section, \"A\")\n        state_data$Native_American_Collection_Rate &lt;- extract_percentage(rate_section, \"N\")\n        state_data$White_Collection_Rate &lt;- extract_percentage(rate_section, \"W\")\n      }\n      \n      # Add to parsed data\n      parsed_data[[length(parsed_data) + 1]] &lt;- state_data\n      \n      # Don't increment i here since we already did it in the while loop\n      i &lt;- i - 1\n    }\n    \n    i &lt;- i + 1\n  }\n  \n  # Convert to data frame\n  df &lt;- do.call(rbind, lapply(parsed_data, data.frame))\n  \n  return(df)\n}\n\nracial_data_path &lt;- file.path(here(\"data\", \"annual_dna_collection\", \"intermediate\", \"MurphyTong_Racial_Breakdown.txt\"))\n\nsummary_data &lt;- parse_dna_data(racial_data_path)\n\n# Display interactive table\ndatatable(summary_data, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Complete state-by-state breakdown of DNA collection by race\")\n\n\n\n\nTable 1: Racial Breakdown of Annual DNA Collection for Each State\n\n\n\n\n\n\n\n\n\n\n\n\nDisparity Analysis\n\n\nShow disparity analysis code\n# Calculate disparity ratios\ndisparity_data &lt;- summary_data %&gt;%\n  filter(!is.na(Black_Collection_Rate) & !is.na(White_Collection_Rate)) %&gt;%\n  mutate(Black_White_Ratio = Black_Collection_Rate / White_Collection_Rate) %&gt;%\n  arrange(desc(Black_White_Ratio))\n\n# Create visualization\nggplot(disparity_data %&gt;% head(20), aes(x = reorder(State, Black_White_Ratio), y = Black_White_Ratio)) +\n  geom_bar(stat = \"identity\", fill = \"darkred\") +\n  coord_flip() +\n  labs(title = \"Top 20 States: Black-White DNA Collection Disparity Ratio\",\n       subtitle = \"Ratio of collection rates (higher = greater disparity)\",\n       x = \"State\",\n       y = \"Black/White Collection Rate Ratio\") +\n  theme_minimal() +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"gray50\")\n\n\n\n\n\n\n\n\nFigure 1: DNA Collection Rates by Race Relative to Population Percentage"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#state-by-state-detailed-methodology",
    "href": "qmd_root/appendix_analysis.html#state-by-state-detailed-methodology",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "State-by-State Detailed Methodology",
    "text": "State-by-State Detailed Methodology\n\nMethodology: Parsing State Methodology Paragraphs\nThe detailed methodology for each state was extracted from a separate text file (MurphyTong_States_Paragraphs.txt) containing narrative descriptions of data collection approaches for all 50 states. This section explains how we systematically parsed this unstructured text into a structured dataset.\nProcessing Steps:\n\nState Detection: The parser identifies state entries by searching for the 50 U.S. state names as they appear in the text\nSection Extraction: For each state, the parser captures all text from the state name until the next state name appears\nComponent Parsing: Within each state’s section, the code extracts four key components:\n\nLegal Framework: The statutory basis for DNA collection in that state\nCollection Triggers: Specific offenses or events that trigger DNA collection\nData Sources: Types of data used (e.g., conviction records, prison admissions, arrest data)\nSource URLs: Web links to the original data sources\nData Limitations: Known gaps, proxies, or methodological caveats\n\nStructured Data Creation: Each data source line is parsed into a type-note pair (e.g., “Prison admissions: Used as proxy for conviction data”)\nCategorization: The parser automatically categorizes:\n\nCollection trigger types: Comprehensive, selective, felony-only, etc.\nData limitation types: Missing conviction data, ethnicity issues, limited racial categories, etc.\nData source types: Conviction data, arrest data, prison data, sex crime data, etc.\n\n\nThis systematic extraction allows for consistent comparison across states and identification of common patterns in data collection methodologies and limitations.\n\n\nShow paragraph extraction code\n# Pre-define all 50 U.S. states\nus_states &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n               \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n               \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n               \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n               \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n               \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n               \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n               \"Wisconsin\", \"Wyoming\")\n\n# Read the text file\ndata_file &lt;- file.path(here(\"data\", \"annual_dna_collection\", \"intermediate\", \"MurphyTong_States_Paragraphs.txt\"))\n\ntext_content &lt;- readLines(data_file, warn = FALSE)\n\n# Combine all lines into a single string\nfull_text &lt;- paste(text_content, collapse = \"\\n\")\n\n# Create a pattern to match state names\nstate_pattern &lt;- paste0(\"(?m)^[[:space:]]*(\", paste(us_states, collapse = \"|\"), \")\\\\b\")\nstate_matches &lt;- str_locate_all(full_text, state_pattern)[[1]]\n\n# Extract state sections\nstate_sections &lt;- list()\nstate_names &lt;- character()\n\nif (nrow(state_matches) &gt; 0) {\n  for (i in 1:nrow(state_matches)) {\n    start_pos &lt;- state_matches[i, \"start\"]\n    if (i &lt; nrow(state_matches)) {\n      end_pos &lt;- state_matches[i + 1, \"start\"] - 1\n    } else {\n      end_pos &lt;- nchar(full_text)\n    }\n    \n    state_name &lt;- substr(full_text, start_pos, state_matches[i, \"end\"])\n    state_content &lt;- substr(full_text, start_pos, end_pos)\n    \n    state_names &lt;- c(state_names, state_name)\n    state_sections &lt;- c(state_sections, state_content)\n  }\n}\n\n# Function to extract information from each state section\nparse_state_section &lt;- function(section, state_name) {\n  if (is.na(state_name)) return(NULL)\n  \n  # Extract legal framework\n  legal_framework &lt;- str_extract(section, \"Legal Framework:[^\\n]+\") %&gt;%\n    {ifelse(is.na(.), NA, str_remove(., \"Legal Framework:\") %&gt;% str_trim())}\n  \n  # Extract collection triggers\n  collection_triggers &lt;- str_extract(section, \"Collection Triggers:[^\\n]+\") %&gt;%\n    {ifelse(is.na(.), NA, str_remove(., \"Collection Triggers:\") %&gt;% str_trim())}\n  \n  # Extract data sources - capture everything until Source URLs or Data Limitations\n  data_sources_text &lt;- str_extract(section, \"Data Sources:[\\\\s\\\\S]*?(?=Source URLs:|Data Limitations:|$)\")\n  data_source_df &lt;- tibble(data_source_type = NA_character_, data_source_note = NA_character_)\n  \n  if (!is.na(data_sources_text)) {\n    data_sources_text &lt;- str_remove(data_sources_text, \"Data Sources:\") %&gt;% str_trim()\n    # Split by newlines and clean up\n    data_source_lines &lt;- str_split(data_sources_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^Source URLs:|^Data Limitations:\"))\n    \n    if (length(data_source_lines) &gt; 0) {\n      data_source_df &lt;- map_df(data_source_lines, function(line) {\n        if (str_detect(line, \":\")) {\n          tibble(\n            data_source_type = str_extract(line, \"^[^:]+\") %&gt;% str_trim(),\n            data_source_note = str_remove(line, \"^[^:]+:\") %&gt;% str_trim()\n          )\n        } else {\n          tibble(\n            data_source_type = line,\n            data_source_note = NA_character_\n          )\n        }\n      })\n    }\n  }\n  \n  # Extract source URLs\n  source_urls_text &lt;- str_extract(section, \"Source URLs:[\\\\s\\\\S]*?(?=Data Limitations:|$)\")\n  source_urls &lt;- character(0)\n  \n  if (!is.na(source_urls_text)) {\n    source_urls_text &lt;- str_remove(source_urls_text, \"Source URLs:\") %&gt;% str_trim()\n    source_url_lines &lt;- str_split(source_urls_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^Data Limitations:\"))\n    \n    if (length(source_url_lines) &gt; 0) {\n      source_urls &lt;- source_url_lines\n    }\n  }\n  \n  # Extract data limitations - capture everything until next state or end\n  data_limitations_text &lt;- str_extract(section, \"Data Limitations:[\\\\s\\\\S]*?(?=\\\\b(A|Ala|Alas|Ari|Arka|Cali|Colo|Conn|Del|Flo|Geo|Haw|Ida|Ill|Ind|Iow|Kan|Ken|Lou|Mai|Mar|Mas|Mic|Min|Mis|Mon|Neb|Nev|New|Nor|Ohi|Okl|Ore|Pen|Rho|Sou|Ten|Tex|Uta|Ver|Vir|Was|Wis|Wyo)\\\\b|$)\")\n  data_limitations &lt;- NA_character_\n  \n  if (!is.na(data_limitations_text)) {\n    data_limitations_text &lt;- str_remove(data_limitations_text, \"Data Limitations:\") %&gt;% str_trim()\n    data_limitations_lines &lt;- str_split(data_limitations_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^\\\\b(A|Ala|Alas|Ari|Arka|Cali|Colo|Conn|Del|Flo|Geo|Haw|Ida|Ill|Ind|Iow|Kan|Ken|Lou|Mai|Mar|Mas|Mic|Min|Mis|Mon|Neb|Nev|New|Nor|Ohi|Okl|Ore|Pen|Rho|Sou|Ten|Tex|Uta|Ver|Vir|Was|Wis|Wyo)\\\\b\"))\n    \n    if (length(data_limitations_lines) &gt; 0) {\n      data_limitations &lt;- paste(data_limitations_lines, collapse = \"; \")\n    }\n  }\n  \n  # If no data sources were found, create at least one row for the state\n  if (nrow(data_source_df) == 0) {\n    data_source_df &lt;- tibble(data_source_type = NA_character_, data_source_note = NA_character_)\n  }\n  \n  # Create result dataframe\n  result_df &lt;- data_source_df %&gt;%\n    mutate(\n      state = state_name,\n      legal_framework = legal_framework,\n      collection_triggers = collection_triggers,\n      source_url = ifelse(length(source_urls) &gt; 0, paste(source_urls, collapse = \"; \"), NA_character_),\n      data_limitations = data_limitations,\n      .before = everything()\n    )\n  \n  return(result_df)\n}\n\n# Parse all state sections\nstate_data_list &lt;- lapply(seq_along(state_sections), function(i) {\n  parse_state_section(state_sections[[i]], state_names[i])\n})\n\n# Combine all results into one dataframe\nstate_data &lt;- bind_rows(state_data_list)\n\n# Clean up the data - remove rows where all data columns are NA\nfinal_df &lt;- state_data %&gt;%\n  mutate(across(where(is.character), ~ ifelse(.x == \"\" | is.na(.x), NA, .x))) %&gt;%\n  filter(!(is.na(data_source_type) & is.na(source_url) & is.na(data_limitations)))\n\n# Fill in the missing legal_framework and other methodology for each state\nfinal_df_clean &lt;- final_df %&gt;%\n  group_by(state) %&gt;%\n  fill(legal_framework, collection_triggers, .direction = \"downup\") %&gt;%\n  ungroup()\n\n# Now let's fill source_url and data_limitations across all rows for each state\nfinal_df_clean &lt;- final_df_clean %&gt;%\n  group_by(state) %&gt;%\n  mutate(\n    source_url = ifelse(all(is.na(source_url)), NA, \n                       paste(na.omit(unique(source_url)), collapse = \"; \")),\n    data_limitations = ifelse(all(is.na(data_limitations)), NA,\n                             paste(na.omit(unique(data_limitations)), collapse = \"; \"))\n  ) %&gt;%\n  ungroup()\n\n# Create categorization columns for easier analysis\nfinal_df_clean &lt;- final_df_clean %&gt;%\n  mutate(\n    # Categorize collection triggers\n    collection_trigger_category = case_when(\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests.*all felonies\") ~ \"Comprehensive: All felonies + broad arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests.*certain|specific\") ~ \"Selective: All felonies + specific arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests\") ~ \"Broad: All felonies + various arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions\") ~ \"Felony convictions only\",\n      str_detect(collection_triggers, \"(?i)felony.*misdemeanor.*convictions\") ~ \"Mixed: Felony + misdemeanor convictions\",\n      TRUE ~ \"Other/Unspecified\"\n    ),\n    \n    # Categorize data limitations\n    data_limitation_category = case_when(\n      is.na(data_limitations) ~ \"No limitations noted\",\n      str_detect(data_limitations, \"(?i)no direct.*conviction data\") ~ \"Missing conviction data\",\n      str_detect(data_limitations, \"(?i)prison admissions.*proxy\") ~ \"Prison data as proxy\",\n      str_detect(data_limitations, \"(?i)hispanic|ethnicity\") ~ \"Ethnicity categorization issues\",\n      str_detect(data_limitations, \"(?i)racial.*limited|black.*white.*only\") ~ \"Limited racial categories\",\n      str_detect(data_limitations, \"(?i)no.*data.*available|unavailable\") ~ \"Various data unavailable\",\n      TRUE ~ \"Other limitations\"\n    ),\n    \n    # Categorize data source types\n    data_source_category = case_when(\n      str_detect(data_source_type, \"(?i)conviction\") ~ \"Conviction Data\",\n      str_detect(data_source_type, \"(?i)arrest\") ~ \"Arrest Data\",\n      str_detect(data_source_type, \"(?i)sex|sexual\") ~ \"Sex Crime Data\",\n      str_detect(data_source_type, \"(?i)prison|admission|correction\") ~ \"Prison/Incarceration Data\",\n      TRUE ~ \"Other Data Source\"\n    )\n  )\n\n# Create a summary table for quick overview - ONE ROW PER STATE\nmethodology_summary &lt;- final_df_clean %&gt;%\n  distinct(state, legal_framework, collection_triggers, collection_trigger_category, \n           data_limitations, data_limitation_category, source_url) %&gt;%\n  arrange(state)\n\n# Create interactive table\ndatatable_methodology &lt;- datatable(\n  methodology_summary,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 10,\n    columnDefs = list(\n      list(className = 'dt-left', targets = 0:(ncol(methodology_summary)-1)),\n      list(width = '200px', targets = c(1, 2, 3))\n    ),\n    autoWidth = TRUE\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  class = 'cell-border stripe hover'\n) %&gt;%\n  formatStyle(\n    columns = names(methodology_summary),\n    fontSize = '12px',\n    lineHeight = '90%'\n  )\n\n# Show the table\ndatatable_methodology"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#combining-information-into-master-dataset",
    "href": "qmd_root/appendix_analysis.html#combining-information-into-master-dataset",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Combining Information into Master Dataset",
    "text": "Combining Information into Master Dataset\nThe final dataset combines quantitative DNA collection metrics with qualitative methodology to create a comprehensive one-row-per-state resource. This integration allows researchers to understand both the magnitude of DNA collection and the quality/limitations of the underlying data sources.\nIntegration Process:\n\nPrimary Dataset: The summary_data table contains all quantitative metrics:\n\nDNA collection counts and percentages by race\nState population demographics by race\nCollection rates per 100,000 population by race\n\nStudy Methodology Addition: The methodology_summary table provides contextual information:\n\nLegal framework for DNA collection\nSpecific collection triggers\nData source types and limitations\nSource URLs for verification\n\nJoining Strategy:\n\nStates are matched using full state names\nA crosswalk table converts two-letter abbreviations to full names\nLeft join ensures all states from the summary data are retained\n\nColumn Selection: The final dataset includes:\n\nState identifier: Full state name\nCollection metrics: All DNA collection counts, percentages, and rates by race\nMethodology context: Legal framework, collection triggers, data limitations\nQuality flags: Categorized data limitation types for filtering/analysis\n\nOutput Format: One row per state with 30+ columns covering demographics, collection rates, and methodology\n\nThis unified structure enables analyses that account for data quality differences across states when interpreting racial disparities in DNA collection.\n\n\nShow data combination and exportation code\n# Create state name crosswalk for joining\nstate_crosswalk &lt;- tibble(\n  State = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n            \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n            \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n            \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n            \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\"),\n  state_full = c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n                 \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n                 \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n                 \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n                 \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n                 \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n                 \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n                 \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n                 \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n                 \"Wisconsin\", \"Wyoming\", \"District of Columbia\")\n)\n\n# Join summary data with methodology\nannual_dna_combined &lt;- summary_data %&gt;%\n  left_join(state_crosswalk, by = \"State\") %&gt;%\n  left_join(methodology_summary, by = c(\"state_full\" = \"state\")) %&gt;%\n  select(\n    # State identifier\n    state = state_full,\n    state_abbrev = State,\n    \n    # DNA Collection metrics\n    Black_DNA_Pct, Black_DNA_N,\n    Hispanic_DNA_Pct, Hispanic_DNA_N,\n    Asian_DNA_Pct, Asian_DNA_N,\n    Native_American_DNA_Pct, Native_American_DNA_N,\n    White_DNA_Pct, White_DNA_N,\n    \n    # Population demographics\n    Black_Pop_Pct, Hispanic_Pop_Pct, Asian_Pop_Pct,\n    Native_American_Pop_Pct, White_Pop_Pct,\n    \n    # Collection rates\n    Black_Collection_Rate, Hispanic_Collection_Rate,\n    Asian_Collection_Rate, Native_American_Collection_Rate,\n    White_Collection_Rate,\n    \n    # Methodology\n    legal_framework,\n    collection_triggers,\n    collection_trigger_category,\n    data_limitations,\n    data_limitation_category,\n    source_url\n  )\n\n# Calculate total DNA profiles\nannual_dna_combined &lt;- annual_dna_combined %&gt;%\n  mutate(across(c(Black_DNA_N, Hispanic_DNA_N, Asian_DNA_N, Native_American_DNA_N, White_DNA_N), ~ replace_na(., 0)),\n  Total_DNA_Profiles = Black_DNA_N + Hispanic_DNA_N + Asian_DNA_N + \n                         Native_American_DNA_N + White_DNA_N\n  ) %&gt;%\n  relocate(Total_DNA_Profiles, .after = White_DNA_N)\n\n# Show interactive preview\ndatatable(\n  annual_dna_combined,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"400px\",\n    scroller = TRUE,\n    pageLength = 10,\n    columnDefs = list(\n      list(className = 'dt-left', targets = 0:(ncol(annual_dna_combined)-1))\n    ),\n    autoWidth = TRUE\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  class = 'cell-border stripe hover',\n  caption = \"Complete Annual DNA Collection Dataset - One Row Per State\"\n) %&gt;%\n  formatStyle(\n    columns = names(annual_dna_combined),\n    fontSize = '11px',\n    lineHeight = '90%'\n  )"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#data-export-and-versioning",
    "href": "qmd_root/appendix_analysis.html#data-export-and-versioning",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Data Export and Versioning",
    "text": "Data Export and Versioning\n\n\nShow export code\n# Create output directory structure\nintermediate_dir &lt;- here(\"data\", \"annual_dna_collection\", \"intermediate\")\ndir.create(intermediate_dir, recursive = TRUE, showWarnings = FALSE)\n\nfinal_dir &lt;- here(\"data\", \"annual_dna_collection\", \"final\")\ndir.create(final_dir, recursive = TRUE, showWarnings = FALSE)\n\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\n\n# Export final combined dataset\noutput_path &lt;- file.path(final_dir, \"Annual_DNA_Collection.csv\")\nwrite_csv(annual_dna_combined, output_path)\ncat(paste(\"✓ Exported Annual DNA Collection dataset to:\", output_path, \"\\n\"))\n\n# Create frozen version (v1.0) for long-term reference\nfrozen_path &lt;- file.path(frozen_dir, \"Annual_DNA_Collection.csv\")\nwrite_csv(annual_dna_combined, frozen_path)\ncat(paste(\"✓ Created frozen version 1.0 at:\", frozen_path, \"\\n\\n\"))\n\n# Also export the methodology summary separately for reference\nmethodology_output_path &lt;- file.path(intermediate_dir, \"State_Methodology.csv\")\nwrite_csv(methodology_summary, methodology_output_path)\ncat(paste(\"✓ Exported study methodology summary to:\", methodology_output_path, \"\\n\"))\n\n\n✓ Exported Annual DNA Collection dataset to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/annual_dna_collection/final/Annual_DNA_Collection.csv \n✓ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/v1.0/Annual_DNA_Collection.csv \n\n✓ Exported study methodology summary to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/annual_dna_collection/intermediate/State_Methodology.csv"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#summary-and-key-findings",
    "href": "qmd_root/appendix_analysis.html#summary-and-key-findings",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Summary and Key Findings",
    "text": "Summary and Key Findings\n\nDataset Completeness\n\n\nShow completeness analysis\n# Analyze data completeness by category\ncompleteness_summary &lt;- annual_dna_combined %&gt;%\n  summarise(\n    States_with_Black_Data = sum(!is.na(Black_Collection_Rate)),\n    States_with_Hispanic_Data = sum(!is.na(Hispanic_Collection_Rate)),\n    States_with_Asian_Data = sum(!is.na(Asian_Collection_Rate)),\n    States_with_Native_American_Data = sum(!is.na(Native_American_Collection_Rate)),\n    States_with_White_Data = sum(!is.na(White_Collection_Rate)),\n    States_with_Legal_Framework = sum(!is.na(legal_framework))\n  )\n\nkable(t(completeness_summary), col.names = \"Count\", \n      caption = \"Data Completeness Across States\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nData Completeness Across States\n\n\n\nCount\n\n\n\n\nStates_with_Black_Data\n50\n\n\nStates_with_Hispanic_Data\n39\n\n\nStates_with_Asian_Data\n26\n\n\nStates_with_Native_American_Data\n34\n\n\nStates_with_White_Data\n50\n\n\nStates_with_Legal_Framework\n1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Website: https://donadiojp.github.io/PODFRIDGE_Databases/\n\n\nThis repository contains data collection, processing, and analysis code for a comprehensive study of U.S. forensic DNA databases spanning 2001-2025. The project reconstructs the historical growth of the National DNA Index System (NDIS), compiles current state-level DNA database policies and statistics (SDIS), and standardizes demographic data from Freedom of Information Act (FOIA) requests.\n\n\n\nThis dataset is described in:\nLasisi, T., Donadio, J.P., Muller, M., Wilson, J., Mooney, J., & Edge, M.D. (2025). United States forensic DNA databases: national time series (2001–2025) and state cross-sections.\nDataset DOI: [To be added]\n\n\n\n\n\nReconstructs the growth of the FBI’s National DNA Index System using archived snapshots from the Internet Archive’s Wayback Machine.\n\nData Source: FBI CODIS-NDIS Statistics pages\nCoverage: Monthly snapshots from 2001-2025\nMetrics: Offender profiles, arrestee profiles, forensic profiles, participating laboratories, investigations aided\nMethods: Web scraping, HTML parsing, temporal validation, outlier detection\n\nView NDIS Scraping Methodology →\nView NDIS Analysis →\n\n\n\nCompiles current state-level DNA database statistics and policy information across all 50 states and Washington D.C.\n\nData Source: State government websites, legislative databases\nCoverage: Current snapshot (August 2025)\nContent: Profile counts by type (where available), arrestee collection policies, familial search authorization, statutory citations\nMethods: Systematic web searches, policy documentation, legal statute review\n\nView SDIS Analysis →\n\n\n\nStandardizes demographic composition data from state DNA databases obtained through public records requests documented in Murphy & Tong (2020).\n\nData Source: FOIA responses from 7 states (Murphy & Tong, 2020, Appendix A)\nCoverage: 2012-2018 (varies by state)\nContent: Racial and gender composition by profile type (offender/arrestee/forensic)\nMethods: OCR processing, data standardization, quality validation\n\nView FOIA Analysis →\n\n\n\nDocuments the methodology and data sources used in Murphy & Tong (2020) for calculating annual DNA collection rates by race.\n\nData Source: Murphy & Tong (2020, Appendix B)\nCoverage: All 50 states\nContent: Annual collection estimates, Census demographics, calculated collection rates by race\nMethods: Data provenance tracking, methodology documentation\n\nView Methodology →\n\n\n\n\nPODFRIDGE_Databases/\n├── index.qmd # Main website landing page\n├── ndis_collection.qmd # NDIS data collection notebook\n├── ndis_analysis.qmd # NDIS technical validation & figures\n├── sdis.qmd # SDIS compilation & analysis\n├── foia.qmd # FOIA data processing\n├── racial_disparities.qmd # Murphy & Tong methodology documentation\n├── _quarto.yml # Quarto website configuration\n├── styles.css # Custom styling\n├── scripts/ # Helper functions & utilities\n│ ├── wayback_scraper.R # Wayback Machine API functions\n│ ├── html_parsers.R # Era-specific HTML parsing\n│ ├── jurisdiction_mapping.R # Name standardization\n│ └── validation_functions.R # Outlier detection & QC\n├── data/ # Data files (see data/README.md)\n│ ├── raw/ # Unprocessed source data\n│ ├── intermediate/ # Processing outputs\n│ └── final/ # Clean, versioned datasets\n└── docs/ # Rendered website (GitHub Pages)\n\n\n\n\nTina Lasisi\nJ. P. Donadio\nM. Muller\nJ. Wilson\nJ. Mooney\nM. D. Edge\n\nCorresponding author: tlasisi@umich.edu\n\n\n\n\n\n\nPython (≥ 3.13)\nR (≥ 4.0)\nQuarto (≥ 1.3)\nPython packages: requests, beautifulsoup4, lxml, pandas, tqdm, hashlib, collections, pathlib, datetime, os\nR packages: tidyverse, rvest, httr, lubridate, jsonlite, knitr, plotly\n\n\n\n\n\nWeb Scraping: Internet Archive Wayback Machine API\nData Validation: Monotonicity testing, median absolute deviation (MAD) outlier detection\nExternal Validation: Comparison with peer-reviewed publications and FBI press releases\nReproducibility: All processing code available; versioned datasets archived on Zenodo\n\n\n\n\n\nAll final datasets are archived and publicly available on Zenodo:\nZenodo Repository: [DOI to be added upon publication]\nThe repository includes: - NDIS_time_series.csv - Monthly NDIS statistics (2001-2025) - SDIS_cross_section.csv - State-level profiles and policies (2025) - FOIA_Demographics.csv - Demographic composition from FOIA responses - Annual_DNA_Collection.csv - Annual collection rates (Murphy & Tong 2020) - Raw HTML files, intermediate processing outputs, and complete documentation\nFor detailed data dictionaries and usage notes, see data/README.md.\n\n\n\nIf you use these data or code, please cite both the dataset and the associated paper:\n@article{lasisi2025dna,\n  title={United States forensic DNA databases: national time series (2001–2025) and state cross-sections},\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  journal={-},\n  year={2025},\n  note={In press}\n}\n\n@dataset{lasisi2025dna_data,\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  title={United States forensic DNA databases: NDIS, SDIS, and FOIA datasets},\n  year={2025},\n  publisher={Zenodo},\n  doi={[to be added]}\n}\n\n\n\nCode: MIT License Data: CC BY 4.0 (pending Zenodo publication; FOIA-derived data subject to original authors’ permissions)\n\n\n\nWe thank Erin Murphy for providing access to state-level demographic disclosures and FOIA response materials. We acknowledge the Internet Archive’s Wayback Machine for preserving historical web content that made this reconstruction possible."
  },
  {
    "objectID": "about.html#project-overview",
    "href": "about.html#project-overview",
    "title": "About PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is an open-source research initiative focused on forensic DNA database systems in the United States. Our mission is to increase transparency and accessibility of DNA database information through systematic data collection, analysis, and public documentation."
  },
  {
    "objectID": "about.html#research-objectives",
    "href": "about.html#research-objectives",
    "title": "About PODFRIDGE-Databases",
    "section": "Research Objectives",
    "text": "Research Objectives\n\nDocument Historical Growth: Track the expansion of the National DNA Index System (NDIS) from 2001-2025 using archived data from the FBI website\nAnalyze State Variations: Examine differences in state DNA database policies, practices, and legal frameworks\nInvestigate Demographic Patterns: Process and standardize demographic data from FOIA responses to study racial disparities\nEnsure Reproducibility: Create transparent, documented pipelines for forensic data research"
  },
  {
    "objectID": "about.html#data-sources",
    "href": "about.html#data-sources",
    "title": "About PODFRIDGE-Databases",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary Data Collections\n\nWayback Machine Archives: Historical snapshots of FBI NDIS statistics (2001-2025)\nFOIA Responses: Demographic data from 7 states (CA, FL, IN, ME, NV, SD, TX)\nState Statutes: Legal frameworks governing DNA collection and database management\nGovernment Reports: Official publications and statistical reports\n\n\n\nMethodology\nOur approach emphasizes:\n\nTransparency: All data processing steps are documented and reproducible\nQuality Control: Multiple validation checks for data integrity\nStandardization: Consistent formatting across heterogeneous data sources\nEthical Considerations: Responsible handling of sensitive criminal justice data"
  },
  {
    "objectID": "about.html#project-structure",
    "href": "about.html#project-structure",
    "title": "About PODFRIDGE-Databases",
    "section": "Project Structure",
    "text": "Project Structure\nPODFRIDGE-Databases/\n├── data/ # Frozen datasets\n├── raw/ # Raw data files\n├── output/ # Analysis outputs and visualizations\n├── analysis/ # Processing pipelines and analysis code\n├── archive/ # Documentation and research papers\n└── _site/ # Quarto website files"
  },
  {
    "objectID": "about.html#team",
    "href": "about.html#team",
    "title": "About PODFRIDGE-Databases",
    "section": "Team",
    "text": "Team\nTina Lasisi - Principal Investigator\nAssistant Professor at the University of Michigan Anthropology\nJoão P. Donadio - Editor & Technical Advisor\nData Scientist & Research Collaborator"
  },
  {
    "objectID": "about.html#acknowledgments",
    "href": "about.html#acknowledgments",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "We thank Erin Murphy for providing access to state-level demographic disclosures and FOIA response materials. We acknowledge the Internet Archive’s Wayback Machine for preserving historical web content that made this reconstruction possible."
  },
  {
    "objectID": "about.html#citation",
    "href": "about.html#citation",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "If you use these data or code, please cite both the dataset and the associated paper:\n@article{lasisi2025dna,\n  title={United States forensic DNA databases: national time series (2001–2025) and state cross-sections},\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  journal={-},\n  year={2025},\n  note={In press}\n}\n\n@dataset{lasisi2025dna_data,\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  title={United States forensic DNA databases: NDIS, SDIS, and FOIA datasets},\n  year={2025},\n  publisher={Zenodo},\n  doi={[to be added]}\n}"
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Code: MIT License Data: CC BY 4.0 (pending Zenodo publication; FOIA-derived data subject to original authors’ permissions)"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About PODFRIDGE-Databases",
    "section": "Contact",
    "text": "Contact\nFor questions about this research or to collaborate:\n\nEmail:\nGitHub: https://github.com/tinalasisi/PODFRIDGE-Databases"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is a comprehensive research platform analyzing forensic DNA database systems in the United States. This project combines multiple data sources to examine the growth, composition, and demographic patterns within national and state DNA index systems."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is a comprehensive research platform analyzing forensic DNA database systems in the United States. This project combines multiple data sources to examine the growth, composition, and demographic patterns within national and state DNA index systems."
  },
  {
    "objectID": "index.html#research-projects",
    "href": "index.html#research-projects",
    "title": "PODFRIDGE-Databases",
    "section": "Research Projects",
    "text": "Research Projects\n\n1. NDIS Database Analysis\nTracking FBI’s National DNA Index System (2001-2025)\n\nObjective: Reconstruct NDIS growth through Wayback Machine archives\nKey Findings: Exponential profile growth, jurisdictional participation patterns\nMethods: Web scraping, data validation, time-series analysis\n\nView Scraping Methodology →\nView Full Analysis →\n\n\n2. SDIS Summary Analysis\nState DNA Database Policies and Practices\n\nObjective: Examine variation in state DNA database policies\nKey Findings: Arrestee collection policies, familial search allowances\nMethods: Legal statute analysis, policy documentation\n\nView Full Analysis →\n\n\n3. FOIA Document Processing\nRacial Composition of State DNA Databases\n\nObjective: Process FOIA responses on demographic disparities\nKey Findings: Standardized data from 7 states, transparency framework\nMethods: OCR processing, data standardization, quality control\n\nView Full Analysis →\n\n\n4. Annual DNA Collection\nMurphy & Tong Study Appendix\n\nObjective: Document data sources and methodology for racial disparity research\nKey Findings: Consistent disparities, data limitations, methodological challenges\nMethods: Data provenance tracking, methodology documentation\n\nView Full Analysis →"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html",
    "href": "qmd_root/ndis_scraping.html",
    "title": "NDIS Database",
    "section": "",
    "text": "The National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.\nThis project focuses on systematically compiling the growth and evolution of NDIS by parsing historical statistics published on the FBI’s website and preserved in the Internet Archive’s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.\n\n\n\nDevelop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.\nIdentify and correct inconsistencies and data quality issues across historical snapshots.\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time."
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#introduction",
    "href": "qmd_root/ndis_scraping.html#introduction",
    "title": "NDIS Database",
    "section": "",
    "text": "The National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.\nThis project focuses on systematically compiling the growth and evolution of NDIS by parsing historical statistics published on the FBI’s website and preserved in the Internet Archive’s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.\n\n\n\nDevelop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.\nIdentify and correct inconsistencies and data quality issues across historical snapshots.\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time."
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#setup-configuration",
    "href": "qmd_root/ndis_scraping.html#setup-configuration",
    "title": "NDIS Database",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nSystem Requirements\nRequired Packages:\n\nCore: requests, beautifulsoup4, and lxml (scraping/parsing).\nData/Visualization: pandas and tqdm (progress tracking).\n\n\n\nShow Configuration code\nimport sys\nimport subprocess\nimport importlib\n\nrequired_packages = [\n    'requests',         # API/HTTP\n    'beautifulsoup4',   # HTML parsing\n    'lxml',             # Faster parsing\n    'pandas',           # Data handling\n    'tqdm',              # Progress bars\n    'hashlib',\n    'collections',\n    'pathlib',\n    'datetime',\n    'os'\n]\n\nfor package in required_packages:\n    try:\n        importlib.import_module(package)\n        print(f\"✓ {package} already installed\")\n    except ImportError:\n        print(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nprint(f\"📚 All required packages are installed.\")\n\n\n\n\nProject Structure\nMain Configurations:\n\nDirectory paths for raw HTML (data_snapshots), metadata (data_metadata), and outputs (data_outputs).\nStandardization mappings for jurisdiction names and known data typos.\n\n\n\nShow Configuration code\nfrom pathlib import Path\nimport re, json, requests, time, hashlib\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import defaultdict\nimport os\n\n# Configuration\nBASE_DIR = Path(\"..\")  # Project root directory\nHTML_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_snapshots\"    # Storage for downloaded HTML\nMETA_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_metadata\"    # Metadata storage\nOUTPUT_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_outputs\"       # Processed data output\n\nNDIS_SNAPSHOTS_DIR = HTML_DIR\nNDIS_SNAPSHOTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create directory structure\nfor directory in [HTML_DIR, META_DIR, OUTPUT_DIR]:\n    directory.mkdir(parents=True, exist_ok=True)\n\n\n\n\nProject directories initialized:\n  - Working directory: C:\\Users\\Donadio\\Documents\\PODFRIDGE_Databases\n  - HTML storage: ..\\data\\ndis\\raw\\ndis_snapshots\n  - Metadata directory: ..\\data\\ndis\\raw\\ndis_metadata\n  - Output directory: ..\\data\\ndis\\raw\\ndis_outputs"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#WMSS",
    "href": "qmd_root/ndis_scraping.html#WMSS",
    "title": "NDIS Database",
    "section": "Wayback Machine Snapshot Search",
    "text": "Wayback Machine Snapshot Search\nA function was developed to systematically search the Internet Archive’s Wayback Machine for all preserved snapshots of FBI NDIS statistics pages using a comprehensive multi-phase approach.\n\nScraping Method\n\nMulti-Phase Search Strategy:\n\n\nFirst searches for snapshots from the pre-2007 era using state-specific URLs\nThen targets consolidated pages from post-2007 periods\nHandles both HTTP and HTTPS protocol variants\n\n\nRobust Error Handling:\n\n\nAutomatic retries with exponential backoff (1s → 2s → 4s delays)\nDeduplicates results by timestamp\nFailed requests are tracked and retried with increased retry attempts\nPreserves complete error context for troubleshooting\n\n\n\nTechnical Implementation\n\nAPI Request\nConverts JSON responses to clean DataFrame\nSorts chronologically (oldest → newest)\n\n\n\nCore Search Implementation\n\nmake_request_with_retry(): Implements exponential backoff (1s → 2s → 4s delays) for fault-tolerant API requests with configurable retry attempts.\n\n\n\nShow search helpers function code\ndef make_request_with_retry(params, max_retries=3, initial_delay=1):\n    API_URL = \"https://web.archive.org/cdx/search/cdx\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(API_URL, params=params, timeout=30)\n            resp.raise_for_status()\n            print(f\"✓ Successful request for {params['url']} (offset: {params.get('offset', 0)})\")\n            return resp\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"✗ Final attempt failed for {params['url']} (offset: {params.get('offset', 0)}): {str(e)}\")\n                return None\n            print(f\"! Attempt {attempt+1} failed for {params['url']}, retrying in {delay} seconds...\")\n            time.sleep(delay)\n            delay *= 2\n\n\n\nPre-2007 (Clickmap Era)\n\n\n\nPre-2007 Table Format\n\n\n\nEach state/agency has its own page (e.g., ne.htm for Nebraska, dc.htm for DC/FBI Lab).\nThe search iterates through the known two-letter codes and queries Wayback for each URL individually.\nsearch_pre2007_snapshots(): Searches individual state-specific pages using 50+ state codes (al.htm, ak.htm, etc.).\n\n\n\nShow pre-2007 search function code\ndef search_pre2007_snapshots():\n    state_codes = [\"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"dc\",\n                   \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\",\n                   \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\",\n                   \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\",\n                   \"ok\", \"or\", \"pa\", \"pr\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\",\n                   \"army\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"]\n    all_rows = []\n    seen_timestamps = set()\n    total_saved = 0\n    \n    print(f\"Starting pre-2007 snapshot search for {len(state_codes)} state codes at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, code in enumerate(state_codes, 1):\n        url = f\"http://www.fbi.gov/hq/lab/codis/{code}.htm\"\n        offset = 0\n        state_snapshots = 0\n        has_more_results = True\n        \n        print(f\"\\n[{i}/{len(state_codes)}] Searching for {code.upper()} snapshots...\")\n        \n        while has_more_results:\n            params = {\n                \"url\": url,\n                \"matchType\": \"exact\",\n                \"output\": \"json\",\n                \"fl\": \"timestamp,original,mimetype,statuscode\",\n                \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\": \"5000\",\n                \"offset\": str(offset)\n            }\n            resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n            if not resp: \n                print(f\"✗ Failed to retrieve {code.upper()}\")\n                break\n                \n            data = resp.json()\n            \n            if len(data) &lt;= 1:\n                print(f\"  No more results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n                break\n                \n            new_snapshots = 0\n            for row in data[1:]:\n                timestamp = row[0]\n                if timestamp not in seen_timestamps:\n                    all_rows.append(row)\n                    seen_timestamps.add(timestamp)\n                    new_snapshots += 1\n                    state_snapshots += 1\n                    total_saved += 1\n            \n            if new_snapshots &gt; 0:\n                print(f\"  → Saved {new_snapshots} new snapshots for {code.upper()} (offset: {offset})\")\n            \n            # Check if we've reached the end of results\n            if len(data) &lt; 5001:\n                print(f\"  Reached end of results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n            else:\n                offset += 5000\n                time.sleep(1)  # Brief pause between requests\n        \n        if state_snapshots &gt; 0:\n            print(f\"✓ Found {state_snapshots} total snapshots for {code.upper()}\")\n        else:\n            print(f\"✗ No snapshots found for {code.upper()}\")\n    \n    print(f\"\\nPre-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n\n\n\n\nPost-2007 (Consolidated Pages)\n\n\n\n2008 Table Format\n\n\n\n\n\n2025 Table Format\n\n\n\nAll state/agency records are on a single page per snapshot.\nSearches use the known consolidated URL patterns per era.\nsearch_post2007_snapshots(): Searches consolidated pages across 5 historical URL patterns with both HTTP and HTTPS variants.\n\n\n\nShow search function code\ndef search_post2007_snapshots():\n    urls = [\n        \"https://www.fbi.gov/hq/lab/codis/stats.htm\",\n        \"https://www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"https://www.fbi.gov/about-us/lab/biometric-analysis/codis/ndis-statistics\",\n        \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\"\n    ]\n    \n    all_rows = []\n    seen_timestamps = set()\n    protocols = [\"http://\", \"https://\"]\n    total_saved = 0\n    \n    print(f\"Starting post-2007 snapshot search for {len(urls)} URLs at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, base_url in enumerate(urls, 1):\n        url_snapshots = 0\n        \n        for protocol in protocols:\n            current_url = base_url.replace(\"https://\",\"\").replace(\"http://\",\"\")\n            full_url = f\"{protocol}{current_url}\"\n            offset = 0\n            has_more_results = True\n            \n            print(f\"\\n[{i}/{len(urls)}] Searching for {full_url} snapshots...\")\n            \n            while has_more_results:\n                params = {\n                    \"url\": full_url,\n                    \"matchType\": \"exact\",\n                    \"output\": \"json\",\n                    \"fl\": \"timestamp,original,mimetype,statuscode\",\n                    \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                    \"limit\": \"5000\",\n                    \"offset\": str(offset)\n                }\n                resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n                if not resp: \n                    print(f\"✗ Failed to retrieve {full_url}\")\n                    break\n                    \n                data = resp.json()\n                \n                if len(data) &lt;= 1:\n                    print(f\"  No more results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                    break\n                    \n                new_snapshots = 0\n                for row in data[1:]:\n                    timestamp = row[0]\n                    if timestamp not in seen_timestamps:\n                        all_rows.append(row)\n                        seen_timestamps.add(timestamp)\n                        new_snapshots += 1\n                        url_snapshots += 1\n                        total_saved += 1\n                \n                if new_snapshots &gt; 0:\n                    print(f\"  → Saved {new_snapshots} new snapshots for {full_url} (offset: {offset})\")\n                \n                if len(data) &lt; 5001:\n                    print(f\"  Reached end of results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                else:\n                    offset += 5000\n                    time.sleep(1)\n        \n        if url_snapshots &gt; 0:\n            print(f\"✓ Found {url_snapshots} total snapshots for {base_url}\")\n        else:\n            print(f\"✗ No snapshots found for {base_url}\")\n    \n    print(f\"\\nPost-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n\n\n\n\n\nSearch Execution\n\nCalls both pre-2007 and post-2007 search functions to retrieve comprehensive NDIS records.\nCombines results and removes duplicates based on timestamps.\nStores technical details (timestamps, URL variants, duplicate counts) in structured JSON metadata.\nProvides detailed logging and progress tracking throughout the search process.\n\n\n\nShow search code\npre2007_df, _ = search_pre2007_snapshots()\npost2007_df, _ = search_post2007_snapshots()\n\n# Combine all snapshots\nsnap_df = pd.concat([pre2007_df, post2007_df]).drop_duplicates(\"timestamp\").sort_values(\"timestamp\").reset_index(drop=True)\n\n# Save search metadata\nsearch_meta = {\n    \"search_performed\": datetime.now().isoformat(),\n    \"total_snapshots\": len(snap_df),\n    \"time_span\": {\n        \"first\": snap_df[\"timestamp\"].min(),\n        \"last\": snap_df[\"timestamp\"].max()\n    },\n    \"url_variants\": snap_df[\"original\"].nunique()\n}\n\nwith open(META_DIR / \"search_metadata.json\", \"w\") as f:\n    json.dump(search_meta, f, indent=2)\n\n\n\n\n\nSearch Results Summary\n=====================\nLoaded 10310 unique snapshots\nTime coverage:\n20010715040342\nto\n20250815002050\nUnique URL patterns found: 255\nOutput saved to: C:\\Users\\Donadio\\Documents\\PODFRIDGE_Databases\\data\\ndis\\raw\\ndis_metadata/search_metadata.json"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#downloaderSnap",
    "href": "qmd_root/ndis_scraping.html#downloaderSnap",
    "title": "NDIS Database",
    "section": "Snapshot Downloader",
    "text": "Snapshot Downloader\nThis system provides a robust method for downloading historical webpage snapshots from the Internet Archive’s Wayback Machine, specifically designed for the FBI NDIS statistics pages.\n\nDownload Methods\nThe download system implements a sequential approach optimized for reliability and respectful API usage:\n\nResilient Downloading: Automatic retries with exponential backoff (2s → 4s → 8s → 16s → 32s delays) and extended 60-second timeouts for reliable network handling\nSmart File Management: Context-aware naming scheme using timestamp + state/scope identifier (e.g., 20040312_ne.html for pre-2007, 20150621_ndis.html for post-2007)\nDuplicate Prevention: Automatically skips already downloaded files to prevent redundant operations\nProgress Tracking: Real-time download status with completion counters and detailed success/failure reporting\nRate Limiting: 1.5-second delays between requests to avoid overloading the Wayback Machine servers\n\n\n\nShow downloader helpers code\ndef download_with_retry(url, save_path, max_retries=4, initial_delay=2):\n    \"\"\"\n    Download an archived snapshot with retries and exponential backoff.\n    \"\"\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(url, timeout=120)\n            resp.raise_for_status()\n            \n            # Validate content is reasonable\n            if len(resp.content) &lt; 500:\n                if b\"error\" in resp.content.lower() or b\"not found\" in resp.content.lower():\n                    raise requests.exceptions.RequestException(f\"Possible error page: {len(resp.content)} bytes\")\n            \n            # Save to disk\n            with open(save_path, \"wb\") as f:\n                f.write(resp.content)\n            print(f\"✓ Downloaded: {save_path}\")\n            return True\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"✗ Final attempt failed for {url}: {str(e)}\")\n                return False\n            print(f\"! Attempt {attempt+1} failed, retrying in {delay}s...\")\n            time.sleep(delay)\n            delay *= 2\n    return False\n\n\ndef snapshot_to_filepath(row):\n    \"\"\"\n    Map a snapshot record to a local filename.\n    Format: {timestamp}_{state_or_scope}.html\n    \"\"\"\n    ts = row[\"timestamp\"]\n    original = row[\"original\"]\n    \n    # derive name from FBI URL pattern\n    if \"/codis/\" in original and original.endswith(\".htm\"):\n        # Pre-2007: use last part (state code or army/dc)\n        suffix = Path(original).stem\n    else:\n        # Post-2007: consolidated page, use 'ndis'\n        suffix = \"ndis\"\n    \n    save_dir = HTML_DIR\n    save_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n    \n    return save_dir / f\"{ts}_{suffix}.html\"\n\n\n\n\nDownload Execution\nThe download execution phase performs bulk retrieval of historical NDIS snapshots with comprehensive error handling:\n\nSequential Processing: Iterates through snapshot DataFrame chronologically, processing each file individually for maximum reliability\nURL Construction: Uses identity flag (id_) in archive URLs to retrieve unmodified original content: https://web.archive.org/web/{timestamp}id_/{original}\nBinary Preservation: Saves files as binary content to maintain original encoding and prevent character corruption\nComprehensive Logging: Provides real-time progress updates with attempt counters and final success/failure statistics\nFlexible Limiting: Optional download limits for testing or partial processing\n\n\n\nShow download execution code\ndef download_snapshots(snap_df, limit=None):\n    \"\"\"\n    Iterate over snapshot DataFrame and download archived HTML pages.\n    \"\"\"\n    total = len(snap_df) if limit is None else min(limit, len(snap_df))\n    print(f\"Starting download of {total} snapshots at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    successes, failures = 0, 0\n    failed_downloads = []\n    \n    for i, row in enumerate(snap_df.head(total).itertuples(index=False), 1):\n        ts, original, _, _ = row\n        save_path = snapshot_to_filepath(row._asdict())\n        \n        if save_path.exists() and save_path.stat().st_size &gt; 1000:\n            print(f\"- [{i}/{total}] Already exists: {save_path}\")\n            successes += 1\n            continue\n        \n        archive_url = f\"https://web.archive.org/web/{ts}id_/{original}\"\n        print(f\"- [{i}/{total}] Downloading {archive_url}\")\n        \n        if download_with_retry(archive_url, save_path):\n            successes += 1\n        else:\n            failures += 1\n            failed_downloads.append({\n                'timestamp': ts,\n                'url': original,\n                'archive_url': archive_url\n            })\n        \n        if i % 50 == 0:\n            print(f\"Progress: {i}/{total} ({i/total*100:.1f}%) - {successes} success, {failures} failures\")\n        \n        time.sleep(1.5)\n    \n    # Save failure report\n    if failed_downloads:\n        failure_report = {\n            'download_completed': datetime.now().isoformat(),\n            'total_attempted': total,\n            'successful': successes,\n            'failed': failures,\n            'failed_downloads': failed_downloads\n        }\n        with open(META_DIR / \"download_failures.json\", 'w') as f:\n            json.dump(failure_report, f, indent=2)\n    \n    print(f\"\\nDownload completed. Success: {successes}, Failures: {failures}\")\n    return successes, failures\n\n\nsuccesses, failures = download_snapshots(snap_df, limit=None)\n\n\n\n\nDownload Validation\nPost-download validation ensures data integrity and identifies potential issues:\n\nFile Existence Verification: Checks that all expected files were successfully downloaded to the target directory\nContent Quality Assessment: Validates HTML content by examining file headers for proper HTML tags\nError Categorization: Separates missing files from corrupted/non-HTML files for targeted remediation\nMetadata Generation: Creates JSON validation reports with detailed statistics and file counts\nActionable Reporting: Provides clear feedback on download success rates and files requiring attention\n\n\n\nShow download validation code\ndef validate_downloads(snap_df):\n    \"\"\"\n    Validate downloaded files exist and are HTML-like.\n    \"\"\"\n    missing, bad_html = [], []\n    \n    for row in snap_df.itertuples(index=False):\n        save_path = snapshot_to_filepath(row._asdict())\n        if not save_path.exists():\n            missing.append(save_path)\n            continue\n        try:\n            file_size = save_path.stat().st_size\n            if file_size &lt; 1000:\n                bad_html.append(save_path)\n                continue\n                \n            with open(save_path, \"rb\") as f:\n                start = f.read(500).lower()\n            if b\"&lt;html\" not in start and b\"&lt;!doctype\" not in start:\n                bad_html.append(save_path)\n        except Exception:\n            bad_html.append(save_path)\n    \n    print(f\"Validation results → Missing: {len(missing)}, Bad HTML: {len(bad_html)}\")\n    return missing, bad_html\n\n# Run validation\nmissing, bad_html = validate_downloads(snap_df)\n\n# Save validation summary\nvalidation_meta = {\n    \"validation_performed\": datetime.now().isoformat(),\n    \"missing_files\": len(missing),\n    \"bad_html_files\": len(bad_html),\n    \"total_snapshots\": len(snap_df),\n    \"successful_downloads\": successes,\n    \"success_rate\": f\"{(successes/len(snap_df))*100:.1f}%\"\n}\nwith open(META_DIR / \"validation_metadata.json\", \"w\") as f:\n    json.dump(validation_meta, f, indent=2)\n\n\n\n\nShow validation report code\n# Save and print report\nreport_path = META_DIR / f\"validation_metadata.json\"\nwith open(report_path, 'w') as f:\n    json.dump(validation_meta, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(\"DOWNLOAD VALIDATION REPORT\")\nprint(f\"{'='*60}\")\nprint(f\"  Missing files: {validation_meta['missing_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Bad HTML files: {validation_meta['bad_html_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Successful downloads: {validation_meta['successful_downloads']}/{validation_meta['total_snapshots']} ({validation_meta['success_rate']})\")\n\nprint(f\"\\nFull report: {report_path}\")\n\n\n\n============================================================\nDOWNLOAD VALIDATION REPORT\n============================================================\n  Missing files: 48/10310\n  Bad HTML files: 4/10310\n  Successful downloads: 10262/10310 (99.5%)\n\nFull report: ..\\data\\ndis\\raw\\ndis_metadata\\validation_metadata.json"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#extraction-pipe",
    "href": "qmd_root/ndis_scraping.html#extraction-pipe",
    "title": "NDIS Database",
    "section": "Data Extraction",
    "text": "Data Extraction\nThe data extraction pipeline converts downloaded HTML snapshots into structured tabular data, handling the evolution of FBI NDIS reporting formats across different time periods.\n\nExtraction Overview\nThe extraction system processes three distinct eras of NDIS reporting:\n\nPre-2007 Era: Basic statistics without date metadata or arrestee data\n2007-2011 Era: Includes “as of” dates but no arrestee profiles\nPost-2012 Era: Complete format with all profile types and consistent dating\n\nKey Features:\n\nEra-Aware Processing: Automatically routes files to appropriate parsers based on timestamp\nMetadata Recovery: Extracts report dates from “as of” statements when available\nComplete Traceability: Links each record to its source HTML file and original URL\nRobust Error Handling: Processes files individually to prevent single failures from stopping the entire batch\n\n\n\nCore Parser Functions\nEssential text processing utilities for NDIS data extraction:\n\nHTML Cleaning: Removes navigation, scripts, and styling elements to focus on data content\nDate Extraction: Identifies and parses “as of” dates using multiple pattern variations\nText Normalization: Standardizes whitespace and jurisdiction name formatting\nEncoding Handling: Manages various character encodings found in historical snapshots\n\n\n\nShow setup and normalization functions code\ndef extract_ndis_metadata(html_content):\n    \"\"\"\n    Extract key metadata from NDIS HTML content including report dates\n    \n    Returns:\n    --------\n    dict:\n        - report_month: Month from \"as of\" statement (None if not found)\n        - report_year: Year from \"as of\" statement (None if not found)  \n        - clean_text: Normalized text content\n    \"\"\"\n    \n    # Multiple patterns to catch different \"as of\" formats\n    date_patterns = [\n        r'[Aa]s of ([A-Za-z]+) (\\d{4})',           # \"as of November 2008\"\n        r'[Aa]s of ([A-Za-z]+) (\\d{1,2}), (\\d{4})', # \"as of November 15, 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{4})',    # \"Statistics as of November 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{1,2}), (\\d{4})' # \"Statistics as of November 15, 2008\"\n    ]\n    \n    report_month = None\n    report_year = None\n    \n    # Find first occurrence of any date pattern\n    for pattern in date_patterns:\n        date_match = re.search(pattern, html_content)\n        if date_match:\n            month_str = date_match.group(1)\n            if len(date_match.groups()) == 2:  # Month + Year only\n                year_str = date_match.group(2)\n            else:  # Month + Day + Year\n                year_str = date_match.group(3)\n            \n            # Convert month name to number\n            try:\n                month_num = pd.to_datetime(f\"{month_str} 1, 2000\").month\n                report_month = month_num\n                report_year = int(year_str)\n                break\n            except:\n                continue\n    \n    # Clean HTML and normalize text\n    soup = BeautifulSoup(html_content, 'lxml')\n    \n    # Remove scripts, styles, and navigation elements\n    for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n        element.decompose()\n    \n    # Get clean text with normalized whitespace\n    clean_text = re.sub(r'\\s+', ' ', soup.get_text(' ', strip=True))\n    \n    return {\n        'report_month': report_month,\n        'report_year': report_year, \n        'clean_text': clean_text\n    }\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"\n    Clean and standardize jurisdiction names for consistency\n    \"\"\"\n    if not name:\n        return name\n        \n    # Remove common prefixes and suffixes\n    name = re.sub(r'^.*?(Back to top|Tables by NDIS Participant|ation\\.)\\s*', \n                 '', name, flags=re.I).strip()\n    \n    # Standardize known variants\n    replacements = {\n        'D.C./FBI Lab': 'DC/FBI Lab',\n        'D.C./Metro PD': 'DC/Metro PD', \n        'US Army': 'U.S. Army',\n        'D.C.': 'DC'\n    }\n    \n    for old, new in replacements.items():\n        name = name.replace(old, new)\n    \n    return name.strip()\n\ndef extract_original_url_from_filename(html_file):\n    \"\"\"\n    Reconstruct original URL from filename and timestamp\n    \"\"\"\n    filename = html_file.name\n    timestamp = filename.split('_')[0]  # Get timestamp part\n    \n    # Determine URL pattern based on filename suffix\n    if filename.endswith('_ndis.html'):\n        # Post-2007 consolidated format - use most common URL pattern\n        return \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\"\n    else:\n        # Pre-2007 state-specific format\n        state_code = filename.split('_')[1].replace('.html', '')\n        return f\"http://www.fbi.gov/hq/lab/codis/{state_code}.htm\"\n\n\n\n\nEra-Specific Parsers\nTime-period-adapted parsing logic that accounts for format evolution:\nPre-2007 Parser:\n\nExtracts basic statistics from state-specific pages\nUses timestamp-derived year (no report dates available)\nSets arrestee counts to 0 (not reported in this era)\nHandles missing NDIS labs and investigations data\n\n2008-2011 Parser:\n\nProcesses consolidated pages with “Back to top” section dividers\nExtracts month and year from report dates\nHandles missing arrestee data (sets to 0)\nMultiple pattern matching for jurisdiction identification\n\n2012-2016 Parser:\n\nFirst era with arrestee data extraction\nProcesses consolidated pages with “Back to top” section dividers\nMultiple pattern matching for jurisdiction identification\nComplete jurisdiction coverage with standardized names\n\nPost-2017 Parser:\n\nModern format with consistent structure and all fields\nRobust regex pattern for reliable extraction\nFull feature extraction including arrestee profiles\nComplete jurisdiction coverage with standardized names\n\n\n\nShow parser functions code\ndef parse_pre2007_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2001-2007 era (state-specific pages)\n    HTML has table structure with state name followed by data rows\n    \"\"\"\n    records = []\n    \n    # Clean up the text for better matching\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Try multiple patterns to extract state name\n    jurisdiction = None\n    \n    # Pattern 1: State name in large font (from graphic alt text or heading)\n    # Looking for patterns like \"Graphic of Pennsylvania Pennsylvania\" or just the state name\n    state_pattern1 = r'(?:Graphic of|alt=\")([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*?)(?:\"|&gt;|\\s+Offender)'\n    state_match = re.search(state_pattern1, text, re.IGNORECASE)\n    \n    if state_match:\n        jurisdiction = state_match.group(1).strip()\n    \n    # Pattern 2: Try to extract from filename as fallback\n    if not jurisdiction:\n        # Extract state code from filename (e.g., \"20010715040342_pa.html\")\n        filename = html_file.name\n        state_code_match = re.search(r'_([a-z]{2,5})\\.html$', filename)\n        if state_code_match:\n            state_code = state_code_match.group(1)\n            # Map common state codes to names\n            state_map = {\n                'pa': 'Pennsylvania', 'nc': 'North Carolina', 'ct': 'Connecticut',\n                'wv': 'West Virginia', 'ks': 'Kansas', 'nd': 'North Dakota',\n                'wy': 'Wyoming', 'ky': 'Kentucky', 'la': 'Louisiana',\n                'dc': 'DC/FBI Lab', 'de': 'Delaware', 'ne': 'Nebraska',\n                'sc': 'South Carolina', 'tn': 'Tennessee', 'ma': 'Massachusetts',\n                'fl': 'Florida', 'nh': 'New Hampshire', 'sd': 'South Dakota',\n                'me': 'Maine', 'hi': 'Hawaii', 'nm': 'New Mexico',\n                'al': 'Alabama', 'tx': 'Texas', 'mi': 'Michigan',\n                'ut': 'Utah', 'ar': 'Arkansas', 'az': 'Arizona',\n                'mo': 'Missouri', 'ny': 'New York', 'mn': 'Minnesota',\n                'vt': 'Vermont', 'id': 'Idaho', 'oh': 'Ohio',\n                'ok': 'Oklahoma', 'or': 'Oregon', 'ca': 'California',\n                'il': 'Illinois', 'wi': 'Wisconsin', 'ms': 'Mississippi',\n                'wa': 'Washington', 'mt': 'Montana', 'in': 'Indiana',\n                'co': 'Colorado', 'va': 'Virginia', 'ga': 'Georgia',\n                'ak': 'Alaska', 'md': 'Maryland', 'nj': 'New Jersey',\n                'nv': 'Nevada', 'ri': 'Rhode Island', 'ia': 'Iowa',\n                'army': 'U.S. Army'\n            }\n            jurisdiction = state_map.get(state_code, state_code.upper())\n    \n    if jurisdiction:\n        jurisdiction = standardize_jurisdiction_name(jurisdiction)\n        \n        # Extract individual values with more flexible patterns\n        # These patterns work with the table structure in your example\n        offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', text, re.IGNORECASE)\n        forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        # NDIS labs can appear as \"NDIS Participating Labs\" or just \"Number of CODIS Labs\"\n        ndis_labs_match = re.search(r'(?:NDIS\\s+Participating\\s+Labs?|Number\\s+of\\s+CODIS\\s+Labs?)\\s+(\\d+)', text, re.IGNORECASE)\n        \n        investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        if offender_match and forensic_match:\n            records.append({\n                'timestamp': timestamp,\n                'report_month': None,  # Not available pre-2007\n                'report_year': None,   # Not available pre-2007\n                'jurisdiction': jurisdiction,\n                'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                'arrestee': 0,  # Not reported pre-2007\n                'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n            })\n    \n    return records\n\ndef parse_2008_2011_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2008-2011 era\n    Consolidated page with state anchors and \"Back to top\" links\n    No arrestee data in this period\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        # Look for state name pattern (appears as anchor or bold text)\n        # Try multiple patterns to catch different HTML formats\n        jurisdiction = None\n        \n        # Pattern 1: &lt;a name=\"State\"&gt;&lt;/a&gt;&lt;strong&gt;State&lt;/strong&gt;\n        state_match = re.search(r'&lt;a\\s+name=\"([^\"]+)\"[^&gt;]*&gt;.*?(?:&lt;strong&gt;|&lt;b&gt;)\\s*([A-Z][^&lt;]+?)(?:&lt;/strong&gt;|&lt;/b&gt;)', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just the state name in bold/strong tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'(?:&lt;strong&gt;|&lt;b&gt;)\\s*([A-Z][^&lt;]+?)(?:&lt;/strong&gt;|&lt;/b&gt;).*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': 0,  # Not reported 2008-2011\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_2012_2016_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2012-2016 era\n    Includes arrestee data for the first time\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        jurisdiction = None\n        \n        # Look for state name patterns\n        # Pattern 1: &lt;a name=\"State\"&gt;&lt;/a&gt;&lt;b&gt;State&lt;/b&gt;\n        state_match = re.search(r'&lt;a\\s+name=\"([^\"]+)\"[^&gt;]*&gt;.*?&lt;b&gt;([^&lt;]+?)&lt;/b&gt;', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just bold state name\n        if not jurisdiction:\n            state_match = re.search(r'&lt;b&gt;([A-Z][^&lt;]+?)&lt;/b&gt;.*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values INCLUDING arrestee which appears starting 2012\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            arrestee_match = re.search(r'Arrestee\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': int(arrestee_match.group(1).replace(',', '')) if arrestee_match else 0,\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_post2017_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2017+ era\n    Modern format with consistent structure and all fields\n    Keep using your existing working pattern for this era\n    \"\"\"\n    records = []\n    \n    # This is your existing working pattern - don't change it\n    pattern = re.compile(\n        r'([A-Z][\\w\\s\\.\\-\\'\\/&\\(\\)]+?)Statistical Information'\n        r'.*?Offender Profiles\\s+([\\d,]+)'\n        r'.*?Arrestee\\s+([\\d,]+)'\n        r'.*?Forensic Profiles\\s+([\\d,]+)'\n        r'.*?NDIS Participating Labs\\s+(\\d+)'\n        r'.*?Investigations Aided\\s+([\\d,]+)',\n        re.IGNORECASE | re.DOTALL\n    )\n    \n    for match in pattern.finditer(text):\n        records.append({\n            'timestamp': timestamp,\n            'report_month': report_month,\n            'report_year': report_year,\n            'jurisdiction': standardize_jurisdiction_name(match.group(1)),\n            'offender_profiles': int(match.group(2).replace(',', '')),\n            'arrestee': int(match.group(3).replace(',', '')),\n            'forensic_profiles': int(match.group(4).replace(',', '')),\n            'ndis_labs': int(match.group(5)),\n            'investigations_aided': int(match.group(6).replace(',', ''))\n        })\n    \n    return records\n\n\n\n\nOutput Schema\nEach extracted record contains the following standardized fields:\n\n\n\n\n\n\n\n\nField\nDescription\nAvailability\n\n\n\n\ntimestamp\nWayback capture timestamp (YYYYMMDDHHMMSS)\nAll eras\n\n\nreport_month\nMonth from “as of” statement\n2007+ only\n\n\nreport_year\nYear from “as of” statement\n2007+ only\n\n\njurisdiction\nStandardized state/agency name\nAll eras\n\n\noffender_profiles\nDNA profiles from convicted offenders\nAll eras\n\n\narrestee\nDNA profiles from arrestees\n2012+ only\n\n\nforensic_profiles\nCrime scene DNA profiles\nAll eras\n\n\nndis_labs\nNumber of participating laboratories\nAll eras\n\n\ninvestigations_aided\nCases assisted by DNA matches\nAll eras\n\n\n\n\n\nBatch Processing\nThe complete extraction workflow:\nFile Discovery\n\nScans download directory for HTML files\nSorts chronologically for consistent processing\nTracks progress with detailed logging\n\nIndividual File Processing\n\nReads HTML content with encoding fallback\nExtracts metadata and cleans content\nRoutes to era-appropriate parser based on timestamp\nCaptures source file information for traceability\n\nData Consolidation\n\nCombines all records into single DataFrame\nAdds derived timestamp columns (capture_date, year)\nValidates data integrity and completeness\nSorts by capture date and jurisdiction for consistency\n\n\n\nShow batch processing function code\ndef process_ndis_snapshot(html_file):\n    \"\"\"\n    Convert single NDIS HTML file to structured data\n    Routes to appropriate parser based on timestamp year\n    \"\"\"\n    try:\n        # Read HTML content with multiple encoding attempts\n        content = None\n        for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:\n            try:\n                content = html_file.read_text(encoding=encoding)\n                break\n            except UnicodeDecodeError:\n                continue\n        \n        if content is None:\n            # Final fallback with error ignoring\n            content = html_file.read_text(encoding='latin-1', errors='ignore')\n        \n        # Check if file has reasonable content\n        if len(content.strip()) &lt; 100:\n            print(f\"⚠  Small file detected: {html_file.name} ({len(content)} chars)\")\n            return []\n        \n        # Extract metadata\n        metadata = extract_ndis_metadata(content)\n        timestamp = html_file.stem.split('_')[0]\n        \n        try:\n            year = int(timestamp[:4])\n        except ValueError:\n            print(f\"⚠  Invalid timestamp in filename: {html_file.name}\")\n            return []\n        \n        # Route to appropriate parser with fallback\n        records = []\n        parser_used = None\n        \n        if year &lt;= 2007:\n            records = parse_pre2007_ndis(metadata['clean_text'], timestamp, html_file,\n                                       metadata['report_month'], metadata['report_year'])\n            parser_used = \"pre2007\"\n        elif year &lt;= 2011:\n            records = parse_2008_2011_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2008-2011\"\n        elif year &lt;= 2016:\n            records = parse_2012_2016_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2012-2016\"\n        else:\n            records = parse_post2017_ndis(metadata['clean_text'], timestamp, html_file,\n                                        metadata['report_month'], metadata['report_year'])\n            parser_used = \"post2017\"\n        \n        if records:\n            # Add parser info for debugging\n            for record in records:\n                record['parser_used'] = parser_used\n                record['source_file'] = html_file.name\n            return records\n        else:\n            print(f\"⚠  No records extracted from {html_file.name} (year: {year}, parser: {parser_used})\")\n            return []\n            \n    except Exception as e:\n        print(f\"❌ Error processing {html_file.name}: {str(e)}\")\n        return []\n\n\ndef process_all_snapshots():\n    \"\"\"\n    Process all downloaded snapshots into a single DataFrame\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Combined dataset with all snapshots\n    \"\"\"\n    all_records = []\n    html_files = sorted(NDIS_SNAPSHOTS_DIR.glob(\"*.html\"))\n    \n    print(f\"Processing {len(html_files)} HTML snapshots...\")\n    \n    successful_files = 0\n    failed_files = 0\n    failed_list = []\n    \n    for html_file in tqdm(html_files, desc=\"Extracting NDIS data\"):\n        records = process_ndis_snapshot(html_file)\n        if records:\n            all_records.extend(records)\n            successful_files += 1\n        else:\n            failed_files += 1\n            failed_list.append(html_file.name)\n    \n    print(f\"Processing complete: {successful_files} successful, {failed_files} failed\")\n    \n    # Save failure report\n    if failed_list:\n        failure_report_path = OUTPUT_DIR / \"processing_failures.json\"\n        with open(failure_report_path, 'w') as f:\n            json.dump({\n                'timestamp': datetime.now().isoformat(),\n                'total_files': len(html_files),\n                'successful': successful_files,\n                'failed': failed_files,\n                'failed_files': failed_list[:100]  # Limit to first 100\n            }, f, indent=2)\n        print(f\"Failure report saved: {failure_report_path}\")\n    \n    if not all_records:\n        print(\"Warning: No records extracted!\")\n        return pd.DataFrame()\n    \n    df = pd.DataFrame(all_records)\n    \n    # Add derived date columns\n    df['capture_date'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S', errors='coerce')\n    df['capture_year'] = df['capture_date'].dt.year\n    \n    # Remove any records with invalid dates\n    invalid_dates = df['capture_date'].isna().sum()\n    if invalid_dates &gt; 0:\n        print(f\"⚠  Removed {invalid_dates} records with invalid dates\")\n        df = df[df['capture_date'].notna()]\n    \n    return df.sort_values(['capture_date', 'jurisdiction']).reset_index(drop=True)\n\n\n\n\nExport & Validation\nStructured output generation with comprehensive quality control:\nValidation Checks:\n\nVerifies all required columns are present\nChecks for null values in critical fields\nValidates numeric ranges (non-negative counts)\nConfirms timestamp format consistency\nEnsures jurisdiction names contain valid characters\n\nExport Features\n\nSaves as UTF-8 encoded CSV for maximum compatibility\nGenerates timestamped filenames for version control\nCreates metadata summary with file statistics\nPerforms round-trip validation to confirm data integrity\n\nQuality Metrics\n\nRecords total file count and processing success rate\nTracks temporal coverage (earliest to latest snapshots)\nDocuments jurisdiction coverage across time periods\nReports data completeness by era and field\n\n\n\nShow execution function code\ndef export_ndis_data(df, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Export processed NDIS data with comprehensive metadata\n    \"\"\"\n    if df.empty:\n        print(\"Warning: DataFrame is empty, skipping export\")\n        return None\n    \n    # Generate output filename\n    export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = output_dir / f\"ndis_data_raw.csv\"\n    \n    # Export main dataset\n    df.to_csv(csv_path, index=False, encoding='utf-8')\n    \n    # Calculate export metadata\n    file_size_mb = csv_path.stat().st_size / (1024 * 1024)\n    \n    export_metadata = {\n        'export_timestamp': export_timestamp,\n        'export_path': str(csv_path.resolve()),\n        'record_count': len(df),\n        'file_size_mb': round(file_size_mb, 2),\n        'unique_snapshots': df['timestamp'].nunique(),\n        'unique_jurisdictions': df['jurisdiction'].nunique(),\n        'date_coverage': {\n            'earliest_capture': df['capture_date'].min().isoformat(),\n            'latest_capture': df['capture_date'].max().isoformat(),\n            'span_years': df['capture_year'].max() - df['capture_year'].min() + 1\n        },\n        'data_completeness': {\n            'with_report_dates': len(df[df['report_year'].notna()]),\n            'with_arrestee_data': len(df[df['arrestee'] &gt; 0]),\n            'total_investigations_aided': int(df['investigations_aided'].sum())\n        }\n    }\n    \n    # Save metadata\n    metadata_path = output_dir / f\"ndis_export_metadata_{export_timestamp}.json\"\n    import json\n    with open(metadata_path, 'w') as f:\n        json.dump(export_metadata, f, indent=2, default=str)\n    \n    print(f\"✓ Data exported: {csv_path}\")\n    print(f\"✓ Metadata saved: {metadata_path}\")\n    print(f\"✓ Export summary: {len(df):,} records, {file_size_mb:.1f} MB\")\n    \n    return export_metadata\n\n\nData integrity validation\n\nSchema Checking: Ensures proper field types and formats.\nNull Validation: Confirms mandatory fields are populated.\nValue Sanity Checks: Verifies non-negative numbers.\n\n\n\nShow validation function code\ndef validate_extracted_data(df):\n    \"\"\"\n    Comprehensive validation of extracted NDIS data\n    \"\"\"\n    print(\"Validating extracted data...\")\n    \n    # Required columns check\n    required_cols = [\n        'timestamp', 'jurisdiction',\n        'offender_profiles', 'arrestee', 'forensic_profiles', \n        'ndis_labs', 'investigations_aided'\n    ]\n    \n    validation_results = {}\n    \n    # Check column presence\n    missing_cols = set(required_cols) - set(df.columns)\n    validation_results['missing_columns'] = list(missing_cols)\n    \n    if missing_cols:\n        print(f\"✗ Missing required columns: {missing_cols}\")\n        return validation_results\n    \n    # Data quality checks\n    validation_results['total_records'] = len(df)\n    validation_results['unique_timestamps'] = df['timestamp'].nunique()\n    validation_results['unique_jurisdictions'] = df['jurisdiction'].nunique()\n    validation_results['date_range'] = {\n        'earliest': df['capture_date'].min().strftime('%Y-%m-%d'),\n        'latest': df['capture_date'].max().strftime('%Y-%m-%d')\n    }\n    \n    # Null value checks\n    critical_nulls = df[['jurisdiction', 'offender_profiles', 'forensic_profiles']].isnull().sum()\n    validation_results['critical_nulls'] = critical_nulls.to_dict()\n    \n    # Value range checks\n    numeric_cols = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    negative_values = {}\n    for col in numeric_cols:\n        negative_count = (df[col] &lt; 0).sum()\n        if negative_count &gt; 0:\n            negative_values[col] = negative_count\n    validation_results['negative_values'] = negative_values\n    \n    # Era-specific validation\n    pre2007_count = len(df[df['capture_year'] &lt; 2007])\n    arrestee_pre2012 = len(df[(df['capture_year'] &lt; 2012) & (df['arrestee'] &gt; 0)])\n    \n    validation_results['era_checks'] = {\n        'pre2007_records': pre2007_count,\n        'arrestee_before_2012': arrestee_pre2012  # Should be 0\n    }\n    \n    # Print summary\n    print(f\"✓ Total records: {validation_results['total_records']:,}\")\n    print(f\"✓ Unique snapshots: {validation_results['unique_timestamps']}\")\n    print(f\"✓ Unique jurisdictions: {validation_results['unique_jurisdictions']}\")\n    print(f\"✓ Date range: {validation_results['date_range']['earliest']} to {validation_results['date_range']['latest']}\")\n    \n    if validation_results['critical_nulls']:\n        print(f\"! Critical null values found: {validation_results['critical_nulls']}\")\n    \n    if validation_results['negative_values']:\n        print(f\"! Negative values found: {validation_results['negative_values']}\")\n    \n    if validation_results['era_checks']['arrestee_before_2012'] &gt; 0:\n        print(f\"! Data integrity issue: {validation_results['era_checks']['arrestee_before_2012']} arrestee records found before 2012\")\n    \n    return validation_results\n\n\n\nExtraction Execution\n\n\nShow main execution code\nif __name__ == \"__main__\":\n    # Process all snapshots\n    ndis_data = process_all_snapshots()\n    \n    if not ndis_data.empty:\n        # Validate data quality\n        validation_results = validate_extracted_data(ndis_data)\n        \n        # Export if validation passes\n        export_metadata = export_ndis_data(ndis_data)\n        print(\"\\n\" + \"=\"*50)\n        print(\"EXTRACTION COMPLETE\")\n        print(\"=\"*50)\n        print(f\"Records extracted: {len(ndis_data):,}\")\n        print(f\"Time span: {ndis_data['capture_year'].min()}-{ndis_data['capture_year'].max()}\")\n        print(f\"Jurisdictions: {ndis_data['jurisdiction'].nunique()}\")\n    else:\n        print(\"No data extracted - check HTML files and parsing logic\")\n\n\nProcessing 11358 HTML snapshots...\n\n\n\n\n\n⚠  No records extracted from 20120101203804_ndis.html (year: 2012, parser: 2012-2016)\n⚠  No records extracted from 20120111115907_ndis.html (year: 2012, parser: 2012-2016)\n⚠  No records extracted from 20120508173804_ndis.html (year: 2012, parser: 2012-2016)\n⚠  No records extracted from 20161228141646_nj.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228141656_stats.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228141708_ca.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228143828_or.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228143832_mt.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228143841_wy.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228143848_co.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228143853_nm.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228144009_nd.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228144424_sd.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228144434_ne.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228144438_ks.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20161228144451_ok.html (year: 2016, parser: 2012-2016)\n⚠  No records extracted from 20210811085903_nj.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085906_stats.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085910_ca.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085918_or.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085922_mt.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085926_wy.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085929_co.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085932_nm.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085935_nd.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085939_sd.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085943_ne.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085947_ks.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20210811085950_ok.html (year: 2021, parser: post2017)\n⚠  No records extracted from 20220209033036_ndis.html (year: 2022, parser: post2017)\n⚠  No records extracted from 20220210054625_ndis.html (year: 2022, parser: post2017)\n⚠  No records extracted from 20231113033840_ndis.html (year: 2023, parser: post2017)\n⚠  No records extracted from 20240415150311_ndis.html (year: 2024, parser: post2017)\nProcessing complete: 11325 successful, 33 failed\nFailure report saved: ..\\data\\ndis\\raw\\ndis_outputs\\processing_failures.json\nValidating extracted data...\n✓ Total records: 32,008\n✓ Unique snapshots: 11280\n✓ Unique jurisdictions: 169\n✓ Date range: 2001-07-15 to 2025-08-15\n! Critical null values found: {'jurisdiction': 0, 'offender_profiles': 0, 'forensic_profiles': 0}\n✓ Data exported: ..\\data\\ndis\\raw\\ndis_outputs\\ndis_data_raw.csv\n✓ Metadata saved: ..\\data\\ndis\\raw\\ndis_outputs\\ndis_export_metadata_20251005_225312.json\n✓ Export summary: 32,008 records, 3.5 MB\n\n==================================================\nEXTRACTION COMPLETE\n==================================================\nRecords extracted: 32,008\nTime span: 2001-2025\nJurisdictions: 169\n\n\nView Analyses →"
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "This repository contains data collection, processing, and analysis code for a comprehensive study of U.S. forensic DNA databases spanning 2001-2025. The project reconstructs the historical growth of the National DNA Index System (NDIS), compiles current state-level DNA database policies and statistics (SDIS), and standardizes demographic data from Freedom of Information Act (FOIA) requests."
  },
  {
    "objectID": "about.html#associated-publication",
    "href": "about.html#associated-publication",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "This dataset is described in:\nLasisi, T., Donadio, J.P., Muller, M., Wilson, J., Mooney, J., & Edge, M.D. (2025). United States forensic DNA databases: national time series (2001–2025) and state cross-sections.\nDataset DOI: [To be added]"
  },
  {
    "objectID": "about.html#project-components",
    "href": "about.html#project-components",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Reconstructs the growth of the FBI’s National DNA Index System using archived snapshots from the Internet Archive’s Wayback Machine.\n\nData Source: FBI CODIS-NDIS Statistics pages\nCoverage: Monthly snapshots from 2001-2025\nMetrics: Offender profiles, arrestee profiles, forensic profiles, participating laboratories, investigations aided\nMethods: Web scraping, HTML parsing, temporal validation, outlier detection\n\nView NDIS Scraping Methodology →\nView NDIS Analysis →\n\n\n\nCompiles current state-level DNA database statistics and policy information across all 50 states and Washington D.C.\n\nData Source: State government websites, legislative databases\nCoverage: Current snapshot (August 2025)\nContent: Profile counts by type (where available), arrestee collection policies, familial search authorization, statutory citations\nMethods: Systematic web searches, policy documentation, legal statute review\n\nView SDIS Analysis →\n\n\n\nStandardizes demographic composition data from state DNA databases obtained through public records requests documented in Murphy & Tong (2020).\n\nData Source: FOIA responses from 7 states (Murphy & Tong, 2020, Appendix A)\nCoverage: 2012-2018 (varies by state)\nContent: Racial and gender composition by profile type (offender/arrestee/forensic)\nMethods: OCR processing, data standardization, quality validation\n\nView FOIA Analysis →\n\n\n\nDocuments the methodology and data sources used in Murphy & Tong (2020) for calculating annual DNA collection rates by race.\n\nData Source: Murphy & Tong (2020, Appendix B)\nCoverage: All 50 states\nContent: Annual collection estimates, Census demographics, calculated collection rates by race\nMethods: Data provenance tracking, methodology documentation\n\nView Methodology →"
  },
  {
    "objectID": "about.html#repository-structure",
    "href": "about.html#repository-structure",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "PODFRIDGE_Databases/\n├── index.qmd # Main website landing page\n├── ndis_collection.qmd # NDIS data collection notebook\n├── ndis_analysis.qmd # NDIS technical validation & figures\n├── sdis.qmd # SDIS compilation & analysis\n├── foia.qmd # FOIA data processing\n├── racial_disparities.qmd # Murphy & Tong methodology documentation\n├── _quarto.yml # Quarto website configuration\n├── styles.css # Custom styling\n├── scripts/ # Helper functions & utilities\n│ ├── wayback_scraper.R # Wayback Machine API functions\n│ ├── html_parsers.R # Era-specific HTML parsing\n│ ├── jurisdiction_mapping.R # Name standardization\n│ └── validation_functions.R # Outlier detection & QC\n├── data/ # Data files (see data/README.md)\n│ ├── raw/ # Unprocessed source data\n│ ├── intermediate/ # Processing outputs\n│ └── final/ # Clean, versioned datasets\n└── docs/ # Rendered website (GitHub Pages)"
  },
  {
    "objectID": "about.html#authors",
    "href": "about.html#authors",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Tina Lasisi\nJ. P. Donadio\nM. Muller\nJ. Wilson\nJ. Mooney\nM. D. Edge\n\nCorresponding author: tlasisi@umich.edu"
  },
  {
    "objectID": "about.html#technical-details",
    "href": "about.html#technical-details",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Python (≥ 3.13)\nR (≥ 4.0)\nQuarto (≥ 1.3)\nPython packages: requests, beautifulsoup4, lxml, pandas, tqdm, hashlib, collections, pathlib, datetime, os\nR packages: tidyverse, rvest, httr, lubridate, jsonlite, knitr, plotly\n\n\n\n\n\nWeb Scraping: Internet Archive Wayback Machine API\nData Validation: Monotonicity testing, median absolute deviation (MAD) outlier detection\nExternal Validation: Comparison with peer-reviewed publications and FBI press releases\nReproducibility: All processing code available; versioned datasets archived on Zenodo"
  },
  {
    "objectID": "about.html#data-access",
    "href": "about.html#data-access",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "All final datasets are archived and publicly available on Zenodo:\nZenodo Repository: [DOI to be added upon publication]\nThe repository includes: - NDIS_time_series.csv - Monthly NDIS statistics (2001-2025) - SDIS_cross_section.csv - State-level profiles and policies (2025) - FOIA_Demographics.csv - Demographic composition from FOIA responses - Annual_DNA_Collection.csv - Annual collection rates (Murphy & Tong 2020) - Raw HTML files, intermediate processing outputs, and complete documentation\nFor detailed data dictionaries and usage notes, see data/README.md."
  }
]